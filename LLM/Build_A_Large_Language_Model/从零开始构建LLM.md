# 从零开始构建LLM

[书籍链接](https://www.manning.com/books/build-a-large-language-model-from-scratch)

## 1、LLM是什么

大语言模型，主要结构如下：从raw data中进行预训练，得出基础模型（这一部分可以了解一下元学习的概念），这个基础模型所拥有的基础能力为文本补全、短时任务的推理能力。

在基础模型之上，可以导入自己标记的数据进行训练，这一部分可以成为微调（finetune），得到自己的LLM，可以用于分类，总结，翻译，个人助理等任务。

![1716275709784](image/从零开始构建LLM/1716275709784.png)

transformer结构

![1716275687724](image/从零开始构建LLM/1716275687724.png)

BERT与GPT区别：BERT更多的使用于文本填空，GPT则是预测下一个单词。

![1716275758151](image/从零开始构建LLM/1716275758151.png)

构建大模型步骤

![1716275818354](image/从零开始构建LLM/1716275818354.png)

---

## 2、文本数据处理

### 2.1词嵌入

词嵌入的根本目的是为了将非数值数据转换为向量，这样才能放入计算机进行运算。常见词嵌入的有Word2Vec。在GPT架构中，没有使用这一技术，GPT3的嵌入大小达到了12288维。其中，GPT将词嵌入作为训练模型，不断调整。也就是说，GPT将词嵌入这一部分也进行训练。

![1716433691383](image/从零开始构建LLM/1716433691383.png)

### 2.2标记文本

首先分词，再将分词的结果用字典标记token id，token id在进行词嵌入。

![1716433945337](image/从零开始构建LLM/1716433945337.png)

在分词完成后，转换为token id需要对应的字典表。字典表，可以自己构建，通过给每个单词指定唯一id，完成之后即可完成token与id之间的互相转换。

![1716434053588](image/从零开始构建LLM/1716434053588.png)

![1716434299643](image/从零开始构建LLM/1716434299643.png)

### 2.3 特殊处理

在将文本转换为token id时，字典表的大小，覆盖全不全面，对于tokenizer是一个很严峻的考验。如果需要转换的字符不在字典表中，就需要特殊处理，另外对于不同句子之间，也需要分割符。


![1716455910828](image/从零开始构建LLM/1716455910828.png)

不同句子，不同文本之间的合并，可以使用定义的分割符进行连接

![1716455950157](image/从零开始构建LLM/1716455950157.png)

### 2.4 字节对编码（Byte pair encoding, BPE）

使用包：tiktoken

对于未知词，使用分词，然后进行编码，并根据频率进行合并。

![1716778401378](image/从零开始构建LLM/1716778401378.png)

### 2.5 使用滑窗进行数据采样

![1716778580547](image/从零开始构建LLM/1716778580547.png)

![1716778596288](image/从零开始构建LLM/1716778596288.png)

不同步长下的LLM采样

![1716778622405](image/从零开始构建LLM/1716778622405.png)

2.6 创建token嵌入

![1716778710812](image/从零开始构建LLM/1716778710812.png)

这一部分从初始化权重中根据token id进行选择。

![1716778796944](image/从零开始构建LLM/1716778796944.png)

### 2.6 编码单词位置

在上一节中，会出现如下问题，当token id一致时，从权重矩阵中选择的向量也一致。

![1716778847577](image/从零开始构建LLM/1716778847577.png)

为了解决这一问题，引入了位置编码，这样就可以保证每一个编码是独一无二的

![1716778935403](image/从零开始构建LLM/1716778935403.png)

最后，所有的数据处理流程如下：

![1716778990725](image/从零开始构建LLM/1716778990725.png)

## 3. 编码注意力机制
