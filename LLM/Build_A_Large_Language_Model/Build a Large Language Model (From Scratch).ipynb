{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1 理解大语言模型 - Large Language Model (LLM)\n",
    "\n",
    "> 主要结构如下：\n",
    "从raw data中进行预训练，得出基础模型（这一部分可以了解一下元学习的概念），这个基础模型所拥有的基础能力为文本补全、短时任务的推理能力。</br>\n",
    "> 在基础模型之上，可以导入自己标记的数据进行训练，这一部分可以成为微调（finetune），得到自己的LLM，可以用于分类，总结，翻译，个人助理等任务。\n",
    "\n",
    "![1716275709784](image/从零开始构建LLM/1716275709784.png)\n",
    "\n",
    "> **Transformer** 结构概览</br>\n",
    "1、输入需要被翻译的文本</br>\n",
    "2、预处理文本</br>\n",
    "3、编码器将输入文本进行编码</br>\n",
    "4、将编码部分送入解码器</br>\n",
    "5、模型每次只完成一个单词的翻译</br>\n",
    "6、预处理文本</br>\n",
    "7、解码器生成一个单词</br>\n",
    "8、完成翻译</br>\n",
    "\n",
    "![1716275687724](image/从零开始构建LLM/1716275687724.png)\n",
    "\n",
    "> BERT与GPT区别：BERT更多的使用于文本填空，GPT则是预测下一个单词。\n",
    "\n",
    "![1716275758151](image/从零开始构建LLM/1716275758151.png)\n",
    "\n",
    "> **构建大模型步骤**</br>\n",
    "\n",
    "|阶段|子项|\n",
    "|---|---|\n",
    "|一|准备数据和样本|\n",
    "||实现注意力机制|\n",
    "||实现LLM结构|\n",
    "|二|训练|\n",
    "||模型评估|\n",
    "||加载预训练模型权重|\n",
    "|三|微调自己的模型|\n",
    "\n",
    "![1716275818354](image/从零开始构建LLM/1716275818354.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, re, time, json\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import tiktoken\n",
    "\n",
    "from scripts.scripts import *\n",
    "from scripts.dataset import *\n",
    "from scripts.GPTmodel import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2 文本数据处理"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.1 词嵌入\n",
    "词嵌入的根本目的是为了**将非数值数据转换为向量**，这样才能放入计算机进行运算。常见词嵌入的有**Word2Vec**。在GPT架构中，没有使用这一技术，GPT3的嵌入大小达到了12288维。其中，GPT将词嵌入作为训练模型，不断调整。也就是说，**GPT将词嵌入这一部分也进行训练**。\n",
    "\n",
    "![1716433691383](image/从零开始构建LLM/1716433691383.png)\n",
    "\n",
    "## 2.2 标记文本\n",
    "标记文本就是将文本进行拆分，拆分为单个单词后，对每个单词进行唯一映射。可以使用字典进行标记，将每个单词映射为token id，再使用token id进行词嵌入。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">> Total number of character: 20479\n",
      ">> raw text: I HAD always thought Jack Gisburn rather a cheap genius--though a good fellow enough--so it was no g\n",
      "\n",
      ">> preprocessed: ['I', 'HAD', 'always', 'thought', 'Jack', 'Gisburn', 'rather', 'a', 'cheap', 'genius', '--', 'though', 'a', 'good', 'fellow', 'enough', '--', 'so', 'it', 'was', 'no', 'great', 'surprise', 'to', 'me', 'to', 'hear', 'that', ',', 'in']\n",
      ">> length: 4690\n",
      "\n",
      ">> size of vocab after removed duplicate words: 1130\n",
      ">> vocab: front 20 items\n",
      "! 0\n",
      "\" 1\n",
      "' 2\n",
      "( 3\n",
      ") 4\n",
      ", 5\n",
      "-- 6\n",
      ". 7\n",
      ": 8\n",
      "; 9\n",
      "? 10\n",
      "A 11\n",
      "Ah 12\n",
      "Among 13\n",
      "And 14\n",
      "Are 15\n",
      "Arrt 16\n",
      "As 17\n",
      "At 18\n",
      "Be 19\n",
      "Begin 20\n"
     ]
    }
   ],
   "source": [
    "filepath = os.path.join('data', 'the-verdict.txt')\n",
    "assert os.path.exists(filepath), f\"{filepath} is not exists.\"\n",
    "with open(filepath) as f:\n",
    "    raw_text = f.read()\n",
    "print(\">> Total number of character:\", len(raw_text))\n",
    "print(\">> raw text:\", raw_text[:100])\n",
    "print()\n",
    "\n",
    "# split raw text\n",
    "preprocessed = re.split(r'([,.:;?_!\"()\\']|--|\\s)', raw_text)\n",
    "preprocessed = [item.strip() for item in preprocessed if item.strip()]  # remove empty string\n",
    "print(\">> preprocessed:\", preprocessed[:30])\n",
    "print(\">> length:\", len(preprocessed))\n",
    "print()\n",
    "\n",
    "# remove duplicate words\n",
    "all_words = sorted(set(preprocessed))\n",
    "vocab_size = len(all_words)\n",
    "print(\">> size of vocab after removed duplicate words:\", vocab_size)\n",
    "\n",
    "# create vocab\n",
    "vocab = {token:integer for integer,token in enumerate(all_words)}\n",
    "print(\">> vocab: front 20 items\")\n",
    "for tok, i in vocab.items():\n",
    "    if i > 20:\n",
    "        break\n",
    "    print(tok, i)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![1716433945337](image/从零开始构建LLM/1716433945337.png)\n",
    "\n",
    "字典表的创建方式可以通过自己创建，通过创建后的字典表，可以实现文本与token id之间的互相转换。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">> original text:  \"It's the last he painted, you know,\" \n",
      "           Mrs. Gisburn said with pardonable pride.\n",
      ">> encoded data: [1, 56, 2, 850, 988, 602, 533, 746, 5, 1126, 596, 5, 1, 67, 7, 38, 851, 1108, 754, 793, 7]\n",
      ">> decoded data: \" It' s the last he painted, you know,\" Mrs. Gisburn said with pardonable pride.\n"
     ]
    }
   ],
   "source": [
    "class SimpleTokenizerV1:\n",
    "    def __init__(self, vocab):  # our vocab\n",
    "        self.str_to_int = vocab\n",
    "        self.int_to_str = {i:s for s,i in vocab.items()}  # reverse k, v\n",
    "    \n",
    "    def encode(self, text):  # our text\n",
    "        preprocessed = re.split(r'([,.?_!\"()\\']|--|\\s)', text)\n",
    "        preprocessed = [\n",
    "            item.strip() for item in preprocessed if item.strip()  # remove empty string\n",
    "        ]\n",
    "        ids = [self.str_to_int[s] for s in preprocessed]\n",
    "        return ids\n",
    "        \n",
    "    def decode(self, ids):\n",
    "        text = \" \".join([self.int_to_str[i] for i in ids])\n",
    "        # Replace spaces before the specified punctuations\n",
    "        text = re.sub(r'\\s+([,.?!\"()\\'])', r'\\1', text)\n",
    "        return text\n",
    "\n",
    "tokenizer = SimpleTokenizerV1(vocab)\n",
    "\n",
    "text = \"\"\"\"It's the last he painted, you know,\" \n",
    "           Mrs. Gisburn said with pardonable pride.\"\"\"\n",
    "print(\">> original text: \", text)\n",
    "\n",
    "ids = tokenizer.encode(text)\n",
    "print(\">> encoded data:\", ids)\n",
    "\n",
    "decoded_text = tokenizer.decode(ids)\n",
    "print(\">> decoded data:\", decoded_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![1716434053588](image/从零开始构建LLM/1716434053588.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.3 特殊处理\n",
    "正如一般的数据预处理流程，文本中的异常数据也应当注意。当上述字典表覆盖不全面时，针对不在字典表中的字符就需要特殊处理，并且不同句子之间，也需要分割符。</br>\n",
    "\n",
    "**为未知单词加入一些特殊标记**是非常有用的。作用如下：\n",
    "\n",
    "* 使用特殊标记来帮助 LLM 提供额外的上下文\n",
    "* 注：一些特殊标记如下<br/>\n",
    "    1. [BOS] Beginning of sequence. 文本开始<br/>\n",
    "    2. [EOS] end of sequence. 文本结束<br/>\n",
    "    3. [PAD] padding. 使训练文本长度统一<br/>\n",
    "    [UNK] 未知字符，不在字典表中<br/>\n",
    "* GPT-2中仅使用`<|endoftext|>`减少复杂性，`<|endoftext|>`与`[EOS]`用法类似。GPT-2同时使用`<|endoftext|>`来进行PAD操作。\n",
    "* 对于未知单词，GPT-2未使用[UNK]进行替代，而是使用字节对编码-(byte-pair encoding, BPE)将单词进行分解。\n",
    "\n",
    "因此在上述V1版本上，我们需要进行改进，将未知字符与分割符加入字典表中：\n",
    "\n",
    "`all_tokens.extend([\"<|endoftext|>\", \"<|unk|>\"])`\n",
    "\n",
    "![1716455910828](image/从零开始构建LLM/1716455910828.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">> size of vocab after removed duplicate words: 1161\n",
      ">> vocab: last 5 items\n",
      "('younger', 1156)\n",
      "('your', 1157)\n",
      "('yourself', 1158)\n",
      "('<|endoftext|>', 1159)\n",
      "('<|unk|>', 1160)\n"
     ]
    }
   ],
   "source": [
    "# preprocessed = re.split(r'([,.:;?_!\"()\\']|--|\\s)', raw_text)  # pre version\n",
    "preprocessed = re.split(r'([,.?_!\"()\\']|--|\\s)', raw_text)\n",
    "preprocessed = [item.strip() for item in preprocessed if item.strip()]\n",
    "\n",
    "all_tokens = sorted(list(set(preprocessed)))\n",
    "all_tokens.extend([\"<|endoftext|>\", \"<|unk|>\"])\n",
    "\n",
    "vocab = {token:integer for integer,token in enumerate(all_tokens)}\n",
    "\n",
    "vocab_size = len(vocab)\n",
    "print(\">> size of vocab after removed duplicate words:\", vocab_size)\n",
    "\n",
    "print(\">> vocab: last 5 items\")\n",
    "for i, tok in enumerate(list(vocab.items())[-5:]):\n",
    "    print(tok)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">> input text: Hello, do you like tea? <|endoftext|> In the sunlit terraces of the palace.\n",
      ">> encoded data: [1160, 5, 362, 1155, 642, 1000, 10, 1159, 57, 1013, 981, 1009, 738, 1013, 1160, 7]\n",
      ">> decoded data: <|unk|>, do you like tea? <|endoftext|> In the sunlit terraces of the <|unk|>.\n"
     ]
    }
   ],
   "source": [
    "class SimpleTokenizerV2:\n",
    "    def __init__(self, vocab):\n",
    "        self.str_to_int = vocab\n",
    "        self.int_to_str = { i:s for s,i in vocab.items()}\n",
    "    \n",
    "    def encode(self, text):\n",
    "        preprocessed = re.split(r'([,.?_!\"()\\']|--|\\s)', text)\n",
    "        preprocessed = [item.strip() for item in preprocessed if item.strip()]\n",
    "        preprocessed = [\n",
    "            item if item in self.str_to_int \n",
    "            else \"<|unk|>\" for item in preprocessed\n",
    "        ]\n",
    "\n",
    "        ids = [self.str_to_int[s] for s in preprocessed]\n",
    "        return ids\n",
    "        \n",
    "    def decode(self, ids):\n",
    "        text = \" \".join([self.int_to_str[i] for i in ids])\n",
    "        # Replace spaces before the specified punctuations\n",
    "        text = re.sub(r'\\s+([,.?!\"()\\'])', r'\\1', text)\n",
    "        return text\n",
    "\n",
    "\n",
    "tokenizer = SimpleTokenizerV2(vocab)\n",
    "\n",
    "text1 = \"Hello, do you like tea?\"\n",
    "text2 = \"In the sunlit terraces of the palace.\"\n",
    "\n",
    "text = \" <|endoftext|> \".join((text1, text2))\n",
    "\n",
    "print(\">> input text:\", text)\n",
    "\n",
    "ids = tokenizer.encode(text)\n",
    "print(\">> encoded data:\", ids)\n",
    "\n",
    "decoded_text = tokenizer.decode(ids)\n",
    "print(\">> decoded data:\", decoded_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.4 字节对编码\n",
    "\n",
    "`pip install tiktoken`\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in indexes: https://pypi.tuna.tsinghua.edu.cn/simple\n",
      "Requirement already satisfied: tiktoken in d:\\python\\python39\\lib\\site-packages (0.7.0)\n",
      "Requirement already satisfied: regex>=2022.1.18 in d:\\python\\python39\\lib\\site-packages (from tiktoken) (2023.12.25)\n",
      "Requirement already satisfied: requests>=2.26.0 in d:\\python\\python39\\lib\\site-packages (from tiktoken) (2.31.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in d:\\python\\python39\\lib\\site-packages (from requests>=2.26.0->tiktoken) (3.2.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in d:\\python\\python39\\lib\\site-packages (from requests>=2.26.0->tiktoken) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in d:\\python\\python39\\lib\\site-packages (from requests>=2.26.0->tiktoken) (2.0.4)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in d:\\python\\python39\\lib\\site-packages (from requests>=2.26.0->tiktoken) (2023.7.22)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 24.0 -> 24.1.1\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "%pip install tiktoken\n",
    "import tiktoken"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">> encoded data: [15496, 11, 466, 345, 588, 8887, 30, 220, 50256, 554, 262, 4252, 18250, 8812, 2114, 286, 617, 34680, 27271, 13]\n",
      ">> decoded data: Hello, do you like tea? <|endoftext|> In the sunlit terraces of someunknownPlace.\n"
     ]
    }
   ],
   "source": [
    "tokenizer = tiktoken.get_encoding(\"gpt2\")\n",
    "\n",
    "text = \"Hello, do you like tea? <|endoftext|> In the sunlit terraces of someunknownPlace.\"\n",
    "\n",
    "ids = tokenizer.encode(text, allowed_special={\"<|endoftext|>\"})\n",
    "print(\">> encoded data:\", ids)\n",
    "\n",
    "decoded_text = tokenizer.decode(ids)\n",
    "print(\">> decoded data:\", decoded_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> BPE会将未知单词拆分成独立个体的单词\n",
    "\n",
    "![1716778401378](image/从零开始构建LLM/1716778401378.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.5 使用滑窗进行数据采样\n",
    "\n",
    "![1716778580547](image/从零开始构建LLM/1716778580547.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">> x: [290, 4920, 2241, 287]\n",
      ">> y: [4920, 2241, 287, 257]\n",
      "\n",
      ">> tokenizer encode in one context:\n",
      ">> [290] --> 4920\n",
      ">> [290, 4920] --> 2241\n",
      ">> [290, 4920, 2241] --> 287\n",
      ">> [290, 4920, 2241, 287] --> 257\n",
      "\n",
      ">> tokenizer decode in one context:\n",
      ">>  and -->  established\n",
      ">>  and established -->  himself\n",
      ">>  and established himself -->  in\n",
      ">>  and established himself in -->  a\n"
     ]
    }
   ],
   "source": [
    "enc_text = tokenizer.encode(raw_text)\n",
    "enc_sample = enc_text[50:]\n",
    "\n",
    "context_size = 4\n",
    "x = enc_sample[:context_size]\n",
    "y = enc_sample[1:context_size + 1]\n",
    "print(f\">> x: {x}\")\n",
    "print(f\">> y: {y}\")\n",
    "print()\n",
    "\n",
    "print(\">> tokenizer encode in one context:\")\n",
    "for i in range(1, context_size + 1):\n",
    "    context = enc_sample[:i]\n",
    "    desired = enc_sample[i]\n",
    "    print(f\">> {context} --> {desired}\")\n",
    "print()\n",
    "\n",
    "print(\">> tokenizer decode in one context:\")\n",
    "for i in range(1, context_size + 1):\n",
    "    context = enc_sample[:i]\n",
    "    desired = enc_sample[i]\n",
    "    print(f\">> {tokenizer.decode(context)} --> {tokenizer.decode([desired])}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "因此，我们主要关心的只有两个向量，输入和输出\n",
    "\n",
    "![1716778596288](image/从零开始构建LLM/1716778596288.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "\n",
    "class GPTDatasetV1(Dataset):\n",
    "    def __init__(self, txt, tokenizer, max_length, stride):\n",
    "        self.input_ids = []\n",
    "        self.target_ids = []\n",
    "\n",
    "        # Tokenize the entire text\n",
    "        token_ids = tokenizer.encode(txt, allowed_special={\"<|endoftext|>\"})\n",
    "\n",
    "        # Use a sliding window to chunk the book into overlapping sequences of max_length\n",
    "        for i in range(0, len(token_ids) - max_length, stride):\n",
    "            input_chunk = token_ids[i:i + max_length]\n",
    "            target_chunk = token_ids[i + 1: i + max_length + 1]\n",
    "            self.input_ids.append(torch.tensor(input_chunk))\n",
    "            self.target_ids.append(torch.tensor(target_chunk))\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.input_ids)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.input_ids[idx], self.target_ids[idx]\n",
    "\n",
    "\n",
    "def create_dataloader_v1(txt, batch_size=4, max_length=256, \n",
    "                         stride=128, shuffle=True, drop_last=True,\n",
    "                         num_workers=0):\n",
    "\n",
    "    # Initialize the tokenizer\n",
    "    tokenizer = tiktoken.get_encoding(\"gpt2\")\n",
    "\n",
    "    # Create dataset\n",
    "    dataset = GPTDatasetV1(txt, tokenizer, max_length, stride)\n",
    "\n",
    "    # Create dataloader\n",
    "    dataloader = DataLoader(\n",
    "        dataset,\n",
    "        batch_size=batch_size,\n",
    "        shuffle=shuffle,\n",
    "        drop_last=drop_last,\n",
    "        num_workers=0\n",
    "    )\n",
    "\n",
    "    return dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">> [tensor([[  40,  367, 2885, 1464],\n",
      "        [2885, 1464, 1807, 3619]]), tensor([[ 367, 2885, 1464, 1807],\n",
      "        [1464, 1807, 3619,  402]])]\n"
     ]
    }
   ],
   "source": [
    "# modify: batch_size, max_length, stride\n",
    "# will get different data\n",
    "dataloader = create_dataloader_v1(\n",
    "    raw_text, batch_size=2, max_length=4, stride=2, shuffle=False\n",
    ")\n",
    "\n",
    "data_iter = iter(dataloader)\n",
    "data = next(data_iter)\n",
    "print(f\">> {data}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.6 创建token嵌入\n",
    "\n",
    "这一部分将token id转换为嵌入向量\n",
    "\n",
    "![1716778710812](image/从零开始构建LLM/1716778710812.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">> Parameter containing:\n",
      "tensor([[ 0.3374, -0.1778, -0.1690],\n",
      "        [ 0.9178,  1.5810,  1.3010],\n",
      "        [ 1.2753, -0.2010, -0.1606],\n",
      "        [-0.4015,  0.9666, -1.1481],\n",
      "        [-1.1589,  0.3255, -0.6315],\n",
      "        [-2.8400, -0.7849, -1.4096]], requires_grad=True)\n",
      ">> tensor([[ 1.2753, -0.2010, -0.1606],\n",
      "        [-0.4015,  0.9666, -1.1481],\n",
      "        [-2.8400, -0.7849, -1.4096],\n",
      "        [ 0.9178,  1.5810,  1.3010]], grad_fn=<EmbeddingBackward0>)\n"
     ]
    }
   ],
   "source": [
    "# Simple Example\n",
    "input_ids = torch.tensor([2, 3, 5, 1])\n",
    "\n",
    "vocab_size = 6\n",
    "output_dim = 3\n",
    "\n",
    "torch.manual_seed(123)\n",
    "embedding_layer = torch.nn.Embedding(vocab_size, output_dim)\n",
    "print(f\">> {embedding_layer.weight}\")\n",
    "\n",
    "print(f\">> {embedding_layer(input_ids)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.7 编码位置向量\n",
    "当token id一致时，使用同一个词嵌入会得到相同输出，如下图所示：\n",
    "\n",
    "![1716778847577](image/从零开始构建LLM/1716778847577.png)\n",
    "\n",
    "为了解决这一问题，引入了位置编码，这样可以保证每一个编码是独一无二的\n",
    "\n",
    "![1716778935403](image/从零开始构建LLM/1716778935403.png)\n",
    "\n",
    "最后，所有的数据处理流程如下：\n",
    "\n",
    "![1716778990725](image/从零开始构建LLM/1716778990725.png)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">> Token IDs:\n",
      " tensor([[   40,   367,  2885,  1464],\n",
      "        [ 1807,  3619,   402,   271],\n",
      "        [10899,  2138,   257,  7026],\n",
      "        [15632,   438,  2016,   257],\n",
      "        [  922,  5891,  1576,   438],\n",
      "        [  568,   340,   373,   645],\n",
      "        [ 1049,  5975,   284,   502],\n",
      "        [  284,  3285,   326,    11]])\n",
      ">> Inputs shape: torch.Size([8, 4])\n",
      ">> torch.Size([8, 4, 256])\n",
      ">> pos embeddings's shape: torch.Size([4, 256])\n",
      ">> input embeddings's shape: torch.Size([8, 4, 256])\n"
     ]
    }
   ],
   "source": [
    "vocab_size = 50257\n",
    "output_dim = 256\n",
    "\n",
    "token_embedding_layer = torch.nn.Embedding(vocab_size, output_dim)\n",
    "\n",
    "max_length = 4\n",
    "dataloader = create_dataloader_v1(raw_text, batch_size=8, max_length=max_length, stride=max_length, shuffle=False)\n",
    "\n",
    "data_iter = iter(dataloader)\n",
    "\n",
    "inputs, targets = next(data_iter)\n",
    "print(f\">> Token IDs:\\n {inputs}\")\n",
    "print(f\">> Inputs shape: {inputs.shape}\")\n",
    "\n",
    "token_embeddings = token_embedding_layer(inputs)\n",
    "print(f\">> {token_embeddings.shape}\")\n",
    "# >> (8, 4, 256) -> 8: batch_size, 4: max_length, 256: output_dim\n",
    "\n",
    "context_length = max_length\n",
    "pos_embedding_layer = torch.nn.Embedding(context_length, output_dim)\n",
    "pos_embeddings = pos_embedding_layer(torch.arange(context_length))\n",
    "print(f\">> pos embeddings's shape: {pos_embeddings.shape}\")\n",
    "\n",
    "input_embeddings = token_embeddings + pos_embeddings\n",
    "print(f\">> input embeddings's shape: {input_embeddings.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3 编码注意力机制\n",
    "\n",
    "主要流程如下：\n",
    "1. 一个简单的自注意力\n",
    "2. LLM中使用的注意力机制\n",
    "3. 因果关系的注意力机制\n",
    "4. 多头注意力机制\n",
    "\n",
    "![1716780422961](image/从零开始构建LLM/1716780422961.png)\n",
    "\n",
    "## 3.1 长时序建模的问题\n",
    "\n",
    "主要问题是上下文丢失。如RNN不能在解码阶段直接从编码器中访问早期的隐藏状态。因此，它只依赖于当前的隐藏状态，它封装了所有相关的信息。这可能会导致上下文的丢失，特别是在依赖关系可能跨越较长距离的复杂句子中。\n",
    "\n",
    "## 3.2 使用注意机制捕获数据依赖关系\n",
    "\n",
    "早期为了解决RNN对于长时序问题，研究者提出以下结构，被成为*Bahdanau attention*，这一机制使得解码阶段能够访问编码早期状态。\n",
    "\n",
    "![1718697183686](image/从零开始构建LLM/1718697183686.png)\n",
    "\n",
    "之后根据*Bahdanau attention*得到启发，提出了早期的*Transformer*结构。\n",
    "\n",
    "![1716877433399](image/从零开始构建LLM/1716877433399.png)\n",
    "\n",
    "## 3.3 自注意输入的不同部分\n",
    "\n",
    "自注意力是LLM中Transformer的基石。\n",
    "在自注意力中，“自我”是指该机制通过关联单个输入序列中的不同位置来计算注意权重的能力。它关注的是本身不同部分的关系和依赖。而传统的注意力机制则是关注两个序列之间的关系\n",
    "\n",
    "### 3.3.1 一个简单的自我注意机制，没有训练权重\n",
    "\n",
    "自注意的目标是为每个输入元素计算一个上下文向量，它结合了来自所有其他输入元素的信息。在自注意力中，我们的目标是为每一个输入元素${x^{(i)}}$计算上下文向量${z^{(i)}}$。一个上下文向量可以被解释为一个丰富的嵌入向量。</br>\n",
    "如下图所示，*Your journey starts with one step*为输入句子，现在关注${x^{(2)}}$与${z^{(2)}}$，${z^{(2)}}$包含了从${x^{(1)}}$到${x^{(T)}}$之间的所有信息。\n",
    "在自注意过程中，上下文向量起着至关重要的作用。它们的目的是通过在序列中合并来自所有其他元素的信息，在输入序列中（如句子）中创建每个元素的丰富表示，如下图所示。\n",
    "\n",
    "![1716879045635](image/从零开始构建LLM/1716879045635.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![1716881565809](image/从零开始构建LLM/1716881565809.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">> tensor([0.9544, 1.4950, 1.4754, 0.8434, 0.7070, 1.0865])\n"
     ]
    }
   ],
   "source": [
    "inputs = torch.tensor(\n",
    "  [[0.43, 0.15, 0.89], # Your     (x^1)\n",
    "   [0.55, 0.87, 0.66], # journey  (x^2)\n",
    "   [0.57, 0.85, 0.64], # starts   (x^3)\n",
    "   [0.22, 0.58, 0.33], # with     (x^4)\n",
    "   [0.77, 0.25, 0.10], # one      (x^5)\n",
    "   [0.05, 0.80, 0.55]] # step     (x^6)\n",
    ")\n",
    "\n",
    "query = inputs[1]  # 2nd input token is the query\n",
    "\n",
    "attn_scores_2 = torch.empty(inputs.shape[0])\n",
    "for i, x_i in enumerate(inputs):\n",
    "    attn_scores_2[i] = torch.dot(x_i, query) # dot product (transpose not necessary here since they are 1-dim vectors)\n",
    "\n",
    "print(f\">> {attn_scores_2}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> 上述操作可以理解为矩阵的乘法 dot product，其中值越大，表示相关性越高\n",
    "\n",
    "紧接着需要对其进行归一化操作"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">> attn_scores for x^2: tensor([0.1455, 0.2278, 0.2249, 0.1285, 0.1077, 0.1656])\n",
      ">> attn_scores's sum for x^2: 1.0000001192092896\n"
     ]
    }
   ],
   "source": [
    "attn_scores_2 = attn_scores_2 / attn_scores_2.sum()\n",
    "print(f\">> attn_scores for x^2: {attn_scores_2}\")\n",
    "print(f\">> attn_scores's sum for x^2: {attn_scores_2.sum()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> 在实际中，更多的是使用softmax操作，这一操作在处理极值和梯度时有更好的表现。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">> attn_weights_naive for x^2: tensor([0.1630, 0.1770, 0.1765, 0.1603, 0.1570, 0.1663])\n",
      ">> attn_weights_naive's sum for x^2: 1.0\n",
      "\n",
      ">> attn_weights for x^2: tensor([0.1630, 0.1770, 0.1765, 0.1603, 0.1570, 0.1663])\n",
      ">> attn_weights's sum for x^2: 1.0\n"
     ]
    }
   ],
   "source": [
    "def softmax_naive(x):\n",
    "    return torch.exp(x) / torch.exp(x).sum(dim=0)\n",
    "\n",
    "attn_weights_2_naive = softmax_naive(attn_scores_2)\n",
    "print(f\">> attn_weights_naive for x^2: {attn_weights_2_naive}\")\n",
    "print(f\">> attn_weights_naive's sum for x^2: {attn_weights_2_naive.sum()}\")\n",
    "print()\n",
    "\n",
    "attn_weights_2 = torch.softmax(attn_scores_2, dim=0)\n",
    "print(f\">> attn_weights for x^2: {attn_weights_2}\")\n",
    "print(f\">> attn_weights's sum for x^2: {attn_weights_2.sum()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">> context_vec: tensor([0.4325, 0.5937, 0.5349])\n"
     ]
    }
   ],
   "source": [
    "# Above All\n",
    "query = inputs[1]\n",
    "context_vec_2 = torch.zeros(query.shape)\n",
    "for i, x_i in enumerate(inputs):\n",
    "    context_vec_2 += attn_weights_2[i] * x_i\n",
    "print(f\">> context_vec: {context_vec_2}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3.2 为所有输入计算权重\n",
    "\n",
    "![1716945187106](image/从零开始构建LLM/1716945187106.png)\n",
    "\n",
    "计算流程与之前一致\n",
    "\n",
    "![1716945198064](image/从零开始构建LLM/1716945198064.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">> attn scores: tensor([[0.9995, 0.9544, 0.9422, 0.4753, 0.4576, 0.6310],\n",
      "        [0.9544, 1.4950, 1.4754, 0.8434, 0.7070, 1.0865],\n",
      "        [0.9422, 1.4754, 1.4570, 0.8296, 0.7154, 1.0605],\n",
      "        [0.4753, 0.8434, 0.8296, 0.4937, 0.3474, 0.6565],\n",
      "        [0.4576, 0.7070, 0.7154, 0.3474, 0.6654, 0.2935],\n",
      "        [0.6310, 1.0865, 1.0605, 0.6565, 0.2935, 0.9450]])\n",
      ">> attn scores: tensor([[0.9995, 0.9544, 0.9422, 0.4753, 0.4576, 0.6310],\n",
      "        [0.9544, 1.4950, 1.4754, 0.8434, 0.7070, 1.0865],\n",
      "        [0.9422, 1.4754, 1.4570, 0.8296, 0.7154, 1.0605],\n",
      "        [0.4753, 0.8434, 0.8296, 0.4937, 0.3474, 0.6565],\n",
      "        [0.4576, 0.7070, 0.7154, 0.3474, 0.6654, 0.2935],\n",
      "        [0.6310, 1.0865, 1.0605, 0.6565, 0.2935, 0.9450]])\n",
      ">> attn weights (softmax): tensor([[0.9995, 0.9544, 0.9422, 0.4753, 0.4576, 0.6310],\n",
      "        [0.9544, 1.4950, 1.4754, 0.8434, 0.7070, 1.0865],\n",
      "        [0.9422, 1.4754, 1.4570, 0.8296, 0.7154, 1.0605],\n",
      "        [0.4753, 0.8434, 0.8296, 0.4937, 0.3474, 0.6565],\n",
      "        [0.4576, 0.7070, 0.7154, 0.3474, 0.6654, 0.2935],\n",
      "        [0.6310, 1.0865, 1.0605, 0.6565, 0.2935, 0.9450]])\n"
     ]
    }
   ],
   "source": [
    "# >> attention scores\n",
    "# method 1\n",
    "attn_scores = torch.empty(6, 6)\n",
    "for i, x_i in enumerate(inputs):\n",
    "    for j, x_j in enumerate(inputs):\n",
    "        attn_scores[i, j] = torch.dot(x_i, x_j)\n",
    "print(f\">> attn scores: {attn_scores}\")\n",
    "\n",
    "# method 2\n",
    "attn_scores = torch.matmul(inputs, inputs.T)\n",
    "print(f\">> attn scores: {attn_scores}\")\n",
    "\n",
    "# >> attention weights (softmax)\n",
    "attn_weights = torch.softmax(attn_scores, dim=1)\n",
    "print(f\">> attn weights (softmax): {attn_scores}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.4 使用训练权重实现自注意力\n",
    "\n",
    "### 3.4.1 一步一步计算注意力权重\n",
    "\n",
    "这里引入了三个权重${W_q}$, ${W_k}$, ${W_v}$，这三个权重矩阵用于将输入token ${x^i}$ 映射为查询，键， 值向量。\n",
    "\n",
    "![1716947702233](image/从零开始构建LLM/1716947702233.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(1.4676)\n"
     ]
    }
   ],
   "source": [
    "x_2 = inputs[1]\n",
    "d_in = inputs.shape[1]\n",
    "d_out = 2\n",
    "\n",
    "# requires_grad=False to reduce clutter in the outputs for illustration purposes\n",
    "W_query = torch.nn.Parameter(torch.rand(d_in, d_out), requires_grad=False)\n",
    "W_key = torch.nn.Parameter(torch.rand(d_in, d_out), requires_grad=False)\n",
    "W_value = torch.nn.Parameter(torch.rand(d_in, d_out), requires_grad=False)\n",
    "\n",
    "query_2 = torch.matmul(x_2, W_query)\n",
    "key_2 = torch.matmul(x_2, W_key)\n",
    "value_2 = torch.matmul(x_2, W_value)\n",
    "\n",
    "attn_scores_22 = torch.dot(query_2, key_2)\n",
    "print(attn_scores_22)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> 权重与注意力权重的区别：</br>\n",
    "权重 ${W}$ 是指神经网络中的权重，在训练过程中被优化的部分。<br/>\n",
    "注意权重决定了上下文向量依赖于输入的不同部分的程度。<br/>\n",
    "<br/>\n",
    "总的来说，权重参数是定义神经网络的基础的、可学习的参数，而注意里权重是上下文特定的、动态的值。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">> keys shape: torch.Size([6, 2])\n",
      ">> values shape: torch.Size([6, 2])\n",
      ">> attn weights for x_2: tensor([0.1545, 0.2136, 0.2123, 0.1320, 0.1419, 0.1457])\n",
      ">> context vector for x_2: tensor([0.3341, 1.0655])\n"
     ]
    }
   ],
   "source": [
    "keys = torch.matmul(inputs, W_key)\n",
    "values = torch.matmul(inputs, W_value)\n",
    "\n",
    "print(f\">> keys shape: {keys.shape}\")\n",
    "print(f\">> values shape: {values.shape}\")\n",
    "\n",
    "attn_scores_2 = torch.matmul(query_2, keys.T)\n",
    "\n",
    "d_k = keys.shape[-1]\n",
    "attn_weights_2 = torch.softmax(attn_scores_2 / d_k ** 0.5, dim=-1)\n",
    "print(f\">> attn weights for x_2: {attn_weights_2}\")\n",
    "\n",
    "context_vec_2 = torch.matmul(attn_weights_2, values)\n",
    "print(f\">> context vector for x_2: {context_vec_2}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.4.2 实现一个紧凑的自注意类"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "\n",
    "class SelfAttention_v1(nn.Module):\n",
    "    def __init__(self, d_in, d_out):\n",
    "        super().__init__()\n",
    "        self.d_out = d_out\n",
    "        self.W_query = nn.Parameter(torch.rand(d_in, d_out))\n",
    "        self.W_key = nn.Parameter(torch.rand(d_in, d_out))\n",
    "        self.W_value = nn.Parameter(torch.rand(d_in, d_out))\n",
    "  \n",
    "    def forward(self, x):\n",
    "        keys = torch.matmul(x, self.W_key)\n",
    "        values = torch.matmul(x, self.W_value)\n",
    "        queries = torch.matmul(x, self.W_query)\n",
    "\n",
    "        attn_scores = torch.matmul(queries, keys.T)\n",
    "        attn_weights = torch.softmax(attn_scores / keys.shape[-1] ** 0.5, dim=-1)\n",
    "        context_vec = torch.matmul(attn_weights, values)\n",
    "        return context_vec"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![1717397936262](image/从零开始构建LLM/1717397936262.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">> context: tensor([[0.2996, 0.8053],\n",
      "        [0.3061, 0.8210],\n",
      "        [0.3058, 0.8203],\n",
      "        [0.2948, 0.7939],\n",
      "        [0.2927, 0.7891],\n",
      "        [0.2990, 0.8040]], grad_fn=<MmBackward0>)\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(123)\n",
    "sa_v1 = SelfAttention_v1(d_in, d_out)\n",
    "print(f\">> context: {sa_v1(inputs)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "使用 `nn.Linear` ，除了可以有效计算矩阵外，它还优化了权值初始化方案，有助于模型训练更加稳定和有效"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SelfAttention_v2(nn.Module):\n",
    "    def __init__(self, d_in, d_out, qkv_bias=False):\n",
    "        super().__init__()\n",
    "        self.d_out = d_out\n",
    "        self.W_query = nn.Linear(d_in, d_out, bias=qkv_bias)\n",
    "        self.W_key = nn.Linear(d_in, d_out, bias=qkv_bias)\n",
    "        self.W_value = nn.Linear(d_in, d_out, bias=qkv_bias)\n",
    "  \n",
    "    def forward(self, x):\n",
    "        keys = self.W_key(x)\n",
    "        queries = self.W_query(x)\n",
    "        values = self.W_value(x)\n",
    "\n",
    "        attn_scores = torch.matmul(queries, keys.T)\n",
    "        attn_weights = torch.softmax(attn_scores / keys.shape[-1] ** 0.5, dim=-1)\n",
    "        context_vec = torch.matmul(attn_weights, values)\n",
    "        return context_vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">> context: tensor([[-0.0739,  0.0713],\n",
      "        [-0.0748,  0.0703],\n",
      "        [-0.0749,  0.0702],\n",
      "        [-0.0760,  0.0685],\n",
      "        [-0.0763,  0.0679],\n",
      "        [-0.0754,  0.0693]], grad_fn=<MmBackward0>)\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(789)\n",
    "sa_v2 = SelfAttention_v2(d_in, d_out)\n",
    "print(f\">> context: {sa_v2(inputs)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.5 用因果关系的注意力来隐藏未来的词语\n",
    "\n",
    "![1717401849186](image/从零开始构建LLM/1717401849186.png)\n",
    "\n",
    "### 3.5.1 应用因果注意力掩码\n",
    "\n",
    "![1717401901352](image/从零开始构建LLM/1717401901352.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">> attn weights: tensor([[0.1921, 0.1646, 0.1652, 0.1550, 0.1721, 0.1510],\n",
      "        [0.2041, 0.1659, 0.1662, 0.1496, 0.1665, 0.1477],\n",
      "        [0.2036, 0.1659, 0.1662, 0.1498, 0.1664, 0.1480],\n",
      "        [0.1869, 0.1667, 0.1668, 0.1571, 0.1661, 0.1564],\n",
      "        [0.1830, 0.1669, 0.1670, 0.1588, 0.1658, 0.1585],\n",
      "        [0.1935, 0.1663, 0.1666, 0.1542, 0.1666, 0.1529]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      ">> mask:  tensor([[1., 0., 0., 0., 0., 0.],\n",
      "        [1., 1., 0., 0., 0., 0.],\n",
      "        [1., 1., 1., 0., 0., 0.],\n",
      "        [1., 1., 1., 1., 0., 0.],\n",
      "        [1., 1., 1., 1., 1., 0.],\n",
      "        [1., 1., 1., 1., 1., 1.]])\n",
      ">> masked:  tensor([[0.1921, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.2041, 0.1659, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.2036, 0.1659, 0.1662, 0.0000, 0.0000, 0.0000],\n",
      "        [0.1869, 0.1667, 0.1668, 0.1571, 0.0000, 0.0000],\n",
      "        [0.1830, 0.1669, 0.1670, 0.1588, 0.1658, 0.0000],\n",
      "        [0.1935, 0.1663, 0.1666, 0.1542, 0.1666, 0.1529]],\n",
      "       grad_fn=<MulBackward0>)\n",
      ">> masked norm:  tensor([[1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.5517, 0.4483, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.3800, 0.3097, 0.3103, 0.0000, 0.0000, 0.0000],\n",
      "        [0.2758, 0.2460, 0.2462, 0.2319, 0.0000, 0.0000],\n",
      "        [0.2175, 0.1983, 0.1984, 0.1888, 0.1971, 0.0000],\n",
      "        [0.1935, 0.1663, 0.1666, 0.1542, 0.1666, 0.1529]],\n",
      "       grad_fn=<DivBackward0>)\n"
     ]
    }
   ],
   "source": [
    "queries = sa_v2.W_query(inputs)\n",
    "keys = sa_v2.W_key(inputs)\n",
    "\n",
    "attn_scores = torch.matmul(queries, keys.T)\n",
    "attn_weights = torch.softmax(attn_scores / keys.shape[-1] ** 0.5, dim=-1)\n",
    "print(\">> attn weights:\", attn_weights)\n",
    "\n",
    "context_length = attn_scores.shape[0]\n",
    "mask_simple = torch.tril(torch.ones(context_length, context_length))\n",
    "print(\">> mask: \", mask_simple)\n",
    "\n",
    "masked_simple = attn_weights * mask_simple\n",
    "print(\">> masked: \", masked_simple)\n",
    "\n",
    "row_sums = torch.sum(masked_simple, dim=-1, keepdim=True)\n",
    "masked_simple_norm = masked_simple / row_sums\n",
    "print(\">> masked norm: \", masked_simple_norm)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **信息泄露**</br>\n",
    "当应用掩码时，由于计算的权重已经进行了softmax，因此会有影响。然而，当我们在mask之后重新调整注意力权重时，本质是在一个更小的子集上重新计算softmax，因此mask位置对于softmax没有贡献。</br>\n",
    "\n",
    "因此可以将流程简化为：\n",
    "\n",
    "![1717402483387](image/从零开始构建LLM/1717402483387.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">> mask:  tensor([[0., 1., 1., 1., 1., 1.],\n",
      "        [0., 0., 1., 1., 1., 1.],\n",
      "        [0., 0., 0., 1., 1., 1.],\n",
      "        [0., 0., 0., 0., 1., 1.],\n",
      "        [0., 0., 0., 0., 0., 1.],\n",
      "        [0., 0., 0., 0., 0., 0.]])\n",
      ">> masked:  tensor([[0.2899,   -inf,   -inf,   -inf,   -inf,   -inf],\n",
      "        [0.4656, 0.1723,   -inf,   -inf,   -inf,   -inf],\n",
      "        [0.4594, 0.1703, 0.1731,   -inf,   -inf,   -inf],\n",
      "        [0.2642, 0.1024, 0.1036, 0.0186,   -inf,   -inf],\n",
      "        [0.2183, 0.0874, 0.0882, 0.0177, 0.0786,   -inf],\n",
      "        [0.3408, 0.1270, 0.1290, 0.0198, 0.1290, 0.0078]],\n",
      "       grad_fn=<MaskedFillBackward0>)\n",
      ">> attn weights:  tensor([[1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.5517, 0.4483, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.3800, 0.3097, 0.3103, 0.0000, 0.0000, 0.0000],\n",
      "        [0.2758, 0.2460, 0.2462, 0.2319, 0.0000, 0.0000],\n",
      "        [0.2175, 0.1983, 0.1984, 0.1888, 0.1971, 0.0000],\n",
      "        [0.1935, 0.1663, 0.1666, 0.1542, 0.1666, 0.1529]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n"
     ]
    }
   ],
   "source": [
    "mask = torch.triu(torch.ones(context_length, context_length), diagonal=1)\n",
    "print(\">> mask: \", mask)\n",
    "\n",
    "masked = torch.masked_fill(attn_scores, mask.bool(), -torch.inf)\n",
    "print(\">> masked: \", masked)\n",
    "\n",
    "attn_weights = torch.softmax(masked / keys.shape[-1] ** 0.5, dim=1)\n",
    "print(\">> attn weights: \", attn_weights)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.5.2 用dropout来掩盖额外的注意权重\n",
    "\n",
    "在transformer架构中，dropout通常用在两个地方：计算注意力分数之后或者应用注意力权重之前\n",
    "\n",
    "![1717558177482](image/从零开始构建LLM/1717558177482.png)\n",
    "\n",
    "需要注意的是，dropout时，会将原数值进行放大，这样能够保证注意力权重的平衡。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">> dropout rate (0.5):  tensor([[2., 2., 2., 2., 2., 2.],\n",
      "        [0., 2., 0., 0., 0., 0.],\n",
      "        [0., 0., 2., 0., 2., 0.],\n",
      "        [2., 2., 0., 0., 0., 2.],\n",
      "        [2., 0., 0., 0., 0., 2.],\n",
      "        [0., 2., 0., 0., 0., 0.]])\n",
      ">> dropout attn weights:  tensor([[2.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.0000, 0.0000, 0.6206, 0.0000, 0.0000, 0.0000],\n",
      "        [0.0000, 0.4921, 0.0000, 0.4638, 0.0000, 0.0000],\n",
      "        [0.0000, 0.3966, 0.3968, 0.3775, 0.3941, 0.0000],\n",
      "        [0.3869, 0.3327, 0.0000, 0.0000, 0.3331, 0.3058]],\n",
      "       grad_fn=<MulBackward0>)\n",
      ">> dropout rate (0.1):  tensor([[1.1111, 1.1111, 1.1111, 1.1111, 1.1111, 1.1111],\n",
      "        [1.1111, 1.1111, 1.1111, 1.1111, 1.1111, 0.0000],\n",
      "        [0.0000, 1.1111, 1.1111, 1.1111, 1.1111, 1.1111],\n",
      "        [1.1111, 1.1111, 1.1111, 0.0000, 1.1111, 1.1111],\n",
      "        [1.1111, 1.1111, 0.0000, 1.1111, 1.1111, 1.1111],\n",
      "        [1.1111, 1.1111, 1.1111, 1.1111, 1.1111, 1.1111]])\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(123)\n",
    "dropout = torch.nn.Dropout(0.5)\n",
    "example = torch.ones(6, 6)\n",
    "print(\">> dropout rate (0.5): \", dropout(example))\n",
    "\n",
    "print(\">> dropout attn weights: \", dropout(attn_weights))\n",
    "\n",
    "dropout = torch.nn.Dropout(0.1)\n",
    "example = torch.ones(6, 6)\n",
    "print(\">> dropout rate (0.1): \", dropout(example))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.5.3 实现一个紧凑的因果注意力类"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CausalAttention(nn.Module):\n",
    "    def __init__(self, d_in, d_out, context_length, dropout, qkv_bias=False):\n",
    "        super().__init__()\n",
    "        self.d_out = d_out\n",
    "        self.W_query = nn.Linear(d_in, d_out, bias=qkv_bias)\n",
    "        self.W_key = nn.Linear(d_in, d_out, bias=qkv_bias)\n",
    "        self.W_value = nn.Linear(d_in, d_out, bias=qkv_bias)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.register_buffer('mask', torch.triu(torch.ones(context_length, context_length), diagonal=1))\n",
    "\n",
    "    def forward(self, x):\n",
    "        b, num_tokens, d_in = x.shape\n",
    "        keys = self.W_key(x)\n",
    "        queries = self.W_query(x)\n",
    "        values = self.W_value(x)\n",
    "\n",
    "        attn_scores = torch.matmul(queries, keys.transpose(1, 2))\n",
    "        attn_scores.masked_fill_(self.mask.bool()[:num_tokens, :num_tokens], -torch.inf)\n",
    "        attn_weights = torch.softmax(attn_scores / keys.shape[-1] ** 0.5, dim=-1)\n",
    "        attn_weights = self.dropout(attn_weights)\n",
    "        context_vec = torch.matmul(attn_weights, values)\n",
    "        return context_vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">> batch shape:  torch.Size([2, 6, 3])\n",
      ">> context_vecs:  tensor([[[-0.4519,  0.2216],\n",
      "         [-0.5874,  0.0058],\n",
      "         [-0.6300, -0.0632],\n",
      "         [-0.5675, -0.0843],\n",
      "         [-0.5526, -0.0981],\n",
      "         [-0.5299, -0.1081]],\n",
      "\n",
      "        [[-0.4519,  0.2216],\n",
      "         [-0.5874,  0.0058],\n",
      "         [-0.6300, -0.0632],\n",
      "         [-0.5675, -0.0843],\n",
      "         [-0.5526, -0.0981],\n",
      "         [-0.5299, -0.1081]]], grad_fn=<UnsafeViewBackward0>)\n",
      ">> context_vecs.shape: torch.Size([2, 6, 2])\n"
     ]
    }
   ],
   "source": [
    "batch = torch.stack((inputs, inputs), dim=0)\n",
    "print(\">> batch shape: \", batch.shape) # 2 inputs with 6 tokens each, and each token has embedding dimension 3\n",
    "\n",
    "torch.manual_seed(123)\n",
    "\n",
    "context_length = batch.shape[1]\n",
    "ca = CausalAttention(d_in, d_out, context_length, 0.0)\n",
    "\n",
    "context_vecs = ca(batch)\n",
    "\n",
    "print(\">> context_vecs: \", context_vecs)\n",
    "print(\">> context_vecs.shape:\", context_vecs.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.6 将单个注意力扩展到多头注意力\n",
    "\n",
    "### 3.6.1 将多头注意力扩展到多头注意力\n",
    "\n",
    "![1717567513041](image/从零开始构建LLM/1717567513041.png)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadAttentionWrapper(nn.Module):\n",
    "\n",
    "    def __init__(self, d_in, d_out, context_length, dropout, num_heads, qkv_bias=False):\n",
    "        super().__init__()\n",
    "        self.heads = nn.ModuleList(\n",
    "            [CausalAttention(d_in, d_out, context_length, dropout, qkv_bias) \n",
    "             for _ in range(num_heads)]\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return torch.cat([head(x) for head in self.heads], dim=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">> context_vecs: tensor([[[-0.4519,  0.2216,  0.4772,  0.1063],\n",
      "         [-0.5874,  0.0058,  0.5891,  0.3257],\n",
      "         [-0.6300, -0.0632,  0.6202,  0.3860],\n",
      "         [-0.5675, -0.0843,  0.5478,  0.3589],\n",
      "         [-0.5526, -0.0981,  0.5321,  0.3428],\n",
      "         [-0.5299, -0.1081,  0.5077,  0.3493]],\n",
      "\n",
      "        [[-0.4519,  0.2216,  0.4772,  0.1063],\n",
      "         [-0.5874,  0.0058,  0.5891,  0.3257],\n",
      "         [-0.6300, -0.0632,  0.6202,  0.3860],\n",
      "         [-0.5675, -0.0843,  0.5478,  0.3589],\n",
      "         [-0.5526, -0.0981,  0.5321,  0.3428],\n",
      "         [-0.5299, -0.1081,  0.5077,  0.3493]]], grad_fn=<CatBackward0>)\n",
      ">> context_vecs.shape: torch.Size([2, 6, 4])\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(123)\n",
    "\n",
    "context_length = batch.shape[1] # This is the number of tokens\n",
    "d_in, d_out = 3, 2\n",
    "mha = MultiHeadAttentionWrapper(\n",
    "    d_in, d_out, context_length, 0.0, num_heads=2\n",
    ")\n",
    "\n",
    "context_vecs = mha(batch)\n",
    "\n",
    "print(\">> context_vecs:\", context_vecs)\n",
    "print(\">> context_vecs.shape:\", context_vecs.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.6.2 通过权重分割实现多头注意力"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(self, d_in, d_out, context_length, dropout, num_heads, qkv_bias=False) -> None:\n",
    "        super().__init__()\n",
    "        assert (d_out % num_heads == 0), \"d_out must be divisible by num_heads\"\n",
    "\n",
    "        self.d_out = d_out\n",
    "        self.num_heads = num_heads\n",
    "        self.head_dim = d_out // num_heads # Reduce the projection dim to match desired output dim\n",
    "\n",
    "        self.W_query = nn.Linear(d_in, d_out, bias=qkv_bias)\n",
    "        self.W_key = nn.Linear(d_in, d_out, bias=qkv_bias)\n",
    "        self.W_value = nn.Linear(d_in, d_out, bias=qkv_bias)\n",
    "        self.out_proj = nn.Linear(d_out, d_out)  # Linear layer to combine head outputs\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.register_buffer(\n",
    "            \"mask\",\n",
    "            torch.triu(torch.ones(context_length, context_length),\n",
    "                       diagonal=1)\n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        b, num_tokens, d_in = x.shape\n",
    "\n",
    "        keys = self.W_key(x) # Shape: (b, num_tokens, d_out)\n",
    "        queries = self.W_query(x)\n",
    "        values = self.W_value(x)\n",
    "\n",
    "        keys = keys.view(b, num_tokens, self.num_heads, self.head_dim)\n",
    "        queries = queries.view(b, num_tokens, self.num_heads, self.head_dim)\n",
    "        values = values.view(b, num_tokens, self.num_heads, self.head_dim)\n",
    "\n",
    "        keys = keys.transpose(1, 2)\n",
    "        queries = queries.transpose(1, 2)\n",
    "        values = values.transpose(1, 2)\n",
    "\n",
    "        attn_scores = torch.matmul(queries, keys.transpose(2, 3))\n",
    "\n",
    "        mask_bool = self.mask.bool()[:num_tokens, :num_tokens]\n",
    "\n",
    "        attn_scores.masked_fill_(mask_bool, -torch.inf)\n",
    "\n",
    "        attn_weights = torch.softmax(attn_scores / keys.shape[-1]**0.5, dim=-1)\n",
    "        attn_weights = self.dropout(attn_weights)\n",
    "        \n",
    "        context_vec = torch.matmul(attn_weights, values).transpose(1, 2)\n",
    "\n",
    "        context_vec = context_vec.contiguous().view(b, num_tokens, self.d_out)\n",
    "        context_vec = self.out_proj(context_vec)\n",
    "\n",
    "        return context_vec"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![1718186662363](image/从零开始构建LLM/1718186662363.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">> context_vecs: tensor([[[0.3190, 0.4858],\n",
      "         [0.2943, 0.3897],\n",
      "         [0.2856, 0.3593],\n",
      "         [0.2693, 0.3873],\n",
      "         [0.2639, 0.3928],\n",
      "         [0.2575, 0.4028]],\n",
      "\n",
      "        [[0.3190, 0.4858],\n",
      "         [0.2943, 0.3897],\n",
      "         [0.2856, 0.3593],\n",
      "         [0.2693, 0.3873],\n",
      "         [0.2639, 0.3928],\n",
      "         [0.2575, 0.4028]]], grad_fn=<ViewBackward0>)\n",
      ">> context_vecs.shape: torch.Size([2, 6, 2])\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(123)\n",
    "\n",
    "batch_size, context_length, d_in = batch.shape\n",
    "d_out = 2\n",
    "mha = MultiHeadAttention(d_in, d_out, context_length, 0.0, num_heads=2)\n",
    "\n",
    "context_vecs = mha(batch)\n",
    "\n",
    "print(\">> context_vecs:\", context_vecs)\n",
    "print(\">> context_vecs.shape:\", context_vecs.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4 实现GPT并生成文本\n",
    "\n",
    "## 4.1 实现一个LLM结构\n",
    "\n",
    "LLM总体框架图如下：\n",
    "1. 词嵌入\n",
    "2. 多头注意力\n",
    "3. 输出层\n",
    "\n",
    "![1718247386638](image/从零开始构建LLM/1718247386638.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# GPT-2 parameter\n",
    "GPT_CONFIG_124M = {\n",
    "    \"vocab_size\": 50257,    # Vocabulary size\n",
    "    \"context_length\": 1024, # Context length\n",
    "    \"emb_dim\": 768,         # Embedding dimension\n",
    "    \"n_heads\": 12,          # Number of attention heads\n",
    "    \"n_layers\": 12,         # Number of layers\n",
    "    \"drop_rate\": 0.1,       # Dropout rate\n",
    "    \"qkv_bias\": False       # Query-Key-Value bias\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> 根据下图，一步一步编写GPT模型\n",
    "\n",
    "![1718258629553](image/从零开始构建LLM/1718258629553.png)\n",
    "\n",
    "编写代码如下所示，但是并没有编写归一化与具体的Transformer块。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DummyGPTModel(nn.Module):\n",
    "    def __init__(self, cfg):\n",
    "        super().__init__()\n",
    "        self.tok_emb = nn.Embedding(cfg[\"vocab_size\"], cfg[\"emb_dim\"])\n",
    "        self.pos_emb = nn.Embedding(cfg[\"context_length\"], cfg[\"emb_dim\"])\n",
    "        self.drop_emb = nn.Dropout(cfg[\"drop_rate\"])\n",
    "\n",
    "        self.trf_blocks = nn.Sequential(*[DummyTransformerBlock(cfg) for _ in range(cfg[\"n_layers\"])])\n",
    "\n",
    "        self.final_norm = DummyLayerNorm(cfg[\"emb_dim\"])\n",
    "        self.out_head = nn.Linear(cfg[\"emb_dim\"], cfg[\"vocab_size\"], bias=False)\n",
    "  \n",
    "    def forward(self, in_idx):\n",
    "        batch_size, seq_len = in_idx.shape\n",
    "        tok_embeds = self.tok_emb(in_idx)\n",
    "        pos_embeds = self.pos_emb(torch.arange(seq_len, device=in_idx.device))\n",
    "        x = tok_embeds + pos_embeds\n",
    "        x = self.trf_blocks(x)\n",
    "        x = self.final_norm(x)\n",
    "        logits = self.out_head(x)\n",
    "        return logits\n",
    "\n",
    "\n",
    "class DummyTransformerBlock(nn.Module):\n",
    "    def __init__(self, cfg):\n",
    "        super().__init__()\n",
    "  \n",
    "    def forward(self, x):\n",
    "        return x\n",
    "\n",
    "\n",
    "class DummyLayerNorm(nn.Module):\n",
    "    def __init__(self, normalized_shape, eps=1e-5) -> None:\n",
    "        super().__init__()\n",
    "  \n",
    "    def forward(self, x):\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> GPT数据流\n",
    "\n",
    "![1718262314570](image/从零开始构建LLM/1718262314570.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">> batch:  tensor([[6109, 3626, 6100,  345],\n",
      "        [6109, 1110, 6622,  257]])\n",
      ">> out shape:  torch.Size([2, 4, 50257])\n",
      ">> out:  tensor([[[-1.1947,  0.1392, -0.8616,  ..., -1.4987, -0.0314, -0.4490],\n",
      "         [ 0.0497,  0.3861, -0.3281,  ..., -0.1826,  1.3084,  0.9867],\n",
      "         [ 0.7005,  1.4747, -0.4149,  ...,  1.7756, -0.2280,  0.5384],\n",
      "         [ 0.4885,  1.7545, -0.6707,  ...,  1.1501, -0.1143, -0.9368]],\n",
      "\n",
      "        [[-1.1947,  0.1392, -0.8616,  ..., -1.4987, -0.0314, -0.4490],\n",
      "         [-0.5591,  0.5797, -0.1296,  ...,  0.2691,  0.3151,  1.4046],\n",
      "         [ 0.8524,  1.2833, -0.1786,  ..., -0.1982,  0.1097,  0.2812],\n",
      "         [-0.0190, -0.8277,  0.2299,  ...,  1.7974, -0.1646, -0.1049]]],\n",
      "       grad_fn=<UnsafeViewBackward0>)\n"
     ]
    }
   ],
   "source": [
    "import tiktoken\n",
    "tokenizer = tiktoken.get_encoding(\"gpt2\")\n",
    "batch = []\n",
    "txt1 = \"Every effort moves you\"\n",
    "txt2 = \"Every day holds a\"\n",
    "\n",
    "batch.append(torch.tensor(tokenizer.encode(txt1)))\n",
    "batch.append(torch.tensor(tokenizer.encode(txt2)))\n",
    "batch = torch.stack(batch, dim=0)\n",
    "print(\">> batch: \", batch)\n",
    "\n",
    "torch.manual_seed(123)\n",
    "model = DummyGPTModel(GPT_CONFIG_124M)\n",
    "logits = model(batch)\n",
    "print(\">> out shape: \", logits.shape)\n",
    "print(\">> out: \", logits)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.2 使用layer normalization进行归一化激活\n",
    "\n",
    "> 由于梯度消失或爆炸等问题，训练多层深度神经网络有时会具有挑战性。这些问题导致了不稳定的训练动态，使网络难以有效地调整其权值，这意味着学习过程很难为神经网络找到一组参数（权值），以最小化损失函数。换句话说，该网络很难学习数据中的潜在模式，其程度将使其能够做出准确的预测或决策。</br>\n",
    "\n",
    "> 层归一化背后的主要思想是调整神经网络层的激活（输出），使其均值为0，方差为1，也称为单位方差。这种调整加速了收敛到有效的权重，并确保了一致、可靠的训练。</br>\n",
    "\n",
    "> **层归一化通常在多头注意模块前后和最终输出层之前应用。**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# torch.set_printoptions(sci_mode=True)  # set float number\n",
    "\n",
    "class LayerNorm(nn.Module):\n",
    "    def __init__(self, emb_dim) -> None:\n",
    "        super().__init__()\n",
    "        self.eps = 1e-5\n",
    "        self.scale = nn.Parameter(torch.ones(emb_dim))\n",
    "        self.shift = nn.Parameter(torch.zeros(emb_dim))\n",
    "  \n",
    "    def forward(self, x):\n",
    "        mean = x.mean(dim=-1, keepdim=True)\n",
    "        var = x.var(dim=-1, keepdim=True, unbiased=False)\n",
    "        norm_x = (x - mean) / torch.sqrt(var + self.eps)\n",
    "        return self.scale * norm_x + self.shift"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.3 使用层归一化实现前向神经网络\n",
    "\n",
    "**GeLU**激活函数\n",
    "\n",
    "在神经网络中，使用最广泛的是ReLU函数，但是在LLM，除了ReLU外，还有两种显著的激活函数：GELU (Gaussian Error Linear Unit) 和 SwiGLU (Sigmoid-Weighted Linear Unit)。GELU和SwiGLU分别是更复杂和光滑的包含高斯单元和s型门控线性单位的激活函数。他们可以表现的更好。\n",
    "\n",
    "$$\n",
    "\\text{GELU}(x) \\approx 0.5 \\cdot x \\cdot \\left(1 + \\tanh\\left[\\sqrt{\\frac{2}{\\pi}} \\cdot \\left(x + 0.044715 \\cdot x^3\\right)\\right]\\right)\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GELU(nn.Module):\n",
    "    def __init__(self) -> None:\n",
    "        super().__init__()\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return 0.5 * x * ( 1 + torch.tanh(torch.sqrt(torch.tensor(2 / torch.pi)) * (x + 0.044715 * torch.pow(x, 3))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 800x300 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# GELU Test\n",
    "import matplotlib.pyplot as plt\n",
    "gelu, relu = GELU(), nn.ReLU()\n",
    "x = torch.linspace(-3, 3, 100)\n",
    "y_gelu, y_relu = gelu(x), relu(x)\n",
    "plt.figure(figsize=(8, 3))\n",
    "for i, (y, label) in enumerate(zip([y_gelu, y_relu], ['GELU', \"ReLU\"])):\n",
    "    plt.subplot(1, 2, i+1)\n",
    "    plt.plot(x, y)\n",
    "    plt.title(f\"{label} activation function\")\n",
    "    plt.xlabel('x')\n",
    "    plt.ylabel(f\"{label}(x)\")\n",
    "    plt.grid()\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> 在此基础上实现一个前馈网络，这个前馈网络至关重要，主要解决非线性问题，并且可以探索更丰富的空间。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FeedForward(nn.Module):\n",
    "    def __init__(self, cfg):\n",
    "        super().__init__()\n",
    "        self.layer = nn.Sequential(\n",
    "            nn.Linear(cfg[\"emb_dim\"], 4 * cfg[\"emb_dim\"]),\n",
    "            GELU(),\n",
    "            nn.Linear(4 * cfg[\"emb_dim\"], cfg[\"emb_dim\"])\n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.layer(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">> ffn out size:  torch.Size([2, 3, 768])\n"
     ]
    }
   ],
   "source": [
    "ffn = FeedForward(GPT_CONFIG_124M)\n",
    "x = torch.rand(2, 3, 768)\n",
    "out = ffn(x)\n",
    "print(\">> ffn out size: \", out.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.4 增加短连接\n",
    "\n",
    "short connection通过跳过一个或多个层来创建一个梯度通过网络的更短路径，这是通过将一个层的输出添加到后面一个层的输出来实现的。这就是为什么这些连接也被称为跳过连接。在训练过程中，它们在保持梯度的流动方面起着至关重要的作用。\n",
    "\n",
    "![1718273582034](image/从零开始构建LLM/1718273582034.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ExampleDeepNeuralNetwork(nn.Module):\n",
    "    def __init__(self, layer_sizes, use_shortcut) -> None:\n",
    "        super().__init__()\n",
    "        self.use_shortcut = use_shortcut\n",
    "        self.layers = nn.ModuleList([\n",
    "            nn.Sequential(nn.Linear(layer_sizes[0], layer_sizes[1]), GELU()),\n",
    "            nn.Sequential(nn.Linear(layer_sizes[1], layer_sizes[2]), GELU()),\n",
    "            nn.Sequential(nn.Linear(layer_sizes[2], layer_sizes[3]), GELU()),\n",
    "            nn.Sequential(nn.Linear(layer_sizes[3], layer_sizes[4]), GELU()),\n",
    "            nn.Sequential(nn.Linear(layer_sizes[4], layer_sizes[5]), GELU()),\n",
    "        ])\n",
    "    \n",
    "    def forward(self, x):\n",
    "        for layer in self.layers:\n",
    "            layer_output = layer(x)\n",
    "            if self.use_shortcut and x.shape == layer_output.shape:\n",
    "                x = x + layer_output\n",
    "            else:\n",
    "                x = layer_output\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "layer_sizes = [3, 3, 3, 3, 3, 1]\n",
    "sample_input = torch.tensor([[1., 0., -1.]])\n",
    "model_without_shorcut = ExampleDeepNeuralNetwork(layer_sizes, False)\n",
    "\n",
    "def print_gradients(model, x):\n",
    "    output = model(x)\n",
    "    target = torch.tensor([[0.]])\n",
    "\n",
    "    loss = nn.MSELoss()\n",
    "    loss = loss(output, target)\n",
    "    loss.backward()\n",
    "\n",
    "    for name, param in model.named_parameters():\n",
    "        if 'weight' in name:\n",
    "            print(f\">> {name} has gradient mean of {param.grad.abs()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">> layers.0.0.weight has gradient mean of tensor([[0.0018, 0.0000, 0.0018],\n",
      "        [0.0010, 0.0000, 0.0010],\n",
      "        [0.0065, 0.0000, 0.0065]])\n",
      ">> layers.1.0.weight has gradient mean of tensor([[6.3482e-07, 3.8762e-07, 7.6243e-06],\n",
      "        [1.5987e-04, 9.7618e-05, 1.9201e-03],\n",
      "        [8.4790e-05, 5.1773e-05, 1.0183e-03]])\n",
      ">> layers.2.0.weight has gradient mean of tensor([[0.0039, 0.0031, 0.0015],\n",
      "        [0.0041, 0.0033, 0.0015],\n",
      "        [0.0050, 0.0040, 0.0019]])\n",
      ">> layers.3.0.weight has gradient mean of tensor([[0.0491, 0.0031, 0.0283],\n",
      "        [0.0257, 0.0016, 0.0148],\n",
      "        [0.0465, 0.0029, 0.0268]])\n",
      ">> layers.4.0.weight has gradient mean of tensor([[0.0417, 0.0856, 0.0110]])\n"
     ]
    }
   ],
   "source": [
    "print_gradients(model_without_shorcut, sample_input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">> layers.0.0.weight has gradient mean of tensor([[3.2044e-06, 0.0000e+00, 3.2044e-06],\n",
      "        [1.8830e-03, 0.0000e+00, 1.8830e-03],\n",
      "        [3.0898e-03, 0.0000e+00, 3.0898e-03]])\n",
      ">> layers.1.0.weight has gradient mean of tensor([[1.5119e-04, 1.8919e-05, 1.4878e-04],\n",
      "        [8.6263e-04, 1.0795e-04, 8.4891e-04],\n",
      "        [2.0144e-04, 2.5207e-05, 1.9824e-04]])\n",
      ">> layers.2.0.weight has gradient mean of tensor([[2.1789e-04, 6.5373e-06, 2.5559e-04],\n",
      "        [1.2170e-03, 3.6514e-05, 1.4276e-03],\n",
      "        [4.6673e-05, 1.4003e-06, 5.4749e-05]])\n",
      ">> layers.3.0.weight has gradient mean of tensor([[1.8056e-04, 2.5326e-05, 2.2459e-04],\n",
      "        [3.3904e-03, 4.7555e-04, 4.2170e-03],\n",
      "        [1.9221e-03, 2.6960e-04, 2.3907e-03]])\n",
      ">> layers.4.0.weight has gradient mean of tensor([[0.0184, 0.0108, 0.0193]])\n"
     ]
    }
   ],
   "source": [
    "model_without_shorcut = ExampleDeepNeuralNetwork(layer_sizes, True)\n",
    "print_gradients(model_without_shorcut, sample_input)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.5 连接注意力层和线性层\n",
    "\n",
    "**transformer block的说明**\n",
    "\n",
    "![1718347509428](image/从零开始构建LLM/1718347509428.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerBlock(nn.Module):\n",
    "    def __init__(self, cfg) -> None:\n",
    "        super().__init__()\n",
    "        self.att = MultiHeadAttention(d_in=cfg[\"emb_dim\"], \n",
    "                                      d_out=cfg[\"emb_dim\"],\n",
    "                                      context_length=cfg[\"context_length\"], \n",
    "                                      num_heads=cfg[\"n_heads\"],\n",
    "                                      dropout=cfg[\"drop_rate\"],\n",
    "                                      qkv_bias=cfg[\"qkv_bias\"])\n",
    "        self.ff = FeedForward(cfg)\n",
    "        self.norm1 = LayerNorm(cfg[\"emb_dim\"])\n",
    "        self.norm2 = LayerNorm(cfg[\"emb_dim\"])\n",
    "        self.drop_resid = nn.Dropout(cfg[\"drop_rate\"])\n",
    "    \n",
    "    def forward(self, x):\n",
    "        shortcut = x\n",
    "        x = self.norm1(x)\n",
    "        x = self.att(x)\n",
    "        x = self.drop_resid(x)\n",
    "        x = x + shortcut\n",
    "\n",
    "        shortcut = x\n",
    "\n",
    "        x = self.norm2(x)\n",
    "        x = self.ff(x)\n",
    "        x = self.drop_resid(x)\n",
    "        x = x + shortcut\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.6 编码GPT模型\n",
    "\n",
    "![1718352064601](image/从零开始构建LLM/1718352064601.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GPTModel(nn.Module):\n",
    "    def __init__(self, cfg):\n",
    "        super().__init__()\n",
    "        self.tok_emb = nn.Embedding(cfg[\"vocab_size\"], cfg[\"emb_dim\"])\n",
    "        self.pos_emb = nn.Embedding(cfg[\"context_length\"], cfg[\"emb_dim\"])\n",
    "        self.drop_emb = nn.Dropout(cfg[\"drop_rate\"])\n",
    "        \n",
    "        self.trf_blocks = nn.Sequential(\n",
    "            *[TransformerBlock(cfg) for _ in range(cfg[\"n_layers\"])])\n",
    "        \n",
    "        self.final_norm = LayerNorm(cfg[\"emb_dim\"])\n",
    "        self.out_head = nn.Linear(\n",
    "            cfg[\"emb_dim\"], cfg[\"vocab_size\"], bias=False\n",
    "        )\n",
    "\n",
    "    def forward(self, in_idx):\n",
    "        batch_size, seq_len = in_idx.shape\n",
    "        tok_embeds = self.tok_emb(in_idx)\n",
    "        pos_embeds = self.pos_emb(torch.arange(seq_len, device=in_idx.device))\n",
    "        x = tok_embeds + pos_embeds  # Shape [batch_size, num_tokens, emb_size]\n",
    "        x = self.drop_emb(x)\n",
    "        x = self.trf_blocks(x)\n",
    "        x = self.final_norm(x)\n",
    "        logits = self.out_head(x)\n",
    "        return logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">> input batch:  tensor([[6109, 3626, 6100,  345],\n",
      "        [6109, 1110, 6622,  257]])\n",
      ">> output shape:  torch.Size([2, 4, 50257])\n",
      ">> out:  tensor([[[ 0.2159,  1.7803, -1.3144,  ...,  0.1518,  0.3433, -0.2426],\n",
      "         [-0.4640,  0.0124, -0.0818,  ...,  0.1752,  0.8148,  0.2054],\n",
      "         [-0.3823,  0.1988,  0.2054,  ...,  0.0659, -0.1415,  0.1057],\n",
      "         [-0.1542,  0.7591, -0.2797,  ..., -0.2908,  0.6896, -0.1746]],\n",
      "\n",
      "        [[-0.2024,  1.6316, -0.9047,  ...,  0.1310,  0.5152, -0.3395],\n",
      "         [-0.0648,  0.8608, -0.4911,  ...,  0.5268, -0.0134,  0.3010],\n",
      "         [-0.1856, -0.6939,  0.0868,  ...,  0.5741, -0.0706,  0.2637],\n",
      "         [-0.0827, -0.1011,  0.4543,  ...,  0.1231, -0.1420,  0.0325]]],\n",
      "       grad_fn=<UnsafeViewBackward0>)\n"
     ]
    }
   ],
   "source": [
    "model = GPTModel(GPT_CONFIG_124M)\n",
    "\n",
    "out = model(batch)\n",
    "print(\">> input batch: \", batch)\n",
    "print(\">> output shape: \", out.shape)\n",
    "print(\">> out: \", out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">> total number of parameters: 163009536\n"
     ]
    }
   ],
   "source": [
    "total_params = sum(p.numel() for p in model.parameters())\n",
    "print(f\">> total number of parameters: {total_params}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> 之前提到代码参数工1.24亿，但是为何这里输出为1.63亿呢？<br/>\n",
    "其原因是在原始的GPT-2体系结构中使用了一个称为**权重绑定**的概念，这意味着原始的GPT-2体系结构在其输出层中重用了来自标记嵌入层的权重。为了理解这意味着什么，来看看我们之前通过GPTModel在模型上初始化的令牌嵌入层和线性输出层的形状："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">> Token embedding layer shape: torch.Size([50257, 768])\n",
      ">> Output layer shape: torch.Size([50257, 768])\n"
     ]
    }
   ],
   "source": [
    "print(f\">> Token embedding layer shape: {model.tok_emb.weight.shape}\")\n",
    "print(f\">> Output layer shape: {model.out_head.weight.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> 由于字典表的大小为50257，这导致嵌入层非常大。且输出层重用了嵌入层的权重，因此应减去这一部分的参数。\n",
    "\n",
    "> 在一般情况下，不是用权重绑定。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">> Number of trainable parameters considering weight tying: 124412160\n"
     ]
    }
   ],
   "source": [
    "total_params_gpt2 = total_params - sum(p.numel() for p in model.out_head.parameters())\n",
    "print(f\">> Number of trainable parameters considering weight tying: {total_params_gpt2}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total size of the model: 621.83 MB\n"
     ]
    }
   ],
   "source": [
    "total_size_bytes = total_params * 4\n",
    "total_size_mb = total_size_bytes / (1024 * 1024)\n",
    "print(f\"Total size of the model: {total_size_mb:.2f} MB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.7 生成文本\n",
    "\n",
    "![1718358855510](image/从零开始构建LLM/1718358855510.png)\n",
    "\n",
    "![1718358887750](image/从零开始构建LLM/1718358887750.png)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_text_simple(model, idx, max_new_tokens, context_size):\n",
    "    for _ in range(max_new_tokens):\n",
    "        idx_cond = idx[:, -context_size:]\n",
    "        with torch.no_grad():\n",
    "            logits = model(idx_cond)\n",
    "        logits = logits[:, -1, :]\n",
    "        probas = torch.softmax(logits, dim=-1)\n",
    "        idx_next = torch.argmax(probas, dim=-1, keepdim=True)\n",
    "        idx = torch.cat((idx, idx_next), dim=-1)\n",
    "    return idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">> encoded: [15496, 11, 314, 716]\n",
      ">> encoded_tensor.shape: torch.Size([1, 4])\n"
     ]
    }
   ],
   "source": [
    "start_context = \"Hello, I am\"\n",
    "encoded = tokenizer.encode(start_context)\n",
    "print(\">> encoded:\", encoded)\n",
    "encoded_tensor = torch.tensor(encoded).unsqueeze(0)\n",
    "print(\">> encoded_tensor.shape:\", encoded_tensor.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">> Output: tensor([[15496,    11,   314,   716, 17480, 23268,  2497, 19749,  1333, 15262]])\n",
      ">> Output length: 10\n"
     ]
    }
   ],
   "source": [
    "model.eval()\n",
    "out = generate_text_simple(\n",
    "    model=model,\n",
    "    idx=encoded_tensor,\n",
    "    max_new_tokens=6,\n",
    "    context_size=GPT_CONFIG_124M[\"context_length\"]\n",
    ")\n",
    "print(\">> Output:\", out)\n",
    "print(\">> Output length:\", len(out[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">> Generated text:  Hello, I amINGTON vow saw bourgeois triBay\n"
     ]
    }
   ],
   "source": [
    "decoded_text = tokenizer.decode(out.squeeze(0).tolist())\n",
    "print(\">> Generated text: \", decoded_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5 在未标记的数据上进行训练\n",
    "\n",
    "## 5.1 评估生成文本模型\n",
    "\n",
    "![1718851702204](image/从零开始构建LLM/1718851702204.png)\n",
    "\n",
    "5.1.1 使用GPT模型生成文本\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GPTModel(\n",
       "  (tok_emb): Embedding(50257, 768)\n",
       "  (pos_emb): Embedding(256, 768)\n",
       "  (drop_emb): Dropout(p=0.1, inplace=False)\n",
       "  (trf_blocks): Sequential(\n",
       "    (0): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layer): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_resid): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (1): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layer): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_resid): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (2): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layer): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_resid): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (3): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layer): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_resid): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (4): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layer): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_resid): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (5): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layer): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_resid): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (6): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layer): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_resid): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (7): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layer): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_resid): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (8): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layer): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_resid): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (9): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layer): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_resid): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (10): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layer): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_resid): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (11): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layer): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_resid): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "  )\n",
       "  (final_norm): LayerNorm()\n",
       "  (out_head): Linear(in_features=768, out_features=50257, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "GPT_CONFIG_124M = {\n",
    "    \"vocab_size\": 50257,\n",
    "    \"context_length\": 256, #A\n",
    "    \"emb_dim\": 768,\n",
    "    \"n_heads\": 12,\n",
    "    \"n_layers\": 12,\n",
    "    \"drop_rate\": 0.1, #B\n",
    "    \"qkv_bias\": False\n",
    "}\n",
    "model = GPTModel(GPT_CONFIG_124M)\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "与之前的进行对比，context_length减少到256（之前为1024），这一改变减少了计算需求，使得能够在桌面版计算机上运行。\n",
    "在之后的训练中，将其更新回1024，以便于加载预训练模型。\n",
    "\n",
    "---\n",
    "\n",
    "三步生成文本的过程：\n",
    "1. tokenizer将输入文本转换为一系列token IDs\n",
    "2. 模型将token IDs转换为对应的logits，logits表示每个token在字典中可能的分布状况\n",
    "3. 将logits转换为token IDs，tokenizer再进行解码，生成文本\n",
    "\n",
    "![1718853446792](image/从零开始构建LLM/1718853446792.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">> encoded: tensor([[6109, 3626, 6100,  345]])\n",
      ">> decoded_text: Every effort moves you Ya Primary Haleifacts rallying racing acronym employedliners quasi\n"
     ]
    }
   ],
   "source": [
    "def text_to_token_ids(text, tokenizer):\n",
    "    encoded = tokenizer.encode(text, allowed_special={'<|endoftext|>'})\n",
    "    encoded_tensor = torch.tensor(encoded).unsqueeze(0)\n",
    "    return encoded_tensor\n",
    "\n",
    "def token_ids_to_text(token_ids, tokenizer):\n",
    "    decoded = tokenizer.decode(token_ids.squeeze(0).tolist())\n",
    "    return decoded\n",
    "\n",
    "start_context = \"Every effort moves you\"\n",
    "tokenizer = tiktoken.get_encoding(\"gpt2\")\n",
    "\n",
    "encoded = text_to_token_ids(start_context, tokenizer)\n",
    "print(\">> encoded:\", encoded)\n",
    "\n",
    "token_ids = generate_text_simple(model=model, idx=encoded, max_new_tokens=10, context_size=GPT_CONFIG_124M['context_length'])\n",
    "\n",
    "decoded_text = token_ids_to_text(token_ids, tokenizer)\n",
    "print(\">> decoded_text:\", decoded_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.1.2 计算文本生成的损失\n",
    "\n",
    "![1718854323771](image/从零开始构建LLM/1718854323771.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">> probas:  tensor([[[2.8377e-05, 7.2349e-06, 1.2548e-05,  ..., 5.9388e-05,\n",
      "          1.3404e-05, 1.2288e-05],\n",
      "         [1.3495e-05, 1.4862e-05, 2.8429e-05,  ..., 1.8806e-05,\n",
      "          1.5693e-05, 4.4732e-05],\n",
      "         [1.2664e-05, 1.3142e-05, 1.0387e-05,  ..., 4.5830e-05,\n",
      "          2.7469e-05, 6.0023e-06]],\n",
      "\n",
      "        [[1.3310e-05, 1.8276e-05, 2.6471e-05,  ..., 4.4993e-05,\n",
      "          1.4153e-05, 2.1337e-05],\n",
      "         [3.9846e-05, 1.4927e-05, 2.0788e-05,  ..., 1.3040e-05,\n",
      "          1.7489e-05, 4.1804e-05],\n",
      "         [1.8740e-05, 1.7283e-05, 1.5547e-05,  ..., 3.6920e-05,\n",
      "          2.2174e-05, 8.8161e-06]]])\n",
      ">> token_ids:  tensor([[[26729],\n",
      "         [ 6858],\n",
      "         [40186]],\n",
      "\n",
      "        [[11205],\n",
      "         [25483],\n",
      "         [38800]]])\n",
      ">> target batch:   effort moves you\n",
      ">> predicted batch:   Theme Andrew sluggish\n"
     ]
    }
   ],
   "source": [
    "inputs = torch.tensor([[16833, 3626, 6100],   # [\"every effort moves\",\n",
    "                       [40,    1107, 588]])   #  \"I really like\"]\n",
    "\n",
    "targets = torch.tensor([[3626, 6100, 345  ],  # [\" effort moves you\",\n",
    "                        [1107,  588, 11311]]) #  \" really like chocolate\"]\n",
    "\n",
    "with torch.no_grad():\n",
    "    logits = model(inputs)\n",
    "\n",
    "probas = torch.softmax(logits, dim=-1)\n",
    "print(\">> probas: \", probas)\n",
    "\n",
    "token_ids = torch.argmax(probas, dim=-1, keepdim=True)\n",
    "print(\">> token_ids: \", token_ids)\n",
    "\n",
    "print(\">> target batch: \", token_ids_to_text(targets[0], tokenizer))\n",
    "print(\">> predicted batch: \", token_ids_to_text(token_ids[0].flatten(), tokenizer))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "由于还未被训练，因此产生的为随机文本。\n",
    "训练过程是不断减小目标与预测值之间的“距离”。\n",
    "\n",
    "![1718868480909](image/从零开始构建LLM/1718868480909.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">> Text 1: tensor([1.2227e-05, 1.6803e-05, 1.2385e-05])\n",
      ">> Text 2: tensor([1.2182e-05, 1.3089e-05, 4.1397e-06])\n",
      ">> log probas:  tensor([-11.3119, -10.9940, -11.2990, -11.3156, -11.2437, -12.3949])\n",
      ">> avg log probas:  tensor(-11.4265)\n",
      ">> neg avg log probas:  tensor(11.4265)\n"
     ]
    }
   ],
   "source": [
    "# 2-3\n",
    "text_idx = 0\n",
    "target_probas_1 = probas[text_idx, [0, 1, 2], targets[text_idx]]\n",
    "print(\">> Text 1:\", target_probas_1)\n",
    "\n",
    "text_idx = 1\n",
    "target_probas_2 = probas[text_idx, [0, 1, 2], targets[text_idx]]\n",
    "print(\">> Text 2:\", target_probas_2)\n",
    "\n",
    "# 4\n",
    "log_probas = torch.log(torch.cat((target_probas_1, target_probas_2)))\n",
    "print(\">> log probas: \", log_probas)\n",
    "\n",
    "# 5\n",
    "avg_log_probas = torch.mean(log_probas)\n",
    "print(\">> avg log probas: \", avg_log_probas)\n",
    "\n",
    "# 6\n",
    "neg_avg_log_probas = -avg_log_probas\n",
    "print(\">> neg avg log probas: \", neg_avg_log_probas)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "交叉熵 Cross Entropy Loss， 是用来衡量两个概率分布之间的差异"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">> Logits shape: torch.Size([2, 3, 50257])\n",
      ">> Targets shape: torch.Size([2, 3])\n"
     ]
    }
   ],
   "source": [
    "# Logits have shape (batch_size, num_tokens, vocab_size)\n",
    "print(\">> Logits shape:\", logits.shape)\n",
    "\n",
    "# Targets have shape (batch_size, num_tokens)\n",
    "print(\">> Targets shape:\", targets.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "在进行交叉熵之前，需要检查向量维度"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">> Flattened logits: torch.Size([6, 50257])\n",
      ">> Flattened targets: torch.Size([6])\n"
     ]
    }
   ],
   "source": [
    "logits_flat = logits.flatten(0, 1)\n",
    "targets_flat = targets.flatten()\n",
    "\n",
    "print(\">> Flattened logits:\", logits_flat.shape)\n",
    "print(\">> Flattened targets:\", targets_flat.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">> Loss:  tensor(11.4265)\n"
     ]
    }
   ],
   "source": [
    "loss = nn.functional.cross_entropy(logits_flat, targets_flat)\n",
    "print(\">> Loss: \", loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> 困惑度</br>\n",
    "\n",
    "困惑度也是评价语言模型好坏的指标。它可以提供一种更可解释的方法来理解模型在预测序列中的下一个标记时的不确定性。</br>\n",
    "困惑度衡量了模型预测的概率分布与数据集中单词的实际分布的匹配程度。与损失相似，较低的困惑度表明模型的预测更接近实际分布。</br>\n",
    "困惑度的可解释性在于它表示模型在每一步中都不确定的有效词汇表大小。</br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(91720.6797)\n"
     ]
    }
   ],
   "source": [
    "perplexity = torch.exp(loss)\n",
    "print(perplexity)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.1.3 计算训练和验证损失"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">> Characters: 20479\n",
      ">> Tokens: 5145\n"
     ]
    }
   ],
   "source": [
    "text_data = raw_text\n",
    "total_characters = len(text_data)\n",
    "total_tokens = len(tokenizer.encode(text_data))\n",
    "\n",
    "print(\">> Characters:\", total_characters)\n",
    "print(\">> Tokens:\", total_tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![1718868663879](image/从零开始构建LLM/1718868663879.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_ratio = 0.9\n",
    "split_idx = int(len(text_data) * train_ratio)\n",
    "train_data = text_data[:split_idx]\n",
    "val_data = text_data[split_idx:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = create_dataloader_v1(train_data, \n",
    "                                    batch_size=2, \n",
    "                                    max_length=GPT_CONFIG_124M['context_length'], \n",
    "                                    stride=GPT_CONFIG_124M['context_length'], \n",
    "                                    drop_last=True, \n",
    "                                    shuffle=True)\n",
    "val_loader = create_dataloader_v1(val_data, \n",
    "                                  batch_size=2, \n",
    "                                  max_length=GPT_CONFIG_124M['context_length'], \n",
    "                                  stride=GPT_CONFIG_124M['context_length'], \n",
    "                                  drop_last=False, \n",
    "                                  shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loader:\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "\n",
      "Validation loader:\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n"
     ]
    }
   ],
   "source": [
    "print(\"Train loader:\")\n",
    "for x, y in train_loader:\n",
    "    print(x.shape, y.shape)\n",
    "\n",
    "print(\"\\nValidation loader:\")\n",
    "for x, y in val_loader:\n",
    "    print(x.shape, y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_loss_batch(input_batch, target_batch, model, device):\n",
    "    input_batch, target_batch = input_batch.to(device), target_batch.to(device)\n",
    "    logits = model(input_batch)\n",
    "    loss = nn.functional.cross_entropy(logits.flatten(0, 1), target_batch.flatten())\n",
    "    return loss\n",
    "\n",
    "\n",
    "def calc_loss_loader(data_loader, model, device, num_batches=None):\n",
    "    total_loss = 0\n",
    "    if num_batches is None:\n",
    "        num_batches = len(data_loader)\n",
    "    else:\n",
    "        num_batches = min(len(data_loader), num_batches)\n",
    "    for i, (input_batch, target_batch) in enumerate(data_loader):\n",
    "        if i >= num_batches:\n",
    "            break\n",
    "        loss = calc_loss_batch(input_batch, target_batch, model, device)\n",
    "        total_loss += loss.item()\n",
    "    return total_loss / num_batches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">> Train loss: 10.994715372721354\n",
      ">> Val loss: 11.020210266113281\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)\n",
    "train_loss = calc_loss_loader(train_loader, model, device)\n",
    "print(f\">> Train loss: {train_loss}\")\n",
    "\n",
    "val_loss = calc_loss_loader(val_loader, model, device)\n",
    "print(f\">> Val loss: {val_loss}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.2 训练一个LLM\n",
    "\n",
    "![1718875144870](image/从零开始构建LLM/1718875144870.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluete_model(model, train_loader, val_loader, device, eval_iter):\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        train_loss = calc_loss_loader(train_loader, model, device, eval_iter)\n",
    "        val_loss = calc_loss_loader(val_loader, model, device, eval_iter)\n",
    "    model.train()\n",
    "    return train_loss, val_loss\n",
    "\n",
    "\n",
    "def generate_and_print_sample(model, tokenizer, device, start_context):\n",
    "    model.eval()\n",
    "    context_size = model.pos_emb.weight.shape[0]\n",
    "    excoded = text_to_token_ids(start_context, tokenizer).to(device)\n",
    "    with torch.no_grad():\n",
    "        token_ids = generate_text_simple(model, excoded, 50 , context_size)\n",
    "        decoded_text = token_ids_to_text(token_ids, tokenizer)\n",
    "        print(f\">> {start_context} --> {decoded_text}\")\n",
    "    model.train()\n",
    "\n",
    "\n",
    "def train_model_simple(model, train_loader, val_loader, optimizer, device, num_epochs,\n",
    "                       eval_freq, eval_iter, start_context, tokenizer):\n",
    "    train_losses, val_losses, track_token_seen = [], [], []\n",
    "    token_seen, global_step = 0, -1\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        for input_batch, target_batch in train_loader:\n",
    "            optimizer.zero_grad()\n",
    "            loss = calc_loss_batch(input_batch, target_batch, model, device)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            token_seen += input_batch.numel()\n",
    "            global_step += 1\n",
    "            if global_step % eval_freq == 0:\n",
    "                train_loss, val_loss = evaluete_model(model, train_loader, val_loader, device, eval_iter)\n",
    "                train_losses.append(train_loss)\n",
    "                val_losses.append(val_loss)\n",
    "                track_token_seen.append(token_seen)\n",
    "                print(f\">> Epoch {epoch + 1}, step {global_step: 06d}: train loss {train_loss:.4f}, val loss {val_loss:.4f}\")\n",
    "    \n",
    "        generate_and_print_sample(model, tokenizer, device, start_context)\n",
    "    return train_losses, val_losses, track_token_seen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">> Epoch 1, step  00000: train loss 9.9413, val loss 10.0458\n",
      ">> Epoch 1, step  00005: train loss 8.1316, val loss 8.3626\n",
      ">> Every effort moves you --> Every effort moves you,\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "model = GPTModel(GPT_CONFIG_124M)\n",
    "model.to(device)\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=4e-4, weight_decay=0.1)\n",
    "\n",
    "num_epochs = 1  # Change Here\n",
    "train_losses, val_losses, track_token_seen = train_model_simple(model, train_loader, val_loader, optimizer, device,\n",
    "                                                               num_epochs, 5, 5, \"Every effort moves you\", tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GPTModel(\n",
       "  (tok_emb): Embedding(50257, 768)\n",
       "  (pos_emb): Embedding(256, 768)\n",
       "  (drop_emb): Dropout(p=0.1, inplace=False)\n",
       "  (trf_blocks): Sequential(\n",
       "    (0): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layer): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_resid): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (1): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layer): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_resid): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (2): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layer): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_resid): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (3): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layer): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_resid): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (4): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layer): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_resid): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (5): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layer): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_resid): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (6): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layer): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_resid): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (7): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layer): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_resid): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (8): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layer): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_resid): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (9): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layer): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_resid): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (10): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layer): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_resid): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (11): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layer): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_resid): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "  )\n",
       "  (final_norm): LayerNorm()\n",
       "  (out_head): Linear(in_features=768, out_features=50257, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# save model\n",
    "torch.save(model.state_dict(), \"model.pth\")\n",
    "\n",
    "# load model\n",
    "model = GPTModel(GPT_CONFIG_124M)\n",
    "model.load_state_dict(torch.load(\"model.pth\"))\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1000x500 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def plot_losses(epochs_seen, tokens_seen, train_losses, val_losses):\n",
    "    fig, ax = plt.subplots(figsize=(10, 5))\n",
    "\n",
    "    ax.plot(epochs_seen, train_losses, label=\"train\")\n",
    "    ax.plot(epochs_seen, val_losses, linestyle='-.', label=\"val\")\n",
    "    ax.set_xlabel(\"Epochs\")\n",
    "    ax.set_ylabel(\"Loss\")\n",
    "    ax.legend()\n",
    "\n",
    "    ax1 = ax.twiny()\n",
    "    ax1.plot(tokens_seen, train_losses, alpha=0)\n",
    "    ax1.set_xlabel(\"Tokens Seen\")\n",
    "\n",
    "    fig.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "epoch_tensor = torch.linspace(0, num_epochs, len(train_losses))\n",
    "plot_losses(epoch_tensor, track_token_seen, train_losses, val_losses)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.3 控制随机性的解码策略\n",
    "\n",
    "主要介绍两个函数：`temperature scaling` 和 `top-k sampling`\n",
    "\n",
    "首先需要将模型转到cpu上，因为较小的模型在推理时不需要gpu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GPTModel(\n",
       "  (tok_emb): Embedding(50257, 768)\n",
       "  (pos_emb): Embedding(256, 768)\n",
       "  (drop_emb): Dropout(p=0.1, inplace=False)\n",
       "  (trf_blocks): Sequential(\n",
       "    (0): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layer): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_resid): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (1): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layer): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_resid): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (2): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layer): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_resid): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (3): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layer): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_resid): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (4): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layer): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_resid): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (5): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layer): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_resid): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (6): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layer): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_resid): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (7): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layer): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_resid): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (8): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layer): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_resid): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (9): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layer): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_resid): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (10): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layer): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_resid): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (11): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layer): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_resid): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "  )\n",
       "  (final_norm): LayerNorm()\n",
       "  (out_head): Linear(in_features=768, out_features=50257, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.to('cpu')\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">> generated text:  Every effort moves you,\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "tokenizer = tiktoken.get_encoding(\"gpt2\")\n",
    "token_ids = generate_text_simple(model=model,\n",
    "                                 idx=text_to_token_ids(\"Every effort moves you\", tokenizer),\n",
    "                                 max_new_tokens=25,\n",
    "                                 context_size=GPT_CONFIG_124M[\"context_length\"])\n",
    "print(\">> generated text: \", token_ids_to_text(token_ids, tokenizer))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.3.1 Temperature scaling\n",
    "\n",
    "一种添加概率选择过程到向下一代标记生成任务的技术。</br>\n",
    "在前面的章节中，`generate_text_simple` 函数使用 `torch.argmax` 取最大概率，这一行为被称为贪婪解码 greedy decode。</br>\n",
    "为了使输出更加多样化，我们使用 **`从概率分布进行采样`** 来替换掉`argmax`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">> inverse vocab (argmax):  forward\n",
      ">> inverse vocab (probability distribution):  toward\n"
     ]
    }
   ],
   "source": [
    "vocab = { \n",
    "    \"closer\": 0,\n",
    "    \"every\": 1, \n",
    "    \"effort\": 2, \n",
    "    \"forward\": 3,\n",
    "    \"inches\": 4,\n",
    "    \"moves\": 5, \n",
    "    \"pizza\": 6,\n",
    "    \"toward\": 7,\n",
    "    \"you\": 8,\n",
    "} \n",
    "inverse_vocab = {v: k for k, v in vocab.items()}\n",
    "\n",
    "next_token_logits = torch.tensor(\n",
    "    [4.51, 0.89, -1.90, 6.75, 1.63, -1.62, -1.89, 6.28, 1.79]\n",
    ")\n",
    "\n",
    "probas = torch.softmax(next_token_logits, dim=0)\n",
    "next_token_id = torch.argmax(probas).item()\n",
    "\n",
    "print(\">> inverse vocab (argmax): \", inverse_vocab[next_token_id])\n",
    "\n",
    "next_token_id = torch.multinomial(probas, num_samples=1).item()\n",
    "print(\">> inverse vocab (probability distribution): \", inverse_vocab[next_token_id])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "打印出来的结果都是forward。`multinomial`是根据概率分数的比例来对下一个标记进行采样。因此在这里，forward仍然是最大的概率分数。在执行多次后我们统计一下。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">> sampled 46 times: closer\n",
      ">> sampled 0 times: every\n",
      ">> sampled 0 times: effort\n",
      ">> sampled 578 times: forward\n",
      ">> sampled 4 times: inches\n",
      ">> sampled 0 times: moves\n",
      ">> sampled 0 times: pizza\n",
      ">> sampled 368 times: toward\n",
      ">> sampled 4 times: you\n"
     ]
    }
   ],
   "source": [
    "def print_sampled_tokens(probas):\n",
    "    sample = [torch.multinomial(probas, num_samples=1).item() for _ in range(1000)]\n",
    "    sampled_ids = torch.bincount(torch.tensor(sample))  \n",
    "    for i, freq in enumerate(sampled_ids):\n",
    "        print(f\">> sampled {freq} times: {inverse_vocab[i]}\")\n",
    "\n",
    "print_sampled_tokens(probas)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "可以看出，与argmax不同，大部分情况下，会选择`forward`但是也有其他的可能。</br>\n",
    "我们可以通过一个叫做`temperature scaling`的概念来控制分布和选择过程。`temperature scaling`只是一个大于0的树。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "def softmax_with_temperature(logits, temperature):\n",
    "    scaled_logits = logits / temperature\n",
    "    return torch.softmax(scaled_logits, dim=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "temperature大于1会得到更加均匀的分布，小于1则会得到更尖锐的分布"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "temperatures = [1, 0.1, 5]\n",
    "scaled_probas = [softmax_with_temperature(next_token_logits, t) for t in temperatures]\n",
    "\n",
    "x = torch.arange(len(vocab))\n",
    "bar_width = 0.15\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "for i, T in enumerate(temperatures):\n",
    "    ax.bar(x + i * bar_width, scaled_probas[i], width=bar_width, label=f\"T={T}\")\n",
    "\n",
    "ax.set_ylabel(\"Probability\")\n",
    "ax.set_xticks(x)\n",
    "ax.set_xticklabels(list(vocab.keys()), rotation=90)\n",
    "ax.legend()\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.3.2 Top-k sampling\n",
    "\n",
    "上一节的温度采样和温度尺度可能会生成错误的结果，为了使结果更加准确，我们采用Tok-k采样。当结合概率采样和温度尺度时，可以提高文本生成结果。\n",
    "\n",
    "在top-k抽样中，我们可以将采样的标记限制在最可能的top-k标记中，并通过屏蔽其概率分数，从选择过程中排除所有其他标记，如下图所示。\n",
    "\n",
    "![1718954311719](image/从零开始构建LLM/1718954311719.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">> top logits:  tensor([6.7500, 6.2800, 4.5100])\n",
      ">> top positions:  tensor([3, 7, 0])\n",
      ">> new logits:  tensor([4.5100,   -inf,   -inf, 6.7500,   -inf,   -inf,   -inf, 6.2800,   -inf])\n",
      ">> topk probas:  tensor([0.0615, 0.0000, 0.0000, 0.5775, 0.0000, 0.0000, 0.0000, 0.3610, 0.0000])\n"
     ]
    }
   ],
   "source": [
    "# >> top k and position\n",
    "top_k = 3\n",
    "top_logits, top_pos = torch.topk(next_token_logits, top_k)\n",
    "\n",
    "print(\">> top logits: \", top_logits)\n",
    "print(\">> top positions: \", top_pos)\n",
    "\n",
    "# >> mask logits\n",
    "new_logits = torch.where(condition=next_token_logits < top_logits[-1],\n",
    "                         input=torch.tensor(float('-inf')),\n",
    "                         other=next_token_logits)\n",
    "print(\">> new logits: \", new_logits)\n",
    "\n",
    "# >> topk probas\n",
    "topk_probas = torch.softmax(new_logits, dim=0)\n",
    "print(\">> topk probas: \", topk_probas)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "在此引入温度尺度"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.3.3 修改文本生成函数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate(model, idx, max_new_tokens, context_size, temperature=0.0, top_k=None, eos_id=None):\n",
    "    for _ in range(max_new_tokens):\n",
    "        # >> context\n",
    "        idx_cond = idx if idx.size(0) <= context_size else idx[-context_size:]\n",
    "\n",
    "        # >> logits\n",
    "        with torch.no_grad():\n",
    "            logits = model(idx_cond)\n",
    "        logits = logits[:, -1, :]  # last\n",
    "\n",
    "        # >> topk\n",
    "        if top_k is not None:\n",
    "            top_logits, top_pos = torch.topk(logits, top_k)\n",
    "            min_val = top_logits[:, -1]  # min value\n",
    "            # mask logits\n",
    "            logits = torch.where(condition=logits < min_val, input=torch.tensor(float('-inf')), other=logits)\n",
    "        \n",
    "        # >> temperature\n",
    "        if temperature > 0.0:\n",
    "            logits = logits / temperature\n",
    "            probs = torch.softmax(logits, dim=-1)\n",
    "            idx_next = torch.multinomial(probs, num_samples=1)\n",
    "        else:\n",
    "            idx_next = torch.argmax(logits, dim=-1, keepdim=True)\n",
    "        idx = torch.cat((idx, idx_next), dim=1)\n",
    "    return idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">> generated text:  Every effort moves you.,\n",
      "\n",
      "\n",
      " the to.\"\n",
      " in it,. to,\n"
     ]
    }
   ],
   "source": [
    "token_ids = generate(model=model, idx=text_to_token_ids(\"Every effort moves you\", tokenizer), \n",
    "                     max_new_tokens=15, \n",
    "                     context_size=GPT_CONFIG_124M[\"context_length\"],\n",
    "                     top_k=25,\n",
    "                     temperature=1.4)\n",
    "print(\">> generated text: \", token_ids_to_text(token_ids, tokenizer))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.4 在PyTorch中加载和保存模型权重"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "## save model\n",
    "model_path = os.path.join(\"save_model\", \"model.pth\")\n",
    "torch.save(model.state_dict(), model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GPTModel(\n",
       "  (tok_emb): Embedding(50257, 768)\n",
       "  (pos_emb): Embedding(256, 768)\n",
       "  (drop_emb): Dropout(p=0.1, inplace=False)\n",
       "  (trf_blocks): Sequential(\n",
       "    (0): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layer): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_resid): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (1): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layer): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_resid): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (2): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layer): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_resid): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (3): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layer): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_resid): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (4): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layer): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_resid): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (5): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layer): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_resid): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (6): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layer): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_resid): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (7): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layer): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_resid): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (8): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layer): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_resid): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (9): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layer): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_resid): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (10): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layer): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_resid): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (11): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layer): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_resid): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "  )\n",
       "  (final_norm): LayerNorm()\n",
       "  (out_head): Linear(in_features=768, out_features=50257, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# load model\n",
    "model = GPTModel(GPT_CONFIG_124M)\n",
    "model.load_state_dict(torch.load(model_path))\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> model.eval()作用：**禁用dropout**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "AdamW使用历史数据来动态调整每个模型参数的学习速率。如果没有它，优化器就会重置，模型可能会学习次优，甚至不能正确收敛，这意味着它将失去生成连贯文本的能力。使用torch.save，我们可以保存模型和优化器的state_dict的内容如下："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GPTModel(\n",
       "  (tok_emb): Embedding(50257, 768)\n",
       "  (pos_emb): Embedding(256, 768)\n",
       "  (drop_emb): Dropout(p=0.1, inplace=False)\n",
       "  (trf_blocks): Sequential(\n",
       "    (0): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layer): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_resid): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (1): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layer): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_resid): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (2): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layer): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_resid): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (3): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layer): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_resid): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (4): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layer): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_resid): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (5): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layer): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_resid): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (6): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layer): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_resid): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (7): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layer): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_resid): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (8): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layer): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_resid): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (9): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layer): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_resid): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (10): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layer): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_resid): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (11): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layer): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_resid): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "  )\n",
       "  (final_norm): LayerNorm()\n",
       "  (out_head): Linear(in_features=768, out_features=50257, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_path = os.path.join(\"save_model\", \"model_and_optimizer.pth\")\n",
    "\n",
    "# save model and optimizer\n",
    "torch.save({\n",
    "    \"model_states_dict\": model.state_dict(),\n",
    "    \"optimizer_state_dict\": optimizer.state_dict(),\n",
    "}, model_path)\n",
    "\n",
    "# load model and optimizer\n",
    "checkpoint = torch.load(model_path)\n",
    "model = GPTModel(GPT_CONFIG_124M)\n",
    "model.load_state_dict(checkpoint[\"model_states_dict\"])\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=5e-4, weight_decay=0.1)\n",
    "optimizer.load_state_dict(checkpoint[\"optimizer_state_dict\"])\n",
    "model.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.5 从OpenAI中加载预训练模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pip install tensorflow tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "# gpt_download.py\n",
    "\n",
    "import os\n",
    "import urllib.request\n",
    "\n",
    "# import requests\n",
    "import json\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tqdm import tqdm\n",
    "\n",
    "\n",
    "def download_and_load_gpt2(model_size, models_dir):\n",
    "    # Validate model size\n",
    "    allowed_sizes = (\"124M\", \"355M\", \"774M\", \"1558M\")\n",
    "    if model_size not in allowed_sizes:\n",
    "        raise ValueError(f\"Model size not in {allowed_sizes}\")\n",
    "\n",
    "    # Define paths\n",
    "    model_dir = os.path.join(models_dir, model_size)\n",
    "    base_url = \"https://openaipublic.blob.core.windows.net/gpt-2/models\"\n",
    "    filenames = [\n",
    "        \"checkpoint\", \"encoder.json\", \"hparams.json\",\n",
    "        \"model.ckpt.data-00000-of-00001\", \"model.ckpt.index\",\n",
    "        \"model.ckpt.meta\", \"vocab.bpe\"\n",
    "    ]\n",
    "\n",
    "    # Download files\n",
    "    os.makedirs(model_dir, exist_ok=True)\n",
    "    for filename in filenames:\n",
    "        file_url = os.path.join(base_url, model_size, filename)\n",
    "        file_path = os.path.join(model_dir, filename)\n",
    "        download_file(file_url, file_path)\n",
    "\n",
    "    # Load settings and params\n",
    "    tf_ckpt_path = tf.train.latest_checkpoint(model_dir)\n",
    "    settings = json.load(open(os.path.join(model_dir, \"hparams.json\")))\n",
    "    params = load_gpt2_params_from_tf_ckpt(tf_ckpt_path, settings)\n",
    "\n",
    "    return settings, params\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "def download_file(url, destination):\n",
    "    # Send a GET request to download the file in streaming mode\n",
    "    response = requests.get(url, stream=True)\n",
    "\n",
    "    # Get the total file size from headers, defaulting to 0 if not present\n",
    "    file_size = int(response.headers.get(\"content-length\", 0))\n",
    "\n",
    "    # Check if file exists and has the same size\n",
    "    if os.path.exists(destination):\n",
    "        file_size_local = os.path.getsize(destination)\n",
    "        if file_size == file_size_local:\n",
    "            print(f\"File already exists and is up-to-date: {destination}\")\n",
    "            return\n",
    "\n",
    "    # Define the block size for reading the file\n",
    "    block_size = 1024  # 1 Kilobyte\n",
    "\n",
    "    # Initialize the progress bar with total file size\n",
    "    progress_bar_description = url.split(\"/\")[-1]  # Extract filename from URL\n",
    "    with tqdm(total=file_size, unit=\"iB\", unit_scale=True, desc=progress_bar_description) as progress_bar:\n",
    "        # Open the destination file in binary write mode\n",
    "        with open(destination, \"wb\") as file:\n",
    "            # Iterate over the file data in chunks\n",
    "            for chunk in response.iter_content(block_size):\n",
    "                progress_bar.update(len(chunk))  # Update progress bar\n",
    "                file.write(chunk)  # Write the chunk to the file\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "def download_file(url, destination):\n",
    "    # Send a GET request to download the file\n",
    "    with urllib.request.urlopen(url) as response:\n",
    "        # Get the total file size from headers, defaulting to 0 if not present\n",
    "        file_size = int(response.headers.get(\"Content-Length\", 0))\n",
    "\n",
    "        # Check if file exists and has the same size\n",
    "        if os.path.exists(destination):\n",
    "            file_size_local = os.path.getsize(destination)\n",
    "            if file_size == file_size_local:\n",
    "                print(f\"File already exists and is up-to-date: {destination}\")\n",
    "                return\n",
    "\n",
    "        # Define the block size for reading the file\n",
    "        block_size = 1024  # 1 Kilobyte\n",
    "\n",
    "        # Initialize the progress bar with total file size\n",
    "        progress_bar_description = os.path.basename(url)  # Extract filename from URL\n",
    "        with tqdm(total=file_size, unit=\"iB\", unit_scale=True, desc=progress_bar_description) as progress_bar:\n",
    "            # Open the destination file in binary write mode\n",
    "            with open(destination, \"wb\") as file:\n",
    "                # Read the file in chunks and write to destination\n",
    "                while True:\n",
    "                    chunk = response.read(block_size)\n",
    "                    if not chunk:\n",
    "                        break\n",
    "                    file.write(chunk)\n",
    "                    progress_bar.update(len(chunk))  # Update progress bar\n",
    "\n",
    "\n",
    "def load_gpt2_params_from_tf_ckpt(ckpt_path, settings):\n",
    "    # Initialize parameters dictionary with empty blocks for each layer\n",
    "    params = {\"blocks\": [{} for _ in range(settings[\"n_layer\"])]}\n",
    "\n",
    "    # Iterate over each variable in the checkpoint\n",
    "    for name, _ in tf.train.list_variables(ckpt_path):\n",
    "        # Load the variable and remove singleton dimensions\n",
    "        variable_array = np.squeeze(tf.train.load_variable(ckpt_path, name))\n",
    "\n",
    "        # Process the variable name to extract relevant parts\n",
    "        variable_name_parts = name.split(\"/\")[1:]  # Skip the 'model/' prefix\n",
    "\n",
    "        # Identify the target dictionary for the variable\n",
    "        target_dict = params\n",
    "        if variable_name_parts[0].startswith(\"h\"):\n",
    "            layer_number = int(variable_name_parts[0][1:])\n",
    "            target_dict = params[\"blocks\"][layer_number]\n",
    "\n",
    "        # Recursively access or create nested dictionaries\n",
    "        for key in variable_name_parts[1:-1]:\n",
    "            target_dict = target_dict.setdefault(key, {})\n",
    "\n",
    "        # Assign the variable array to the last key\n",
    "        last_key = variable_name_parts[-1]\n",
    "        target_dict[last_key] = variable_array\n",
    "\n",
    "    return params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File already exists and is up-to-date: models\\124M\\checkpoint\n",
      "File already exists and is up-to-date: models\\124M\\encoder.json\n",
      "File already exists and is up-to-date: models\\124M\\hparams.json\n",
      "File already exists and is up-to-date: models\\124M\\model.ckpt.data-00000-of-00001\n",
      "File already exists and is up-to-date: models\\124M\\model.ckpt.index\n",
      "File already exists and is up-to-date: models\\124M\\model.ckpt.meta\n",
      "File already exists and is up-to-date: models\\124M\\vocab.bpe\n"
     ]
    },
    {
     "ename": "UnicodeDecodeError",
     "evalue": "'utf-8' codec can't decode byte 0xb0 in position 16: invalid start byte",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mUnicodeDecodeError\u001b[0m                        Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[91], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m settings, params \u001b[38;5;241m=\u001b[39m \u001b[43mdownload_and_load_gpt2\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m124M\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmodels\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msettings: \u001b[39m\u001b[38;5;124m\"\u001b[39m, settings)\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mparams: \u001b[39m\u001b[38;5;124m\"\u001b[39m, params)\n",
      "Cell \u001b[1;32mIn[90], line 38\u001b[0m, in \u001b[0;36mdownload_and_load_gpt2\u001b[1;34m(model_size, models_dir)\u001b[0m\n\u001b[0;32m     36\u001b[0m tf_ckpt_path \u001b[38;5;241m=\u001b[39m tf\u001b[38;5;241m.\u001b[39mtrain\u001b[38;5;241m.\u001b[39mlatest_checkpoint(model_dir)\n\u001b[0;32m     37\u001b[0m settings \u001b[38;5;241m=\u001b[39m json\u001b[38;5;241m.\u001b[39mload(\u001b[38;5;28mopen\u001b[39m(os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(model_dir, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhparams.json\u001b[39m\u001b[38;5;124m\"\u001b[39m)))\n\u001b[1;32m---> 38\u001b[0m params \u001b[38;5;241m=\u001b[39m \u001b[43mload_gpt2_params_from_tf_ckpt\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtf_ckpt_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msettings\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     40\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m settings, params\n",
      "Cell \u001b[1;32mIn[90], line 108\u001b[0m, in \u001b[0;36mload_gpt2_params_from_tf_ckpt\u001b[1;34m(ckpt_path, settings)\u001b[0m\n\u001b[0;32m    105\u001b[0m params \u001b[38;5;241m=\u001b[39m {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mblocks\u001b[39m\u001b[38;5;124m\"\u001b[39m: [{} \u001b[38;5;28;01mfor\u001b[39;00m _ \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(settings[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mn_layer\u001b[39m\u001b[38;5;124m\"\u001b[39m])]}\n\u001b[0;32m    107\u001b[0m \u001b[38;5;66;03m# Iterate over each variable in the checkpoint\u001b[39;00m\n\u001b[1;32m--> 108\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m name, _ \u001b[38;5;129;01min\u001b[39;00m \u001b[43mtf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlist_variables\u001b[49m\u001b[43m(\u001b[49m\u001b[43mckpt_path\u001b[49m\u001b[43m)\u001b[49m:\n\u001b[0;32m    109\u001b[0m     \u001b[38;5;66;03m# Load the variable and remove singleton dimensions\u001b[39;00m\n\u001b[0;32m    110\u001b[0m     variable_array \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39msqueeze(tf\u001b[38;5;241m.\u001b[39mtrain\u001b[38;5;241m.\u001b[39mload_variable(ckpt_path, name))\n\u001b[0;32m    112\u001b[0m     \u001b[38;5;66;03m# Process the variable name to extract relevant parts\u001b[39;00m\n",
      "File \u001b[1;32md:\\Python\\Python39\\lib\\site-packages\\tensorflow\\python\\training\\checkpoint_utils.py:141\u001b[0m, in \u001b[0;36mlist_variables\u001b[1;34m(ckpt_dir_or_file)\u001b[0m\n\u001b[0;32m    117\u001b[0m \u001b[38;5;129m@tf_export\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtrain.list_variables\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    118\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mlist_variables\u001b[39m(ckpt_dir_or_file):\n\u001b[0;32m    119\u001b[0m \u001b[38;5;250m  \u001b[39m\u001b[38;5;124;03m\"\"\"Lists the checkpoint keys and shapes of variables in a checkpoint.\u001b[39;00m\n\u001b[0;32m    120\u001b[0m \n\u001b[0;32m    121\u001b[0m \u001b[38;5;124;03m  Checkpoint keys are paths in a checkpoint graph.\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    139\u001b[0m \u001b[38;5;124;03m    List of tuples `(key, shape)`.\u001b[39;00m\n\u001b[0;32m    140\u001b[0m \u001b[38;5;124;03m  \"\"\"\u001b[39;00m\n\u001b[1;32m--> 141\u001b[0m   reader \u001b[38;5;241m=\u001b[39m \u001b[43mload_checkpoint\u001b[49m\u001b[43m(\u001b[49m\u001b[43mckpt_dir_or_file\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    142\u001b[0m   variable_map \u001b[38;5;241m=\u001b[39m reader\u001b[38;5;241m.\u001b[39mget_variable_to_shape_map()\n\u001b[0;32m    143\u001b[0m   names \u001b[38;5;241m=\u001b[39m \u001b[38;5;28msorted\u001b[39m(variable_map\u001b[38;5;241m.\u001b[39mkeys())\n",
      "File \u001b[1;32md:\\Python\\Python39\\lib\\site-packages\\tensorflow\\python\\training\\checkpoint_utils.py:76\u001b[0m, in \u001b[0;36mload_checkpoint\u001b[1;34m(ckpt_dir_or_file)\u001b[0m\n\u001b[0;32m     46\u001b[0m \u001b[38;5;129m@tf_export\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtrain.load_checkpoint\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     47\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mload_checkpoint\u001b[39m(ckpt_dir_or_file):\n\u001b[0;32m     48\u001b[0m \u001b[38;5;250m  \u001b[39m\u001b[38;5;124;03m\"\"\"Returns `CheckpointReader` for checkpoint found in `ckpt_dir_or_file`.\u001b[39;00m\n\u001b[0;32m     49\u001b[0m \n\u001b[0;32m     50\u001b[0m \u001b[38;5;124;03m  If `ckpt_dir_or_file` resolves to a directory with multiple checkpoints,\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     74\u001b[0m \u001b[38;5;124;03m      checkpoints.\u001b[39;00m\n\u001b[0;32m     75\u001b[0m \u001b[38;5;124;03m  \"\"\"\u001b[39;00m\n\u001b[1;32m---> 76\u001b[0m   filename \u001b[38;5;241m=\u001b[39m \u001b[43m_get_checkpoint_filename\u001b[49m\u001b[43m(\u001b[49m\u001b[43mckpt_dir_or_file\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     77\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m filename \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m     78\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCouldn\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mt find \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcheckpoint\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m file or checkpoints in \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m     79\u001b[0m                      \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mgiven directory \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m%\u001b[39m ckpt_dir_or_file)\n",
      "File \u001b[1;32md:\\Python\\Python39\\lib\\site-packages\\tensorflow\\python\\training\\checkpoint_utils.py:479\u001b[0m, in \u001b[0;36m_get_checkpoint_filename\u001b[1;34m(ckpt_dir_or_file)\u001b[0m\n\u001b[0;32m    477\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(ckpt_dir_or_file, os\u001b[38;5;241m.\u001b[39mPathLike):\n\u001b[0;32m    478\u001b[0m   ckpt_dir_or_file \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39mfspath(ckpt_dir_or_file)\n\u001b[1;32m--> 479\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[43mgfile\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mIsDirectory\u001b[49m\u001b[43m(\u001b[49m\u001b[43mckpt_dir_or_file\u001b[49m\u001b[43m)\u001b[49m:\n\u001b[0;32m    480\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m checkpoint_management\u001b[38;5;241m.\u001b[39mlatest_checkpoint(ckpt_dir_or_file)\n\u001b[0;32m    481\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m ckpt_dir_or_file\n",
      "File \u001b[1;32md:\\Python\\Python39\\lib\\site-packages\\tensorflow\\python\\lib\\io\\file_io.py:689\u001b[0m, in \u001b[0;36mis_directory\u001b[1;34m(dirname)\u001b[0m\n\u001b[0;32m    679\u001b[0m \u001b[38;5;129m@tf_export\u001b[39m(v1\u001b[38;5;241m=\u001b[39m[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mgfile.IsDirectory\u001b[39m\u001b[38;5;124m\"\u001b[39m])\n\u001b[0;32m    680\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mis_directory\u001b[39m(dirname):\n\u001b[0;32m    681\u001b[0m \u001b[38;5;250m  \u001b[39m\u001b[38;5;124;03m\"\"\"Returns whether the path is a directory or not.\u001b[39;00m\n\u001b[0;32m    682\u001b[0m \n\u001b[0;32m    683\u001b[0m \u001b[38;5;124;03m  Args:\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    687\u001b[0m \u001b[38;5;124;03m    True, if the path is a directory; False otherwise\u001b[39;00m\n\u001b[0;32m    688\u001b[0m \u001b[38;5;124;03m  \"\"\"\u001b[39;00m\n\u001b[1;32m--> 689\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mis_directory_v2\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdirname\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32md:\\Python\\Python39\\lib\\site-packages\\tensorflow\\python\\lib\\io\\file_io.py:703\u001b[0m, in \u001b[0;36mis_directory_v2\u001b[1;34m(path)\u001b[0m\n\u001b[0;32m    694\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Returns whether the path is a directory or not.\u001b[39;00m\n\u001b[0;32m    695\u001b[0m \n\u001b[0;32m    696\u001b[0m \u001b[38;5;124;03mArgs:\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    700\u001b[0m \u001b[38;5;124;03m  True, if the path is a directory; False otherwise\u001b[39;00m\n\u001b[0;32m    701\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    702\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 703\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_pywrap_file_io\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mIsDirectory\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcompat\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpath_to_bytes\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpath\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    704\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m errors\u001b[38;5;241m.\u001b[39mOpError:\n\u001b[0;32m    705\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mFalse\u001b[39;00m\n",
      "\u001b[1;31mUnicodeDecodeError\u001b[0m: 'utf-8' codec can't decode byte 0xb0 in position 16: invalid start byte"
     ]
    }
   ],
   "source": [
    "settings, params = download_and_load_gpt2(\"124M\", \"models\")\n",
    "print(\"settings: \", settings)\n",
    "print(\"params: \", params)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> 上述代码出错，没有找到好的解决方案，因此使用其他方法来解决<br/>\n",
    "`pip install transformers`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import GPT2LMHeadModel, GPT2Tokenizer\n",
    "\n",
    "model_name = \"gpt2\"\n",
    "tokenizer = GPT2Tokenizer.from_pretrained(model_name)\n",
    "model = GPT2LMHeadModel.from_pretrained(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'generated_text': \"Hello, I'm a language model, but what I'm really doing is making a human-readable document. There are other languages, but those are\"},\n",
       " {'generated_text': \"Hello, I'm a language model, not a syntax model. That's why I like it. I've done a lot of programming projects.\\n\"},\n",
       " {'generated_text': \"Hello, I'm a language model, and I'll do it in no time!\\n\\nOne of the things we learned from talking to my friend\"},\n",
       " {'generated_text': \"Hello, I'm a language model, not a command line tool.\\n\\nIf my code is simple enough:\\n\\nif (use (string\"},\n",
       " {'generated_text': \"Hello, I'm a language model, I've been using Language in all my work. Just a small example, let's see a simplified example.\"}]"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import pipeline, set_seed\n",
    "generator = pipeline('text-generation', model='gpt2')\n",
    "set_seed(42)\n",
    "generator(\"Hello, I'm a language model,\", max_length=30, num_return_sequences=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> 由于网络限制，也报错了\n",
    "\n",
    "移步[./download_huggingface_model.ipynb](./download_huggingface_model.ipynb)下载模型，这个下载方式有很多，可以通过git/网站直接下载都可以，本人通过阅读(官方连接)[https://huggingface.co/docs/transformers/installation#offline-mode]之后进行下载的。<br/>\n",
    "这一部分就不用跟着书看了，只能自己摸索，大致流程也是跟着书一起的"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">> config: GPT2Config {\n",
      "  \"_name_or_path\": \"./gpt2/config.json\",\n",
      "  \"activation_function\": \"gelu_new\",\n",
      "  \"architectures\": [\n",
      "    \"GPT2LMHeadModel\"\n",
      "  ],\n",
      "  \"attn_pdrop\": 0.1,\n",
      "  \"bos_token_id\": 50256,\n",
      "  \"embd_pdrop\": 0.1,\n",
      "  \"eos_token_id\": 50256,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"layer_norm_epsilon\": 1e-05,\n",
      "  \"model_type\": \"gpt2\",\n",
      "  \"n_ctx\": 1024,\n",
      "  \"n_embd\": 768,\n",
      "  \"n_head\": 12,\n",
      "  \"n_inner\": null,\n",
      "  \"n_layer\": 12,\n",
      "  \"n_positions\": 1024,\n",
      "  \"reorder_and_upcast_attn\": false,\n",
      "  \"resid_pdrop\": 0.1,\n",
      "  \"scale_attn_by_inverse_layer_idx\": false,\n",
      "  \"scale_attn_weights\": true,\n",
      "  \"summary_activation\": null,\n",
      "  \"summary_first_dropout\": 0.1,\n",
      "  \"summary_proj_to_labels\": true,\n",
      "  \"summary_type\": \"cls_index\",\n",
      "  \"summary_use_proj\": true,\n",
      "  \"task_specific_params\": {\n",
      "    \"text-generation\": {\n",
      "      \"do_sample\": true,\n",
      "      \"max_length\": 50\n",
      "    }\n",
      "  },\n",
      "  \"transformers_version\": \"4.36.1\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 50257\n",
      "}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from transformers import GPT2Model, GPT2Tokenizer, AutoConfig\n",
    "\n",
    "config = AutoConfig.from_pretrained(\"./gpt2/config.json\")\n",
    "tokenizer = GPT2Tokenizer.from_pretrained(\"./gpt2/\")\n",
    "model = GPT2Model.from_pretrained(\"./gpt2/\")\n",
    "\n",
    "print(\">> config:\", config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">> tokenizer: GPT2Tokenizer(name_or_path='./gpt2/', vocab_size=50257, model_max_length=1024, is_fast=False, padding_side='right', truncation_side='right', special_tokens={'bos_token': '<|endoftext|>', 'eos_token': '<|endoftext|>', 'unk_token': '<|endoftext|>'}, clean_up_tokenization_spaces=True),  added_tokens_decoder={\n",
      "\t50256: AddedToken(\"<|endoftext|>\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=True),\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "print(\">> tokenizer:\", tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">> model: GPT2Model(\n",
      "  (wte): Embedding(50257, 768)\n",
      "  (wpe): Embedding(1024, 768)\n",
      "  (drop): Dropout(p=0.1, inplace=False)\n",
      "  (h): ModuleList(\n",
      "    (0-11): 12 x GPT2Block(\n",
      "      (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "      (attn): GPT2Attention(\n",
      "        (c_attn): Conv1D()\n",
      "        (c_proj): Conv1D()\n",
      "        (attn_dropout): Dropout(p=0.1, inplace=False)\n",
      "        (resid_dropout): Dropout(p=0.1, inplace=False)\n",
      "      )\n",
      "      (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "      (mlp): GPT2MLP(\n",
      "        (c_fc): Conv1D()\n",
      "        (c_proj): Conv1D()\n",
      "        (act): NewGELUActivation()\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (ln_f): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "print(\">> model:\", model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![1719213912457](image/从零开始构建LLM/1719213912457.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Parameter containing:\n",
       "tensor([[-0.1101, -0.0393,  0.0331,  ..., -0.1364,  0.0151,  0.0453],\n",
       "        [ 0.0403, -0.0486,  0.0462,  ...,  0.0861,  0.0025,  0.0432],\n",
       "        [-0.1275,  0.0479,  0.1841,  ...,  0.0899, -0.1297, -0.0879],\n",
       "        ...,\n",
       "        [-0.0445, -0.0548,  0.0123,  ...,  0.1044,  0.0978, -0.0695],\n",
       "        [ 0.1860,  0.0167,  0.0461,  ..., -0.0963,  0.0785, -0.0225],\n",
       "        [ 0.0514, -0.0277,  0.0499,  ...,  0.0070,  0.1552,  0.1207]],\n",
       "       requires_grad=True)"
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# model's block weight\n",
    "model.wte.weight"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "接下来将模型参数加载至我们的GPTModel中"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GPTModel(\n",
       "  (tok_emb): Embedding(50257, 768)\n",
       "  (pos_emb): Embedding(1024, 768)\n",
       "  (drop_emb): Dropout(p=0.1, inplace=False)\n",
       "  (trf_blocks): Sequential(\n",
       "    (0): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layer): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_resid): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (1): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layer): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_resid): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (2): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layer): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_resid): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (3): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layer): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_resid): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (4): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layer): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_resid): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (5): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layer): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_resid): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (6): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layer): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_resid): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (7): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layer): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_resid): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (8): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layer): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_resid): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (9): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layer): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_resid): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (10): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layer): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_resid): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (11): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layer): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_resid): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "  )\n",
       "  (final_norm): LayerNorm()\n",
       "  (out_head): Linear(in_features=768, out_features=50257, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 143,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Define model configurations in a dictionary for compactness\n",
    "model_configs = {\n",
    "    \"gpt2-small (124M)\": {\"emb_dim\": 768, \"n_layers\": 12, \"n_heads\": 12},\n",
    "    \"gpt2-medium (355M)\": {\"emb_dim\": 1024, \"n_layers\": 24, \"n_heads\": 16},\n",
    "    \"gpt2-large (774M)\": {\"emb_dim\": 1280, \"n_layers\": 36, \"n_heads\": 20},\n",
    "    \"gpt2-xl (1558M)\": {\"emb_dim\": 1600, \"n_layers\": 48, \"n_heads\": 25},\n",
    "}\n",
    "\n",
    "# Copy the base configuration and update with specific model settings\n",
    "model_name = \"gpt2-small (124M)\"  # Example model name\n",
    "NEW_CONFIG = GPT_CONFIG_124M.copy()\n",
    "NEW_CONFIG.update(model_configs[model_name])\n",
    "NEW_CONFIG.update({\"context_length\": 1024, \"qkv_bias\": True})\n",
    "\n",
    "gpt = GPTModel(NEW_CONFIG)\n",
    "gpt.eval()\n",
    "gpt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [],
   "source": [
    "# assign the OpenAI weights to the corresponding weight tensors in our GPTModel instance\n",
    "def assign(left, right):\n",
    "    if left.shape != right.shape:\n",
    "        raise ValueError(f\"Shape mismatch. Left: {left.shape}, Right: {right.shape}\")\n",
    "    with torch.no_grad():\n",
    "        left.copy_(torch.tensor(right))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_weights_into_gpt(gpt, model):\n",
    "    assign(gpt.tok_emb.weight, model.wte.weight)\n",
    "    assign(gpt.pos_emb.weight, model.wpe.weight)\n",
    "\n",
    "    for b in range(len(model.h)):\n",
    "        q_w, k_w, v_w = np.split(\n",
    "            model.h[b].attn.c_attn.weight, 3, axis=-1\n",
    "        )\n",
    "\n",
    "        assign(gpt.trf_blocks[b].att.W_query.weight, q_w.T)\n",
    "        assign(gpt.trf_blocks[b].att.W_key.weight, k_w.T)\n",
    "        assign(gpt.trf_blocks[b].att.W_value.weight, v_w.T)\n",
    "\n",
    "        q_b, k_b, v_b = np.split(\n",
    "            model.h[b].attn.c_attn.bias, 3, axis=-1\n",
    "        )\n",
    "\n",
    "        assign(gpt.trf_blocks[b].att.W_query.bias, q_b)\n",
    "        assign(gpt.trf_blocks[b].att.W_key.bias, k_b)\n",
    "        assign(gpt.trf_blocks[b].att.W_value.bias, v_b)\n",
    "        \n",
    "        assign(gpt.trf_blocks[b].att.out_proj.weight, model.h[b].attn.c_proj.weight.T)\n",
    "        assign(gpt.trf_blocks[b].att.out_proj.bias, model.h[b].attn.c_proj.bias)\n",
    "\n",
    "        assign(gpt.trf_blocks[b].ff.layer[0].weight, model.h[b].mlp.c_fc.weight.T)\n",
    "        assign(gpt.trf_blocks[b].ff.layer[0].bias, model.h[b].mlp.c_fc.bias)\n",
    "        assign(gpt.trf_blocks[b].ff.layer[2].weight, model.h[b].mlp.c_proj.weight.T)\n",
    "        assign(gpt.trf_blocks[b].ff.layer[2].bias, model.h[b].mlp.c_proj.bias)\n",
    "\n",
    "        assign(gpt.trf_blocks[b].norm1.scale, model.h[b].ln_1.weight)\n",
    "        assign(gpt.trf_blocks[b].norm1.shift, model.h[b].ln_1.bias)\n",
    "        assign(gpt.trf_blocks[b].norm2.scale, model.h[b].ln_2.weight)\n",
    "        assign(gpt.trf_blocks[b].norm2.shift, model.h[b].ln_2.bias)\n",
    "    \n",
    "    assign(gpt.final_norm.scale, model.ln_f.weight)\n",
    "    assign(gpt.final_norm.shift, model.ln_f.bias)\n",
    "    assign(gpt.out_head.weight, model.wte.weight)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\xuexiaolei\\AppData\\Local\\Temp\\ipykernel_36672\\2213428617.py:6: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  left.copy_(torch.tensor(right))\n"
     ]
    }
   ],
   "source": [
    "load_weights_into_gpt(gpt, model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Keyword arguments {'allowed_special': {'<|endoftext|>'}} not recognized.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">> generated text By MyGPT: \n",
      " Once upon a time there was some danger the enemy would have his hands and heels down onto him\n",
      "\n",
      "But he was sure, this wouldn't\n"
     ]
    }
   ],
   "source": [
    "tokenids = generate(gpt, idx=text_to_token_ids(\"Once upon a time\", tokenizer).to(device),\n",
    "         max_new_tokens=25, context_size=NEW_CONFIG['context_length'],\n",
    "         top_k=50, temperature=1.5)\n",
    "print(\">> generated text By MyGPT: \\n\", token_ids_to_text(tokenids, tokenizer))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Once upon a timeageageageageageageageageageageageageageageageageageageageageageageageageage\n"
     ]
    }
   ],
   "source": [
    "from transformers import GPT2Model, GPT2Tokenizer\n",
    "\n",
    "tokenizer = GPT2Tokenizer.from_pretrained(\"./gpt2/\")\n",
    "model = GPT2Model.from_pretrained(\"./gpt2/\")\n",
    "\n",
    "input_text = \"Once upon a time\"\n",
    "\n",
    "input_ids = tokenizer.encode(input_text, return_tensors='pt')\n",
    "\n",
    "def generate_by_gpt2(model, idx, max_new_tokens, context_size, temperature=0.0, top_k=None, eos_id=None):\n",
    "    for _ in range(max_new_tokens):\n",
    "        # >> context\n",
    "        idx_cond = idx if idx.size(0) <= context_size else idx[-context_size:]\n",
    "\n",
    "        # >> logits\n",
    "        with torch.no_grad():\n",
    "            logits = model(idx_cond)\n",
    "        logits = logits.last_hidden_state[:, -1, :]  # last\n",
    "\n",
    "        # >> topk\n",
    "        if top_k is not None:\n",
    "            top_logits, top_pos = torch.topk(logits, top_k)\n",
    "            min_val = top_logits[:, -1]  # min value\n",
    "            # mask logits\n",
    "            logits = torch.where(condition=logits < min_val, input=torch.tensor(float('-inf')), other=logits)\n",
    "        \n",
    "        # >> temperature\n",
    "        if temperature > 0.0:\n",
    "            logits = logits / temperature\n",
    "            probs = torch.softmax(logits, dim=-1)\n",
    "            idx_next = torch.multinomial(probs, num_samples=1)\n",
    "        else:\n",
    "            idx_next = torch.argmax(logits, dim=-1, keepdim=True)\n",
    "        idx = torch.cat((idx, idx_next), dim=1)\n",
    "    return idx\n",
    "\n",
    "model.eval()\n",
    "\n",
    "generated_ids = generate_by_gpt2(model, idx=input_ids,\n",
    "         max_new_tokens=25, context_size=NEW_CONFIG['context_length'],\n",
    "         top_k=50, temperature=1.5)\n",
    "# 解码生成的文本\n",
    "generated_text = tokenizer.decode(generated_ids[0], skip_special_tokens=True)\n",
    "\n",
    "print(generated_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> GPT2Model 与 GPTLMHeadModel不一样， GPT2Model主要用于获取模型的隐藏状态，而GPTLMHeadModel集成了GPT2Model并添加了语言建模的头部，使其能够进行文本生成。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">> generated text: \n",
      " Once upon a time, the world was a place of great beauty and great danger. The world was a place of great danger, and the world was a place of great danger. The world was a place of great danger, and the world was a\n"
     ]
    }
   ],
   "source": [
    "from transformers import GPT2LMHeadModel, GPT2Tokenizer\n",
    "\n",
    "model = GPT2LMHeadModel.from_pretrained('./gpt2')\n",
    "tokenizer = GPT2Tokenizer.from_pretrained('./gpt2')\n",
    "\n",
    "text = \"Once upon a time\"\n",
    "input_ids = tokenizer.encode(text, return_tensors='pt')\n",
    "\n",
    "output = model.generate(input_ids, max_length=50, num_return_sequences=1)\n",
    "\n",
    "generated_text = tokenizer.decode(output[0], skip_special_tokens=True)\n",
    "\n",
    "print(\">> generated text: \\n\", generated_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6 分类任务的微调"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "matplotlib version: 3.6.3\n",
      "numpy version: 1.23.5\n",
      "tiktoken version: 0.7.0\n",
      "torch version: 2.1.2\n",
      "tensorflow version: 2.16.1\n",
      "pandas version: 2.2.0\n"
     ]
    }
   ],
   "source": [
    "from importlib.metadata import version\n",
    "\n",
    "pkgs = [\"matplotlib\",\n",
    "        \"numpy\",\n",
    "        \"tiktoken\",\n",
    "        \"torch\",\n",
    "        \"tensorflow\", # For OpenAI's pretrained weights\n",
    "        \"pandas\"      # Dataset loading\n",
    "       ]\n",
    "for p in pkgs:\n",
    "    print(f\"{p} version: {version(p)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6.1 不同类别的微调\n",
    "\n",
    "微调语言模型最常见的方法是**指令微调**和**分类微调**。<br/>\n",
    "\n",
    "* 分类微调与训练一个CNN相似，用于特定任务，属于监督学习范畴。但是使用LLM构建比生成式模型更加容易一些。\n",
    "* 指令微调能够做的事情能够多一些。\n",
    "\n",
    "微调流程如下：\n",
    "\n",
    "![1719279066942](image/从零开始构建LLM/1719279066942.png)\n",
    "\n",
    "## 6.2 准备数据集"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sms_spam_collection\\SMSSpamCollection.tsv already exists. Skipping download and extraction.\n"
     ]
    }
   ],
   "source": [
    "import urllib.request\n",
    "import zipfile\n",
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "url = \"https://archive.ics.uci.edu/static/public/228/sms+spam+collection.zip\"\n",
    "zip_path = \"sms_spam_collection.zip\"\n",
    "extracted_path = \"sms_spam_collection\"\n",
    "data_file_path = Path(extracted_path) / \"SMSSpamCollection.tsv\"\n",
    "\n",
    "def download_and_unzip_spam_data(url, zip_path, extracted_path, data_file_path):\n",
    "    if data_file_path.exists():\n",
    "        print(f\"{data_file_path} already exists. Skipping download and extraction.\")\n",
    "        return\n",
    "\n",
    "    # Downloading the file\n",
    "    with urllib.request.urlopen(url) as response:\n",
    "        with open(zip_path, \"wb\") as out_file:\n",
    "            out_file.write(response.read())\n",
    "\n",
    "    # Unzipping the file\n",
    "    with zipfile.ZipFile(zip_path, \"r\") as zip_ref:\n",
    "        zip_ref.extractall(extracted_path)\n",
    "\n",
    "    # Add .tsv file extension\n",
    "    original_file_path = Path(extracted_path) / \"SMSSpamCollection\"\n",
    "    os.rename(original_file_path, data_file_path)\n",
    "    print(f\"File downloaded and saved as {data_file_path}\")\n",
    "\n",
    "download_and_unzip_spam_data(url, zip_path, extracted_path, data_file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Label</th>\n",
       "      <th>Text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ham</td>\n",
       "      <td>Go until jurong point, crazy.. Available only ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ham</td>\n",
       "      <td>Ok lar... Joking wif u oni...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>spam</td>\n",
       "      <td>Free entry in 2 a wkly comp to win FA Cup fina...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ham</td>\n",
       "      <td>U dun say so early hor... U c already then say...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ham</td>\n",
       "      <td>Nah I don't think he goes to usf, he lives aro...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>spam</td>\n",
       "      <td>FreeMsg Hey there darling it's been 3 week's n...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>ham</td>\n",
       "      <td>Even my brother is not like to speak with me. ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>ham</td>\n",
       "      <td>As per your request 'Melle Melle (Oru Minnamin...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>spam</td>\n",
       "      <td>WINNER!! As a valued network customer you have...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>spam</td>\n",
       "      <td>Had your mobile 11 months or more? U R entitle...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  Label                                               Text\n",
       "0   ham  Go until jurong point, crazy.. Available only ...\n",
       "1   ham                      Ok lar... Joking wif u oni...\n",
       "2  spam  Free entry in 2 a wkly comp to win FA Cup fina...\n",
       "3   ham  U dun say so early hor... U c already then say...\n",
       "4   ham  Nah I don't think he goes to usf, he lives aro...\n",
       "5  spam  FreeMsg Hey there darling it's been 3 week's n...\n",
       "6   ham  Even my brother is not like to speak with me. ...\n",
       "7   ham  As per your request 'Melle Melle (Oru Minnamin...\n",
       "8  spam  WINNER!! As a valued network customer you have...\n",
       "9  spam  Had your mobile 11 months or more? U R entitle..."
      ]
     },
     "execution_count": 157,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "data_file_path = os.path.join('data', 'SMSSpamCollection.tsv')\n",
    "df = pd.read_csv(data_file_path, sep=\"\\t\", header=None, names=[\"Label\", \"Text\"])\n",
    "df.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Label\n",
      "ham     4825\n",
      "spam     747\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# >> check label: not balanced\n",
    "print(df[\"Label\"].value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "从上述结果看，类别并不是平衡的"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Label\n",
      "ham     747\n",
      "spam    747\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "def create_balanced_dataset(df):\n",
    "    \n",
    "    # Count the instances of \"spam\"\n",
    "    num_spam = df[df[\"Label\"] == \"spam\"].shape[0]\n",
    "    \n",
    "    # Randomly sample \"ham\" instances to match the number of \"spam\" instances\n",
    "    ham_subset = df[df[\"Label\"] == \"ham\"].sample(num_spam, random_state=123)\n",
    "    \n",
    "    # Combine ham \"subset\" with \"spam\"\n",
    "    balanced_df = pd.concat([ham_subset, df[df[\"Label\"] == \"spam\"]])\n",
    "\n",
    "    return balanced_df\n",
    "\n",
    "balanced_df = create_balanced_dataset(df)\n",
    "print(balanced_df[\"Label\"].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [],
   "source": [
    "balanced_df[\"Label\"] = balanced_df[\"Label\"].map({\"ham\": 0, \"spam\": 1})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [],
   "source": [
    "def random_split(df, train_frac, validation_frac):\n",
    "    # Shuffle the entire DataFrame\n",
    "    df = df.sample(frac=1, random_state=123).reset_index(drop=True)\n",
    "\n",
    "    # Calculate split indices\n",
    "    train_end = int(len(df) * train_frac)\n",
    "    validation_end = train_end + int(len(df) * validation_frac)\n",
    "\n",
    "    # Split the DataFrame\n",
    "    train_df = df[:train_end]\n",
    "    validation_df = df[train_end:validation_end]\n",
    "    test_df = df[validation_end:]\n",
    "\n",
    "    return train_df, validation_df, test_df\n",
    "\n",
    "train_df, validation_df, test_df = random_split(balanced_df, 0.7, 0.1)\n",
    "# Test size is implied to be 0.2 as the remainder\n",
    "\n",
    "train_df.to_csv(\"./data/train.csv\", index=None)\n",
    "validation_df.to_csv(\"./data/validation.csv\", index=None)\n",
    "test_df.to_csv(\"./data/test.csv\", index=None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6.3 dataloader\n",
    "\n",
    "* 注意：文本长度不同，需要以下处理：\n",
    "    1. 将所有本文截断为数据集或批次中最短消息的长度\n",
    "    2. 将所有消息填充到数据集或批次中最长消息的长度\n",
    "* 本文采用第二种方式\n",
    "* 使用第二章的 `<|endoftext|>`进行处理\n",
    "\n",
    "![1719282711195](image/从零开始构建LLM/1719282711195.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">> <|endoftext|>'s token ID is: [50256]\n"
     ]
    }
   ],
   "source": [
    "import tiktoken\n",
    "\n",
    "tokenizer = tiktoken.get_encoding(\"gpt2\")\n",
    "print(\">> <|endoftext|>'s token ID is:\", tokenizer.encode(\"<|endoftext|>\", allowed_special={\"<|endoftext|>\"}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "\n",
    "\n",
    "class SpamDataset(Dataset):\n",
    "    def __init__(self, csv_file, tokenizer, max_length=None, pad_token_id=50256):\n",
    "        self.data = pd.read_csv(csv_file)\n",
    "\n",
    "        # Pre-tokenize texts\n",
    "        self.encoded_texts = [\n",
    "            tokenizer.encode(text) for text in self.data[\"Text\"]\n",
    "        ]\n",
    "\n",
    "        if max_length is None:\n",
    "            self.max_length = self._longest_encoded_length()\n",
    "        else:\n",
    "            self.max_length = max_length\n",
    "            # Truncate sequences if they are longer than max_length\n",
    "            self.encoded_texts = [\n",
    "                encoded_text[:self.max_length]\n",
    "                for encoded_text in self.encoded_texts\n",
    "            ]\n",
    "\n",
    "        # Pad sequences to the longest sequence\n",
    "        self.encoded_texts = [\n",
    "            encoded_text + [pad_token_id] * (self.max_length - len(encoded_text))\n",
    "            for encoded_text in self.encoded_texts\n",
    "        ]\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        encoded = self.encoded_texts[index]\n",
    "        label = self.data.iloc[index][\"Label\"]\n",
    "        return (\n",
    "            torch.tensor(encoded, dtype=torch.long),\n",
    "            torch.tensor(label, dtype=torch.long)\n",
    "        )\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def _longest_encoded_length(self):\n",
    "        max_length = 0\n",
    "        for encoded_text in self.encoded_texts:\n",
    "            encoded_length = len(encoded_text)\n",
    "            if encoded_length > max_length:\n",
    "                max_length = encoded_length\n",
    "        return max_length"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> 创建dataset与dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "120\n"
     ]
    }
   ],
   "source": [
    "train_dataset = SpamDataset(\n",
    "    csv_file=\"./data/train.csv\",\n",
    "    max_length=None,\n",
    "    tokenizer=tokenizer\n",
    ")\n",
    "\n",
    "print(train_dataset.max_length)\n",
    "\n",
    "val_dataset = SpamDataset(\n",
    "    csv_file=\"./data/validation.csv\",\n",
    "    max_length=train_dataset.max_length,\n",
    "    tokenizer=tokenizer\n",
    ")\n",
    "test_dataset = SpamDataset(\n",
    "    csv_file=\"./data/test.csv\",\n",
    "    max_length=train_dataset.max_length,\n",
    "    tokenizer=tokenizer\n",
    ")\n",
    "\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "num_workers = 0\n",
    "batch_size = 8\n",
    "\n",
    "torch.manual_seed(123)\n",
    "\n",
    "train_loader = DataLoader(\n",
    "    dataset=train_dataset,\n",
    "    batch_size=batch_size,\n",
    "    shuffle=True,\n",
    "    num_workers=num_workers,\n",
    "    drop_last=True,\n",
    ")\n",
    "\n",
    "val_loader = DataLoader(\n",
    "    dataset=val_dataset,\n",
    "    batch_size=batch_size,\n",
    "    num_workers=num_workers,\n",
    "    drop_last=False,\n",
    ")\n",
    "\n",
    "test_loader = DataLoader(\n",
    "    dataset=test_dataset,\n",
    "    batch_size=batch_size,\n",
    "    num_workers=num_workers,\n",
    "    drop_last=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">> Input shape: torch.Size([8, 120])\n",
      ">> Target shape: torch.Size([8])\n",
      "\n",
      ">> Train: 130 training batches\n",
      ">> Validation: 19 validation batches\n",
      ">> Test: 38 test batches\n"
     ]
    }
   ],
   "source": [
    "train_iter = iter(train_loader)\n",
    "\n",
    "input_batch, target_batch = next(train_iter)\n",
    "print(\">> Input shape:\", input_batch.shape)\n",
    "print(\">> Target shape:\", target_batch.shape)\n",
    "print()\n",
    "print(f\">> Train: {len(train_loader)} training batches\")\n",
    "print(f\">> Validation: {len(val_loader)} validation batches\")\n",
    "print(f\">> Test: {len(test_loader)} test batches\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6.4 使用预训练权重初始化模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [],
   "source": [
    "CHOOSE_MODEL = \"gpt2-small (124M)\"\n",
    "INPUT_PROMPT = \"Every effort moves\"\n",
    "\n",
    "BASE_CONFIG = {\n",
    "    \"vocab_size\": 50257,     # Vocabulary size\n",
    "    \"context_length\": 1024,  # Context length\n",
    "    \"drop_rate\": 0.0,        # Dropout rate\n",
    "    \"qkv_bias\": True         # Query-key-value bias\n",
    "}\n",
    "\n",
    "model_configs = {\n",
    "    \"gpt2-small (124M)\": {\"emb_dim\": 768, \"n_layers\": 12, \"n_heads\": 12},\n",
    "    \"gpt2-medium (355M)\": {\"emb_dim\": 1024, \"n_layers\": 24, \"n_heads\": 16},\n",
    "    \"gpt2-large (774M)\": {\"emb_dim\": 1280, \"n_layers\": 36, \"n_heads\": 20},\n",
    "    \"gpt2-xl (1558M)\": {\"emb_dim\": 1600, \"n_layers\": 48, \"n_heads\": 25},\n",
    "}\n",
    "\n",
    "BASE_CONFIG.update(model_configs[CHOOSE_MODEL])\n",
    "\n",
    "assert train_dataset.max_length <= BASE_CONFIG[\"context_length\"], (\n",
    "    f\"Dataset length {train_dataset.max_length} exceeds model's context \"\n",
    "    f\"length {BASE_CONFIG['context_length']}. Reinitialize data sets with \"\n",
    "    f\"`max_length={BASE_CONFIG['context_length']}`\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\xuexiaolei\\AppData\\Local\\Temp\\ipykernel_36672\\2213428617.py:6: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  left.copy_(torch.tensor(right))\n"
     ]
    }
   ],
   "source": [
    "model = GPTModel(BASE_CONFIG)\n",
    "\n",
    "gpt_model = GPT2Model.from_pretrained(\"./gpt2\")\n",
    "\n",
    "load_weights_into_gpt(model, gpt_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">> generated text:  every effort moves you to the next step.\n",
      "\n",
      "The first step is to find the right\n"
     ]
    }
   ],
   "source": [
    "text = \"every effort moves you\"\n",
    "\n",
    "token_ids = generate_text_simple(model, text_to_token_ids(text, tokenizer), 15, BASE_CONFIG[\"context_length\"])\n",
    "\n",
    "print(\">> generated text: \", token_ids_to_text(token_ids, tokenizer))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6.5 增加一个用于分类的头\n",
    "\n",
    "![1719284410394](image/从零开始构建LLM/1719284410394.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [],
   "source": [
    "for param in model.parameters():\n",
    "    param.requires_grad = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GPTModel(\n",
       "  (tok_emb): Embedding(50257, 768)\n",
       "  (pos_emb): Embedding(1024, 768)\n",
       "  (drop_emb): Dropout(p=0.0, inplace=False)\n",
       "  (trf_blocks): Sequential(\n",
       "    (0): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layer): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_resid): Dropout(p=0.0, inplace=False)\n",
       "    )\n",
       "    (1): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layer): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_resid): Dropout(p=0.0, inplace=False)\n",
       "    )\n",
       "    (2): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layer): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_resid): Dropout(p=0.0, inplace=False)\n",
       "    )\n",
       "    (3): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layer): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_resid): Dropout(p=0.0, inplace=False)\n",
       "    )\n",
       "    (4): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layer): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_resid): Dropout(p=0.0, inplace=False)\n",
       "    )\n",
       "    (5): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layer): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_resid): Dropout(p=0.0, inplace=False)\n",
       "    )\n",
       "    (6): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layer): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_resid): Dropout(p=0.0, inplace=False)\n",
       "    )\n",
       "    (7): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layer): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_resid): Dropout(p=0.0, inplace=False)\n",
       "    )\n",
       "    (8): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layer): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_resid): Dropout(p=0.0, inplace=False)\n",
       "    )\n",
       "    (9): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layer): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_resid): Dropout(p=0.0, inplace=False)\n",
       "    )\n",
       "    (10): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layer): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_resid): Dropout(p=0.0, inplace=False)\n",
       "    )\n",
       "    (11): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layer): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_resid): Dropout(p=0.0, inplace=False)\n",
       "    )\n",
       "  )\n",
       "  (final_norm): LayerNorm()\n",
       "  (out_head): Linear(in_features=768, out_features=50257, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 172,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_classes = 2\n",
    "model.out_head = torch.nn.Linear(in_features=BASE_CONFIG[\"emb_dim\"], out_features=num_classes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> 从技术上来说，仅需要训练最后一层分类层即可<br/>\n",
    "然而，实验表明微调附加层可以显着提高性能<br/>\n",
    "因此，我们还使最后一个Trf块和将最后一个Trf块连接到输出层的LayerNorm块可训练"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {},
   "outputs": [],
   "source": [
    "for param in model.trf_blocks[-1].parameters():\n",
    "    param.requires_grad = True\n",
    "\n",
    "for param in model.final_norm.parameters():\n",
    "    param.requires_grad = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">> Inputs: tensor([[5211,  345,  423,  640]])\n",
      ">> Inputs dimensions: torch.Size([1, 4])\n",
      ">> Outputs:\n",
      " tensor([[[-1.5529,  2.1970],\n",
      "         [-4.8172,  5.1619],\n",
      "         [-3.1315,  4.4131],\n",
      "         [-3.2033,  5.0488]]])\n",
      ">> Outputs dimensions: torch.Size([1, 4, 2])\n"
     ]
    }
   ],
   "source": [
    "inputs = tokenizer.encode(\"Do you have time\")\n",
    "inputs = torch.tensor(inputs).unsqueeze(0)\n",
    "print(\">> Inputs:\", inputs)\n",
    "print(\">> Inputs dimensions:\", inputs.shape) # shape: (batch_size, num_tokens)\n",
    "\n",
    "with torch.no_grad():\n",
    "    outputs = model(inputs)\n",
    "\n",
    "print(\">> Outputs:\\n\", outputs)\n",
    "print(\">> Outputs dimensions:\", outputs.shape) # shape: (batch_size, num_tokens, num_classes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6.6 计算分类损失和准确率"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">> Last output token: tensor([[-3.2033,  5.0488]])\n",
      ">> Class label: 1\n"
     ]
    }
   ],
   "source": [
    "print(\">> Last output token:\", outputs[:, -1, :])\n",
    "probas = torch.softmax(outputs[:, -1, :], dim=-1)\n",
    "label = torch.argmax(probas)\n",
    "print(\">> Class label:\", label.item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def calc_accuracy_loader(data_loader, model, device, num_batches=None):\n",
    "    model.eval()\n",
    "    correct_predictions, num_examples = 0, 0\n",
    "\n",
    "    if num_batches is None:\n",
    "        num_batches = len(data_loader)\n",
    "    else:\n",
    "        num_batches = min(num_batches, len(data_loader))\n",
    "    for i, (input_batch, target_batch) in enumerate(data_loader):\n",
    "        if i < num_batches:\n",
    "            input_batch, target_batch = input_batch.to(device), target_batch.to(device)\n",
    "\n",
    "            with torch.no_grad():\n",
    "                logits = model(input_batch)[:, -1, :]  # Logits of last output token·\n",
    "            predicted_labels = torch.argmax(logits, dim=-1)\n",
    "\n",
    "            num_examples += predicted_labels.shape[0]\n",
    "            correct_predictions += (predicted_labels == target_batch).sum().item()\n",
    "        else:\n",
    "            break\n",
    "    return correct_predictions / num_examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">> Training accuracy: 38.75%\n",
      ">> Validation accuracy: 45.00%\n",
      ">> Test accuracy: 48.75%\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)\n",
    "\n",
    "train_accuracy = calc_accuracy_loader(train_loader, model, device, num_batches=10)\n",
    "val_accuracy = calc_accuracy_loader(val_loader, model, device, num_batches=10)\n",
    "test_accuracy = calc_accuracy_loader(test_loader, model, device, num_batches=10)\n",
    "\n",
    "print(f\">> Training accuracy: {train_accuracy*100:.2f}%\")\n",
    "print(f\">> Validation accuracy: {val_accuracy*100:.2f}%\")\n",
    "print(f\">> Test accuracy: {test_accuracy*100:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_loss_batch(input_batch, target_batch, model, device):\n",
    "    input_batch, target_batch = input_batch.to(device), target_batch.to(device)\n",
    "    logits = model(input_batch)[:, -1, :]  # Logits of last output token\n",
    "    loss = torch.nn.functional.cross_entropy(logits, target_batch)\n",
    "    return loss\n",
    "\n",
    "\n",
    "def calc_loss_loader(data_loader, model, device, num_batches=None):\n",
    "    total_loss = 0.\n",
    "    if len(data_loader) == 0:\n",
    "        return float(\"nan\")\n",
    "    elif num_batches is None:\n",
    "        num_batches = len(data_loader)\n",
    "    else:\n",
    "        # Reduce the number of batches to match the total number of batches in the data loader\n",
    "        # if num_batches exceeds the number of batches in the data loader\n",
    "        num_batches = min(num_batches, len(data_loader))\n",
    "    for i, (input_batch, target_batch) in enumerate(data_loader):\n",
    "        if i < num_batches:\n",
    "            loss = calc_loss_batch(input_batch, target_batch, model, device)\n",
    "            total_loss += loss.item()\n",
    "        else:\n",
    "            break\n",
    "    return total_loss / num_batches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training loss: 6.090\n",
      "Validation loss: 4.865\n",
      "Test loss: 4.373\n"
     ]
    }
   ],
   "source": [
    "with torch.no_grad():\n",
    "    train_loss = calc_loss_loader(train_loader, model, device, num_batches=5)\n",
    "    val_loss = calc_loss_loader(val_loader, model, device, num_batches=5)\n",
    "    test_loss = calc_loss_loader(test_loader, model, device, num_batches=5)\n",
    "\n",
    "print(f\"Training loss: {train_loss:.3f}\")\n",
    "print(f\"Validation loss: {val_loss:.3f}\")\n",
    "print(f\"Test loss: {test_loss:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6.7 使用标记数据进行微调\n",
    "\n",
    "![1719286267048](image/从零开始构建LLM/1719286267048.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_classifier_simple(model, train_loader, val_loader, optimizer, device, num_epochs,\n",
    "                            eval_freq, eval_iter):\n",
    "    # Initialize lists to track losses and examples seen\n",
    "    train_losses, val_losses, train_accs, val_accs = [], [], [], []\n",
    "    examples_seen, global_step = 0, -1\n",
    "\n",
    "    # Main training loop\n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()  # Set model to training mode\n",
    "\n",
    "        for input_batch, target_batch in train_loader:\n",
    "            optimizer.zero_grad() # Reset loss gradients from previous batch iteration\n",
    "            loss = calc_loss_batch(input_batch, target_batch, model, device)\n",
    "            loss.backward() # Calculate loss gradients\n",
    "            optimizer.step() # Update model weights using loss gradients\n",
    "            examples_seen += input_batch.shape[0] # New: track examples instead of tokens\n",
    "            global_step += 1\n",
    "\n",
    "            # Optional evaluation step\n",
    "            if global_step % eval_freq == 0:\n",
    "                train_loss, val_loss = evaluate_model(\n",
    "                    model, train_loader, val_loader, device, eval_iter)\n",
    "                train_losses.append(train_loss)\n",
    "                val_losses.append(val_loss)\n",
    "                print(f\"Ep {epoch+1} (Step {global_step:06d}): \"\n",
    "                      f\"Train loss {train_loss:.3f}, Val loss {val_loss:.3f}\")\n",
    "\n",
    "        # Calculate accuracy after each epoch\n",
    "        train_accuracy = calc_accuracy_loader(train_loader, model, device, num_batches=eval_iter)\n",
    "        val_accuracy = calc_accuracy_loader(val_loader, model, device, num_batches=eval_iter)\n",
    "        print(f\"Training accuracy: {train_accuracy*100:.2f}% | \", end=\"\")\n",
    "        print(f\"Validation accuracy: {val_accuracy*100:.2f}%\")\n",
    "        train_accs.append(train_accuracy)\n",
    "        val_accs.append(val_accuracy)\n",
    "\n",
    "    return train_losses, val_losses, train_accs, val_accs, examples_seen\n",
    "\n",
    "\n",
    "def evaluate_model(model, train_loader, val_loader, device, eval_iter):\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        train_loss = calc_loss_loader(train_loader, model, device, num_batches=eval_iter)\n",
    "        val_loss = calc_loss_loader(val_loader, model, device, num_batches=eval_iter)\n",
    "    model.train()\n",
    "    return train_loss, val_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ep 1 (Step 000000): Train loss 4.198, Val loss 4.661\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[182], line 10\u001b[0m\n\u001b[0;32m      7\u001b[0m optimizer \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39moptim\u001b[38;5;241m.\u001b[39mAdamW(model\u001b[38;5;241m.\u001b[39mparameters(), lr\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m5e-5\u001b[39m, weight_decay\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.1\u001b[39m)\n\u001b[0;32m      9\u001b[0m num_epochs \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m5\u001b[39m\n\u001b[1;32m---> 10\u001b[0m train_losses, val_losses, train_accs, val_accs, examples_seen \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_classifier_simple\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m     11\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mval_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     12\u001b[0m \u001b[43m    \u001b[49m\u001b[43mnum_epochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnum_epochs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43meval_freq\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m50\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43meval_iter\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m5\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m     13\u001b[0m \u001b[43m)\u001b[49m\n\u001b[0;32m     15\u001b[0m end_time \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime()\n\u001b[0;32m     16\u001b[0m execution_time_minutes \u001b[38;5;241m=\u001b[39m (end_time \u001b[38;5;241m-\u001b[39m start_time) \u001b[38;5;241m/\u001b[39m \u001b[38;5;241m60\u001b[39m\n",
      "Cell \u001b[1;32mIn[181], line 13\u001b[0m, in \u001b[0;36mtrain_classifier_simple\u001b[1;34m(model, train_loader, val_loader, optimizer, device, num_epochs, eval_freq, eval_iter)\u001b[0m\n\u001b[0;32m     11\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m input_batch, target_batch \u001b[38;5;129;01min\u001b[39;00m train_loader:\n\u001b[0;32m     12\u001b[0m     optimizer\u001b[38;5;241m.\u001b[39mzero_grad() \u001b[38;5;66;03m# Reset loss gradients from previous batch iteration\u001b[39;00m\n\u001b[1;32m---> 13\u001b[0m     loss \u001b[38;5;241m=\u001b[39m \u001b[43mcalc_loss_batch\u001b[49m\u001b[43m(\u001b[49m\u001b[43minput_batch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtarget_batch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     14\u001b[0m     loss\u001b[38;5;241m.\u001b[39mbackward() \u001b[38;5;66;03m# Calculate loss gradients\u001b[39;00m\n\u001b[0;32m     15\u001b[0m     optimizer\u001b[38;5;241m.\u001b[39mstep() \u001b[38;5;66;03m# Update model weights using loss gradients\u001b[39;00m\n",
      "Cell \u001b[1;32mIn[179], line 3\u001b[0m, in \u001b[0;36mcalc_loss_batch\u001b[1;34m(input_batch, target_batch, model, device)\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcalc_loss_batch\u001b[39m(input_batch, target_batch, model, device):\n\u001b[0;32m      2\u001b[0m     input_batch, target_batch \u001b[38;5;241m=\u001b[39m input_batch\u001b[38;5;241m.\u001b[39mto(device), target_batch\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[1;32m----> 3\u001b[0m     logits \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43minput_batch\u001b[49m\u001b[43m)\u001b[49m[:, \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, :]  \u001b[38;5;66;03m# Logits of last output token\u001b[39;00m\n\u001b[0;32m      4\u001b[0m     loss \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mnn\u001b[38;5;241m.\u001b[39mfunctional\u001b[38;5;241m.\u001b[39mcross_entropy(logits, target_batch)\n\u001b[0;32m      5\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m loss\n",
      "File \u001b[1;32md:\\Python\\Python39\\lib\\site-packages\\torch\\nn\\modules\\module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32md:\\Python\\Python39\\lib\\site-packages\\torch\\nn\\modules\\module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "Cell \u001b[1;32mIn[49], line 22\u001b[0m, in \u001b[0;36mGPTModel.forward\u001b[1;34m(self, in_idx)\u001b[0m\n\u001b[0;32m     20\u001b[0m x \u001b[38;5;241m=\u001b[39m tok_embeds \u001b[38;5;241m+\u001b[39m pos_embeds  \u001b[38;5;66;03m# Shape [batch_size, num_tokens, emb_size]\u001b[39;00m\n\u001b[0;32m     21\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdrop_emb(x)\n\u001b[1;32m---> 22\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrf_blocks\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     23\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfinal_norm(x)\n\u001b[0;32m     24\u001b[0m logits \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mout_head(x)\n",
      "File \u001b[1;32md:\\Python\\Python39\\lib\\site-packages\\torch\\nn\\modules\\module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32md:\\Python\\Python39\\lib\\site-packages\\torch\\nn\\modules\\module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32md:\\Python\\Python39\\lib\\site-packages\\torch\\nn\\modules\\container.py:215\u001b[0m, in \u001b[0;36mSequential.forward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    213\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m):\n\u001b[0;32m    214\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m:\n\u001b[1;32m--> 215\u001b[0m         \u001b[38;5;28minput\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[43mmodule\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m    216\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28minput\u001b[39m\n",
      "File \u001b[1;32md:\\Python\\Python39\\lib\\site-packages\\torch\\nn\\modules\\module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32md:\\Python\\Python39\\lib\\site-packages\\torch\\nn\\modules\\module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "Cell \u001b[1;32mIn[48], line 25\u001b[0m, in \u001b[0;36mTransformerBlock.forward\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m     22\u001b[0m shortcut \u001b[38;5;241m=\u001b[39m x\n\u001b[0;32m     24\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnorm2(x)\n\u001b[1;32m---> 25\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mff\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     26\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdrop_resid(x)\n\u001b[0;32m     27\u001b[0m x \u001b[38;5;241m=\u001b[39m x \u001b[38;5;241m+\u001b[39m shortcut\n",
      "File \u001b[1;32md:\\Python\\Python39\\lib\\site-packages\\torch\\nn\\modules\\module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32md:\\Python\\Python39\\lib\\site-packages\\torch\\nn\\modules\\module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "Cell \u001b[1;32mIn[42], line 11\u001b[0m, in \u001b[0;36mFeedForward.forward\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m     10\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x):\n\u001b[1;32m---> 11\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlayer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32md:\\Python\\Python39\\lib\\site-packages\\torch\\nn\\modules\\module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32md:\\Python\\Python39\\lib\\site-packages\\torch\\nn\\modules\\module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32md:\\Python\\Python39\\lib\\site-packages\\torch\\nn\\modules\\container.py:215\u001b[0m, in \u001b[0;36mSequential.forward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    213\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m):\n\u001b[0;32m    214\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m:\n\u001b[1;32m--> 215\u001b[0m         \u001b[38;5;28minput\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[43mmodule\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m    216\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28minput\u001b[39m\n",
      "File \u001b[1;32md:\\Python\\Python39\\lib\\site-packages\\torch\\nn\\modules\\module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32md:\\Python\\Python39\\lib\\site-packages\\torch\\nn\\modules\\module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32md:\\Python\\Python39\\lib\\site-packages\\torch\\nn\\modules\\linear.py:114\u001b[0m, in \u001b[0;36mLinear.forward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    113\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[1;32m--> 114\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlinear\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import time\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "torch.manual_seed(123)\n",
    "\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=5e-5, weight_decay=0.1)\n",
    "\n",
    "num_epochs = 5\n",
    "train_losses, val_losses, train_accs, val_accs, examples_seen = train_classifier_simple(\n",
    "    model, train_loader, val_loader, optimizer, device,\n",
    "    num_epochs=num_epochs, eval_freq=50, eval_iter=5,\n",
    ")\n",
    "\n",
    "end_time = time.time()\n",
    "execution_time_minutes = (end_time - start_time) / 60\n",
    "print(f\"Training completed in {execution_time_minutes:.2f} minutes.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def plot_values(epochs_seen, examples_seen, train_values, val_values, label=\"loss\"):\n",
    "    fig, ax1 = plt.subplots(figsize=(5, 3))\n",
    "\n",
    "    # Plot training and validation loss against epochs\n",
    "    ax1.plot(epochs_seen, train_values, label=f\"Training {label}\")\n",
    "    ax1.plot(epochs_seen, val_values, linestyle=\"-.\", label=f\"Validation {label}\")\n",
    "    ax1.set_xlabel(\"Epochs\")\n",
    "    ax1.set_ylabel(label.capitalize())\n",
    "    ax1.legend()\n",
    "\n",
    "    # Create a second x-axis for examples seen\n",
    "    ax2 = ax1.twiny()  # Create a second x-axis that shares the same y-axis\n",
    "    ax2.plot(examples_seen, train_values, alpha=0)  # Invisible plot for aligning ticks\n",
    "    ax2.set_xlabel(\"Examples seen\")\n",
    "\n",
    "    fig.tight_layout()  # Adjust layout to make room\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs_tensor = torch.linspace(0, num_epochs, len(train_losses))\n",
    "examples_seen_tensor = torch.linspace(0, examples_seen, len(train_losses))\n",
    "\n",
    "plot_values(epochs_tensor, examples_seen_tensor, train_losses, val_losses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs_tensor = torch.linspace(0, num_epochs, len(train_accs))\n",
    "examples_seen_tensor = torch.linspace(0, examples_seen, len(train_accs))\n",
    "\n",
    "plot_values(epochs_tensor, examples_seen_tensor, train_accs, val_accs, label=\"accuracy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_accuracy = calc_accuracy_loader(train_loader, model, device)\n",
    "val_accuracy = calc_accuracy_loader(val_loader, model, device)\n",
    "test_accuracy = calc_accuracy_loader(test_loader, model, device)\n",
    "\n",
    "print(f\"Training accuracy: {train_accuracy*100:.2f}%\")\n",
    "print(f\"Validation accuracy: {val_accuracy*100:.2f}%\")\n",
    "print(f\"Test accuracy: {test_accuracy*100:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6.8 应用"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def classify_review(text, model, tokenizer, device, max_length=None, pad_token_id=50256):\n",
    "    model.eval()\n",
    "\n",
    "    # Prepare inputs to the model\n",
    "    input_ids = tokenizer.encode(text)\n",
    "    supported_context_length = model.pos_emb.weight.shape[1]\n",
    "\n",
    "    # Truncate sequences if they too long\n",
    "    input_ids = input_ids[:min(max_length, supported_context_length)]\n",
    "\n",
    "    # Pad sequences to the longest sequence\n",
    "    input_ids += [pad_token_id] * (max_length - len(input_ids))\n",
    "    input_tensor = torch.tensor(input_ids, device=device).unsqueeze(0) # add batch dimension\n",
    "\n",
    "    # Model inference\n",
    "    with torch.no_grad():\n",
    "        logits = model(input_tensor)[:, -1, :]  # Logits of the last output token\n",
    "    predicted_label = torch.argmax(logits, dim=-1).item()\n",
    "\n",
    "    # Return the classified result\n",
    "    return \"spam\" if predicted_label == 1 else \"not spam\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_1 = (\n",
    "    \"You are a winner you have been specially\"\n",
    "    \" selected to receive $1000 cash or a $2000 award.\"\n",
    ")\n",
    "\n",
    "print(classify_review(\n",
    "    text_1, model, tokenizer, device, max_length=train_dataset.max_length\n",
    "))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_2 = (\n",
    "    \"Hey, just wanted to check if we're still on\"\n",
    "    \" for dinner tonight? Let me know!\"\n",
    ")\n",
    "\n",
    "print(classify_review(\n",
    "    text_2, model, tokenizer, device, max_length=train_dataset.max_length\n",
    "))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_path = os.path.join(\"save_model\", \"review_classifier.pth\")\n",
    "\n",
    "torch.save(model.state_dict(), model_path)\n",
    "\n",
    "model_state_dict = torch.load(model_path)\n",
    "model.load_state_dict(model_state_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 7 指令微调\n",
    "\n",
    "![1719309589135](image/从零开始构建LLM/1719309589135.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## 7.1 指令微调介绍\n",
    "\n",
    "功能概览\n",
    "\n",
    "![1719309666974](image/从零开始构建LLM/1719309666974.png)\n",
    "\n",
    "流程\n",
    "\n",
    "![1719309789448](image/从零开始构建LLM/1719309789448.png)\n",
    "\n",
    "## 7.2 准备数据"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of entries: 1100\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import os\n",
    "import urllib\n",
    "\n",
    "\n",
    "def download_and_load_file(file_path, url):\n",
    "\n",
    "    if not os.path.exists(file_path):\n",
    "        with urllib.request.urlopen(url) as response:\n",
    "            text_data = response.read().decode(\"utf-8\")\n",
    "        with open(file_path, \"w\", encoding=\"utf-8\") as file:\n",
    "            file.write(text_data)\n",
    "    else:\n",
    "        with open(file_path, \"r\", encoding=\"utf-8\") as file:\n",
    "            text_data = file.read()\n",
    "\n",
    "    with open(file_path, \"r\") as file:\n",
    "        data = json.load(file)\n",
    "\n",
    "    return data\n",
    "\n",
    "\n",
    "file_path = \"./data/instruction-data.json\"\n",
    "url = \"https://raw.githubusercontent.com/rasbt/LLMs-from-scratch/main/ch07/01_main-chapter-code/instruction-data.json\"\n",
    "\n",
    "data = download_and_load_file(file_path, url)\n",
    "print(\"Number of entries:\", len(data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'instruction': \"What is an antonym of 'complicated'?\",\n",
       " 'input': '',\n",
       " 'output': \"An antonym of 'complicated' is 'simple'.\"}"
      ]
     },
     "execution_count": 184,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data[999]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "LLM的输入由多种形式，下图展示了两种形式：Alpaca和Phi-3，本文使用Alpaca形式进行训练\n",
    "\n",
    "![1719373091773](image/从零开始构建LLM/1719373091773.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_input(entry):\n",
    "    instruction_text = (\n",
    "        f\"Below is an instruction that describes a task. \"\n",
    "        f\"Write a response that appropriately completes the request.\"\n",
    "        f\"\\n\\n### Instruction:\\n{entry['instruction']}\"\n",
    "    )\n",
    "\n",
    "    input_text = f\"\\n\\n### Input:\\n{entry['input']}\" if entry[\"input\"] else \"\"\n",
    "    return instruction_text + input_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Below is an instruction that describes a task. Write a response that appropriately completes the request.\n",
      "\n",
      "### Instruction:\n",
      "Identify the correct spelling of the following word.\n",
      "\n",
      "### Input:\n",
      "Ocassion\n",
      "\n",
      "### Response:\n",
      "The correct spelling is 'Occasion.'\n"
     ]
    }
   ],
   "source": [
    "model_input = format_input(data[50])\n",
    "desired_response = f\"\\n\\n### Response:\\n{data[50]['output']}\"\n",
    "\n",
    "print(model_input + desired_response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Below is an instruction that describes a task. Write a response that appropriately completes the request.\n",
      "\n",
      "### Instruction:\n",
      "What is an antonym of 'complicated'?\n",
      "\n",
      "### Response:\n",
      "An antonym of 'complicated' is 'simple'.\n"
     ]
    }
   ],
   "source": [
    "model_input = format_input(data[999])\n",
    "desired_response = f\"\\n\\n### Response:\\n{data[999]['output']}\"\n",
    "\n",
    "print(model_input + desired_response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">> Training set length: 935\n",
      ">> Validation set length: 55\n",
      ">> Test set length: 110\n"
     ]
    }
   ],
   "source": [
    "train_portion = int(len(data) * 0.85)  # 85% for training\n",
    "test_portion = int(len(data) * 0.1)    # 10% for testing\n",
    "val_portion = len(data) - train_portion - test_portion  # Remaining 5% for validation\n",
    "\n",
    "train_data = data[:train_portion]\n",
    "test_data = data[train_portion:train_portion + test_portion]\n",
    "val_data = data[train_portion + test_portion:]\n",
    "\n",
    "print(\">> Training set length:\", len(train_data))\n",
    "print(\">> Validation set length:\", len(val_data))\n",
    "print(\">> Test set length:\", len(test_data))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7.3 数据封装\n",
    "\n",
    "![1719380932620](image/从零开始构建LLM/1719380932620.png)\n",
    "\n",
    "![1719381496083](image/从零开始构建LLM/1719381496083.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "\n",
    "class InstructionDataset(Dataset):\n",
    "    def __init__(self, data, tokenizer):\n",
    "        self.data = data\n",
    "        self.encoded_texts = []\n",
    "\n",
    "        # Pre-tokenize texts\n",
    "        for entry in data:\n",
    "            instruction_plus_input = self.format_input(entry)\n",
    "            response_text = f\"\\n\\n### Response:\\n{entry['output']}\"\n",
    "            full_text = instruction_plus_input + response_text\n",
    "            self.encoded_texts.append(tokenizer.encode(full_text))\n",
    "    \n",
    "    def format_input(self, entry):\n",
    "        instruction_text = (\n",
    "            f\"Below is an instruction that describes a task. \"\n",
    "            f\"Write a response that appropriately completes the request.\"\n",
    "            f\"\\n\\n### Instruction:\\n{entry['instruction']}\"\n",
    "        )\n",
    "\n",
    "        input_text = f\"\\n\\n### Input:\\n{entry['input']}\" if entry[\"input\"] else \"\"\n",
    "        return instruction_text + input_text\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.encoded_texts[idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[50256]\n"
     ]
    }
   ],
   "source": [
    "import tiktoken\n",
    "\n",
    "tokenizer = tiktoken.get_encoding(\"gpt2\")\n",
    "\n",
    "print(tokenizer.encode(\"<|endoftext|>\", allowed_special={\"<|endoftext|>\"}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "metadata": {},
   "outputs": [],
   "source": [
    "def custom_collate_fn(batch, pad_token_id=50256, ignore_index=-100, allowed_max_length=None, device=\"cpu\"):\n",
    "    batch_max_length = max(len(entry)+1 for entry in batch)\n",
    "\n",
    "    inputs_lst, targets_lst = [], []\n",
    "\n",
    "    for entry in batch:\n",
    "        new_entry = entry.copy()\n",
    "        new_entry += [pad_token_id]\n",
    "\n",
    "        padded = new_entry + [pad_token_id] * (batch_max_length - len(new_entry))\n",
    "\n",
    "        inputs = torch.tensor(padded[:-1])\n",
    "        targets = torch.tensor(padded[1:])\n",
    "\n",
    "        mask = targets == pad_token_id\n",
    "        indices = torch.nonzero(mask).squeeze()\n",
    "        if indices.numel() > 1:\n",
    "            targets[indices[1:]] = ignore_index\n",
    "        \n",
    "        if allowed_max_length is not None:\n",
    "            inputs = inputs[:allowed_max_length]\n",
    "            targets = targets[:allowed_max_length]\n",
    "        \n",
    "        inputs_lst.append(inputs)\n",
    "        targets_lst.append(targets)\n",
    "    inputs_tensor = torch.stack(inputs_lst).to(device)\n",
    "    targets_tensor = torch.stack(targets_lst).to(device)\n",
    "\n",
    "    return inputs_tensor, targets_tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[    0,     1,     2,     3,     4],\n",
      "        [    5,     6, 50256, 50256, 50256],\n",
      "        [    7,     8,     9, 50256, 50256]])\n",
      "tensor([[    1,     2,     3,     4, 50256],\n",
      "        [    6, 50256,  -100,  -100,  -100],\n",
      "        [    8,     9, 50256,  -100,  -100]])\n"
     ]
    }
   ],
   "source": [
    "inputs_1 = [0, 1, 2, 3, 4]\n",
    "inputs_2 = [5, 6]\n",
    "inputs_3 = [7, 8, 9]\n",
    "\n",
    "batch = (\n",
    "    inputs_1,\n",
    "    inputs_2,\n",
    "    inputs_3\n",
    ")\n",
    "inputs, targets = custom_collate_fn(batch)\n",
    "print(inputs)\n",
    "print(targets)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7.4 创建dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "metadata": {},
   "outputs": [],
   "source": [
    "from functools import partial\n",
    "customized_collate_fn = partial(custom_collate_fn, device=device, allowed_max_length=1024)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "\n",
    "num_workers = 0\n",
    "batch_size = 8\n",
    "\n",
    "train_dataset = InstructionDataset(train_data, tokenizer)\n",
    "val_dataset = InstructionDataset(val_data, tokenizer)\n",
    "test_dataset = InstructionDataset(test_data, tokenizer)\n",
    "\n",
    "train_loader = DataLoader(\n",
    "    dataset=train_dataset,\n",
    "    batch_size=batch_size,\n",
    "    shuffle=True,\n",
    "    drop_last=True,\n",
    "    collate_fn=customized_collate_fn,\n",
    "    num_workers=num_workers\n",
    ")\n",
    "\n",
    "val_loader = DataLoader(\n",
    "    dataset=val_dataset,\n",
    "    batch_size=batch_size,\n",
    "    shuffle=False,\n",
    "    drop_last=False,\n",
    "    collate_fn=customized_collate_fn,\n",
    "    num_workers=num_workers\n",
    ")\n",
    "\n",
    "test_loader = DataLoader(\n",
    "    dataset=test_dataset,\n",
    "    batch_size=batch_size,\n",
    "    shuffle=False,\n",
    "    drop_last=False,\n",
    "    collate_fn=customized_collate_fn,\n",
    "    num_workers=num_workers\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loader:\n",
      "torch.Size([8, 64]) torch.Size([8, 64])\n",
      "torch.Size([8, 62]) torch.Size([8, 62])\n",
      "torch.Size([8, 69]) torch.Size([8, 69])\n",
      "torch.Size([8, 80]) torch.Size([8, 80])\n",
      "torch.Size([8, 73]) torch.Size([8, 73])\n",
      "torch.Size([8, 65]) torch.Size([8, 65])\n",
      "torch.Size([8, 65]) torch.Size([8, 65])\n",
      "torch.Size([8, 73]) torch.Size([8, 73])\n",
      "torch.Size([8, 71]) torch.Size([8, 71])\n",
      "torch.Size([8, 55]) torch.Size([8, 55])\n",
      "torch.Size([8, 66]) torch.Size([8, 66])\n",
      "torch.Size([8, 65]) torch.Size([8, 65])\n",
      "torch.Size([8, 75]) torch.Size([8, 75])\n",
      "torch.Size([8, 58]) torch.Size([8, 58])\n",
      "torch.Size([8, 71]) torch.Size([8, 71])\n",
      "torch.Size([8, 71]) torch.Size([8, 71])\n",
      "torch.Size([8, 67]) torch.Size([8, 67])\n",
      "torch.Size([8, 66]) torch.Size([8, 66])\n",
      "torch.Size([8, 83]) torch.Size([8, 83])\n",
      "torch.Size([8, 74]) torch.Size([8, 74])\n",
      "torch.Size([8, 91]) torch.Size([8, 91])\n",
      "torch.Size([8, 69]) torch.Size([8, 69])\n",
      "torch.Size([8, 67]) torch.Size([8, 67])\n",
      "torch.Size([8, 73]) torch.Size([8, 73])\n",
      "torch.Size([8, 74]) torch.Size([8, 74])\n",
      "torch.Size([8, 69]) torch.Size([8, 69])\n",
      "torch.Size([8, 68]) torch.Size([8, 68])\n",
      "torch.Size([8, 65]) torch.Size([8, 65])\n",
      "torch.Size([8, 61]) torch.Size([8, 61])\n",
      "torch.Size([8, 63]) torch.Size([8, 63])\n",
      "torch.Size([8, 69]) torch.Size([8, 69])\n",
      "torch.Size([8, 67]) torch.Size([8, 67])\n",
      "torch.Size([8, 82]) torch.Size([8, 82])\n",
      "torch.Size([8, 65]) torch.Size([8, 65])\n",
      "torch.Size([8, 75]) torch.Size([8, 75])\n",
      "torch.Size([8, 83]) torch.Size([8, 83])\n",
      "torch.Size([8, 83]) torch.Size([8, 83])\n",
      "torch.Size([8, 61]) torch.Size([8, 61])\n",
      "torch.Size([8, 69]) torch.Size([8, 69])\n",
      "torch.Size([8, 63]) torch.Size([8, 63])\n",
      "torch.Size([8, 62]) torch.Size([8, 62])\n",
      "torch.Size([8, 56]) torch.Size([8, 56])\n",
      "torch.Size([8, 69]) torch.Size([8, 69])\n",
      "torch.Size([8, 68]) torch.Size([8, 68])\n",
      "torch.Size([8, 88]) torch.Size([8, 88])\n",
      "torch.Size([8, 57]) torch.Size([8, 57])\n",
      "torch.Size([8, 80]) torch.Size([8, 80])\n",
      "torch.Size([8, 74]) torch.Size([8, 74])\n",
      "torch.Size([8, 66]) torch.Size([8, 66])\n",
      "torch.Size([8, 60]) torch.Size([8, 60])\n",
      "torch.Size([8, 68]) torch.Size([8, 68])\n",
      "torch.Size([8, 80]) torch.Size([8, 80])\n",
      "torch.Size([8, 69]) torch.Size([8, 69])\n",
      "torch.Size([8, 76]) torch.Size([8, 76])\n",
      "torch.Size([8, 76]) torch.Size([8, 76])\n",
      "torch.Size([8, 80]) torch.Size([8, 80])\n",
      "torch.Size([8, 61]) torch.Size([8, 61])\n",
      "torch.Size([8, 63]) torch.Size([8, 63])\n",
      "torch.Size([8, 72]) torch.Size([8, 72])\n",
      "torch.Size([8, 62]) torch.Size([8, 62])\n",
      "torch.Size([8, 62]) torch.Size([8, 62])\n",
      "torch.Size([8, 69]) torch.Size([8, 69])\n",
      "torch.Size([8, 68]) torch.Size([8, 68])\n",
      "torch.Size([8, 63]) torch.Size([8, 63])\n",
      "torch.Size([8, 72]) torch.Size([8, 72])\n",
      "torch.Size([8, 69]) torch.Size([8, 69])\n",
      "torch.Size([8, 71]) torch.Size([8, 71])\n",
      "torch.Size([8, 67]) torch.Size([8, 67])\n",
      "torch.Size([8, 71]) torch.Size([8, 71])\n",
      "torch.Size([8, 76]) torch.Size([8, 76])\n",
      "torch.Size([8, 83]) torch.Size([8, 83])\n",
      "torch.Size([8, 67]) torch.Size([8, 67])\n",
      "torch.Size([8, 60]) torch.Size([8, 60])\n",
      "torch.Size([8, 71]) torch.Size([8, 71])\n",
      "torch.Size([8, 71]) torch.Size([8, 71])\n",
      "torch.Size([8, 65]) torch.Size([8, 65])\n",
      "torch.Size([8, 66]) torch.Size([8, 66])\n",
      "torch.Size([8, 91]) torch.Size([8, 91])\n",
      "torch.Size([8, 68]) torch.Size([8, 68])\n",
      "torch.Size([8, 83]) torch.Size([8, 83])\n",
      "torch.Size([8, 60]) torch.Size([8, 60])\n",
      "torch.Size([8, 56]) torch.Size([8, 56])\n",
      "torch.Size([8, 65]) torch.Size([8, 65])\n",
      "torch.Size([8, 64]) torch.Size([8, 64])\n",
      "torch.Size([8, 68]) torch.Size([8, 68])\n",
      "torch.Size([8, 67]) torch.Size([8, 67])\n",
      "torch.Size([8, 83]) torch.Size([8, 83])\n",
      "torch.Size([8, 67]) torch.Size([8, 67])\n",
      "torch.Size([8, 81]) torch.Size([8, 81])\n",
      "torch.Size([8, 71]) torch.Size([8, 71])\n",
      "torch.Size([8, 79]) torch.Size([8, 79])\n",
      "torch.Size([8, 67]) torch.Size([8, 67])\n",
      "torch.Size([8, 74]) torch.Size([8, 74])\n",
      "torch.Size([8, 68]) torch.Size([8, 68])\n",
      "torch.Size([8, 91]) torch.Size([8, 91])\n",
      "torch.Size([8, 60]) torch.Size([8, 60])\n",
      "torch.Size([8, 68]) torch.Size([8, 68])\n",
      "torch.Size([8, 59]) torch.Size([8, 59])\n",
      "torch.Size([8, 75]) torch.Size([8, 75])\n",
      "torch.Size([8, 70]) torch.Size([8, 70])\n",
      "torch.Size([8, 65]) torch.Size([8, 65])\n",
      "torch.Size([8, 72]) torch.Size([8, 72])\n",
      "torch.Size([8, 74]) torch.Size([8, 74])\n",
      "torch.Size([8, 69]) torch.Size([8, 69])\n",
      "torch.Size([8, 68]) torch.Size([8, 68])\n",
      "torch.Size([8, 64]) torch.Size([8, 64])\n",
      "torch.Size([8, 62]) torch.Size([8, 62])\n",
      "torch.Size([8, 66]) torch.Size([8, 66])\n",
      "torch.Size([8, 77]) torch.Size([8, 77])\n",
      "torch.Size([8, 75]) torch.Size([8, 75])\n",
      "torch.Size([8, 87]) torch.Size([8, 87])\n",
      "torch.Size([8, 77]) torch.Size([8, 77])\n",
      "torch.Size([8, 70]) torch.Size([8, 70])\n",
      "torch.Size([8, 76]) torch.Size([8, 76])\n",
      "torch.Size([8, 63]) torch.Size([8, 63])\n",
      "torch.Size([8, 77]) torch.Size([8, 77])\n"
     ]
    }
   ],
   "source": [
    "print(\"Train loader:\")\n",
    "for inputs, targets in train_loader:\n",
    "    print(inputs.shape, targets.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7.5 加载预训练模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GPTModel(\n",
       "  (tok_emb): Embedding(50257, 1024)\n",
       "  (pos_emb): Embedding(1024, 1024)\n",
       "  (drop_emb): Dropout(p=0.0, inplace=False)\n",
       "  (trf_blocks): Sequential(\n",
       "    (0): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        (W_key): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        (W_value): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        (dropout): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layer): Sequential(\n",
       "          (0): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_resid): Dropout(p=0.0, inplace=False)\n",
       "    )\n",
       "    (1): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        (W_key): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        (W_value): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        (dropout): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layer): Sequential(\n",
       "          (0): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_resid): Dropout(p=0.0, inplace=False)\n",
       "    )\n",
       "    (2): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        (W_key): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        (W_value): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        (dropout): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layer): Sequential(\n",
       "          (0): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_resid): Dropout(p=0.0, inplace=False)\n",
       "    )\n",
       "    (3): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        (W_key): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        (W_value): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        (dropout): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layer): Sequential(\n",
       "          (0): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_resid): Dropout(p=0.0, inplace=False)\n",
       "    )\n",
       "    (4): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        (W_key): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        (W_value): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        (dropout): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layer): Sequential(\n",
       "          (0): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_resid): Dropout(p=0.0, inplace=False)\n",
       "    )\n",
       "    (5): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        (W_key): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        (W_value): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        (dropout): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layer): Sequential(\n",
       "          (0): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_resid): Dropout(p=0.0, inplace=False)\n",
       "    )\n",
       "    (6): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        (W_key): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        (W_value): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        (dropout): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layer): Sequential(\n",
       "          (0): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_resid): Dropout(p=0.0, inplace=False)\n",
       "    )\n",
       "    (7): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        (W_key): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        (W_value): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        (dropout): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layer): Sequential(\n",
       "          (0): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_resid): Dropout(p=0.0, inplace=False)\n",
       "    )\n",
       "    (8): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        (W_key): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        (W_value): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        (dropout): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layer): Sequential(\n",
       "          (0): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_resid): Dropout(p=0.0, inplace=False)\n",
       "    )\n",
       "    (9): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        (W_key): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        (W_value): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        (dropout): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layer): Sequential(\n",
       "          (0): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_resid): Dropout(p=0.0, inplace=False)\n",
       "    )\n",
       "    (10): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        (W_key): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        (W_value): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        (dropout): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layer): Sequential(\n",
       "          (0): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_resid): Dropout(p=0.0, inplace=False)\n",
       "    )\n",
       "    (11): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        (W_key): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        (W_value): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        (dropout): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layer): Sequential(\n",
       "          (0): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_resid): Dropout(p=0.0, inplace=False)\n",
       "    )\n",
       "    (12): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        (W_key): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        (W_value): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        (dropout): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layer): Sequential(\n",
       "          (0): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_resid): Dropout(p=0.0, inplace=False)\n",
       "    )\n",
       "    (13): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        (W_key): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        (W_value): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        (dropout): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layer): Sequential(\n",
       "          (0): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_resid): Dropout(p=0.0, inplace=False)\n",
       "    )\n",
       "    (14): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        (W_key): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        (W_value): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        (dropout): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layer): Sequential(\n",
       "          (0): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_resid): Dropout(p=0.0, inplace=False)\n",
       "    )\n",
       "    (15): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        (W_key): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        (W_value): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        (dropout): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layer): Sequential(\n",
       "          (0): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_resid): Dropout(p=0.0, inplace=False)\n",
       "    )\n",
       "    (16): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        (W_key): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        (W_value): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        (dropout): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layer): Sequential(\n",
       "          (0): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_resid): Dropout(p=0.0, inplace=False)\n",
       "    )\n",
       "    (17): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        (W_key): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        (W_value): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        (dropout): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layer): Sequential(\n",
       "          (0): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_resid): Dropout(p=0.0, inplace=False)\n",
       "    )\n",
       "    (18): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        (W_key): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        (W_value): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        (dropout): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layer): Sequential(\n",
       "          (0): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_resid): Dropout(p=0.0, inplace=False)\n",
       "    )\n",
       "    (19): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        (W_key): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        (W_value): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        (dropout): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layer): Sequential(\n",
       "          (0): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_resid): Dropout(p=0.0, inplace=False)\n",
       "    )\n",
       "    (20): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        (W_key): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        (W_value): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        (dropout): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layer): Sequential(\n",
       "          (0): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_resid): Dropout(p=0.0, inplace=False)\n",
       "    )\n",
       "    (21): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        (W_key): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        (W_value): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        (dropout): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layer): Sequential(\n",
       "          (0): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_resid): Dropout(p=0.0, inplace=False)\n",
       "    )\n",
       "    (22): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        (W_key): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        (W_value): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        (dropout): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layer): Sequential(\n",
       "          (0): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_resid): Dropout(p=0.0, inplace=False)\n",
       "    )\n",
       "    (23): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        (W_key): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        (W_value): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        (dropout): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layer): Sequential(\n",
       "          (0): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_resid): Dropout(p=0.0, inplace=False)\n",
       "    )\n",
       "  )\n",
       "  (final_norm): LayerNorm()\n",
       "  (out_head): Linear(in_features=1024, out_features=50257, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 197,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "BASE_CONFIG = {\n",
    "    \"vocab_size\": 50257,     # Vocabulary size\n",
    "    \"context_length\": 1024,  # Context length\n",
    "    \"drop_rate\": 0.0,        # Dropout rate\n",
    "    \"qkv_bias\": True         # Query-key-value bias\n",
    "}\n",
    "\n",
    "model_configs = {\n",
    "    \"gpt2-small (124M)\": {\"emb_dim\": 768, \"n_layers\": 12, \"n_heads\": 12},\n",
    "    \"gpt2-medium (355M)\": {\"emb_dim\": 1024, \"n_layers\": 24, \"n_heads\": 16},\n",
    "    \"gpt2-large (774M)\": {\"emb_dim\": 1280, \"n_layers\": 36, \"n_heads\": 20},\n",
    "    \"gpt2-xl (1558M)\": {\"emb_dim\": 1600, \"n_layers\": 48, \"n_heads\": 25},\n",
    "}\n",
    "\n",
    "CHOOSE_MODEL = \"gpt2-medium (355M)\"\n",
    "BASE_CONFIG.update(model_configs[CHOOSE_MODEL])\n",
    "model = GPTModel(BASE_CONFIG)\n",
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GPT2Model(\n",
       "  (wte): Embedding(50257, 1024)\n",
       "  (wpe): Embedding(1024, 1024)\n",
       "  (drop): Dropout(p=0.1, inplace=False)\n",
       "  (h): ModuleList(\n",
       "    (0-23): 24 x GPT2Block(\n",
       "      (ln_1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "      (attn): GPT2Attention(\n",
       "        (c_attn): Conv1D()\n",
       "        (c_proj): Conv1D()\n",
       "        (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "        (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ln_2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "      (mlp): GPT2MLP(\n",
       "        (c_fc): Conv1D()\n",
       "        (c_proj): Conv1D()\n",
       "        (act): NewGELUActivation()\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (ln_f): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       ")"
      ]
     },
     "execution_count": 198,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gpt_model = GPT2Model.from_pretrained(\"./gpt2-medium\")\n",
    "gpt_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\xuexiaolei\\AppData\\Local\\Temp\\ipykernel_36672\\2213428617.py:6: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  left.copy_(torch.tensor(right))\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "GPTModel(\n",
       "  (tok_emb): Embedding(50257, 1024)\n",
       "  (pos_emb): Embedding(1024, 1024)\n",
       "  (drop_emb): Dropout(p=0.0, inplace=False)\n",
       "  (trf_blocks): Sequential(\n",
       "    (0): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        (W_key): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        (W_value): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        (dropout): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layer): Sequential(\n",
       "          (0): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_resid): Dropout(p=0.0, inplace=False)\n",
       "    )\n",
       "    (1): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        (W_key): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        (W_value): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        (dropout): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layer): Sequential(\n",
       "          (0): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_resid): Dropout(p=0.0, inplace=False)\n",
       "    )\n",
       "    (2): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        (W_key): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        (W_value): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        (dropout): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layer): Sequential(\n",
       "          (0): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_resid): Dropout(p=0.0, inplace=False)\n",
       "    )\n",
       "    (3): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        (W_key): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        (W_value): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        (dropout): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layer): Sequential(\n",
       "          (0): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_resid): Dropout(p=0.0, inplace=False)\n",
       "    )\n",
       "    (4): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        (W_key): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        (W_value): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        (dropout): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layer): Sequential(\n",
       "          (0): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_resid): Dropout(p=0.0, inplace=False)\n",
       "    )\n",
       "    (5): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        (W_key): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        (W_value): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        (dropout): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layer): Sequential(\n",
       "          (0): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_resid): Dropout(p=0.0, inplace=False)\n",
       "    )\n",
       "    (6): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        (W_key): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        (W_value): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        (dropout): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layer): Sequential(\n",
       "          (0): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_resid): Dropout(p=0.0, inplace=False)\n",
       "    )\n",
       "    (7): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        (W_key): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        (W_value): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        (dropout): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layer): Sequential(\n",
       "          (0): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_resid): Dropout(p=0.0, inplace=False)\n",
       "    )\n",
       "    (8): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        (W_key): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        (W_value): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        (dropout): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layer): Sequential(\n",
       "          (0): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_resid): Dropout(p=0.0, inplace=False)\n",
       "    )\n",
       "    (9): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        (W_key): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        (W_value): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        (dropout): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layer): Sequential(\n",
       "          (0): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_resid): Dropout(p=0.0, inplace=False)\n",
       "    )\n",
       "    (10): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        (W_key): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        (W_value): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        (dropout): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layer): Sequential(\n",
       "          (0): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_resid): Dropout(p=0.0, inplace=False)\n",
       "    )\n",
       "    (11): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        (W_key): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        (W_value): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        (dropout): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layer): Sequential(\n",
       "          (0): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_resid): Dropout(p=0.0, inplace=False)\n",
       "    )\n",
       "    (12): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        (W_key): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        (W_value): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        (dropout): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layer): Sequential(\n",
       "          (0): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_resid): Dropout(p=0.0, inplace=False)\n",
       "    )\n",
       "    (13): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        (W_key): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        (W_value): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        (dropout): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layer): Sequential(\n",
       "          (0): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_resid): Dropout(p=0.0, inplace=False)\n",
       "    )\n",
       "    (14): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        (W_key): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        (W_value): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        (dropout): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layer): Sequential(\n",
       "          (0): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_resid): Dropout(p=0.0, inplace=False)\n",
       "    )\n",
       "    (15): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        (W_key): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        (W_value): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        (dropout): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layer): Sequential(\n",
       "          (0): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_resid): Dropout(p=0.0, inplace=False)\n",
       "    )\n",
       "    (16): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        (W_key): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        (W_value): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        (dropout): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layer): Sequential(\n",
       "          (0): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_resid): Dropout(p=0.0, inplace=False)\n",
       "    )\n",
       "    (17): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        (W_key): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        (W_value): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        (dropout): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layer): Sequential(\n",
       "          (0): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_resid): Dropout(p=0.0, inplace=False)\n",
       "    )\n",
       "    (18): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        (W_key): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        (W_value): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        (dropout): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layer): Sequential(\n",
       "          (0): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_resid): Dropout(p=0.0, inplace=False)\n",
       "    )\n",
       "    (19): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        (W_key): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        (W_value): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        (dropout): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layer): Sequential(\n",
       "          (0): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_resid): Dropout(p=0.0, inplace=False)\n",
       "    )\n",
       "    (20): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        (W_key): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        (W_value): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        (dropout): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layer): Sequential(\n",
       "          (0): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_resid): Dropout(p=0.0, inplace=False)\n",
       "    )\n",
       "    (21): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        (W_key): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        (W_value): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        (dropout): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layer): Sequential(\n",
       "          (0): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_resid): Dropout(p=0.0, inplace=False)\n",
       "    )\n",
       "    (22): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        (W_key): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        (W_value): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        (dropout): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layer): Sequential(\n",
       "          (0): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_resid): Dropout(p=0.0, inplace=False)\n",
       "    )\n",
       "    (23): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        (W_key): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        (W_value): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        (dropout): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layer): Sequential(\n",
       "          (0): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_resid): Dropout(p=0.0, inplace=False)\n",
       "    )\n",
       "  )\n",
       "  (final_norm): LayerNorm()\n",
       "  (out_head): Linear(in_features=1024, out_features=50257, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 199,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "load_weights_into_gpt(model, gpt_model)\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Below is an instruction that describes a task. Write a response that appropriately completes the request.\n",
      "\n",
      "### Instruction:\n",
      "Convert the active sentence to passive: 'The chef cooks the meal every day.'\n"
     ]
    }
   ],
   "source": [
    "input_text = format_input(val_data[0])\n",
    "print(input_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "metadata": {},
   "outputs": [],
   "source": [
    "token_ids = generate(\n",
    "    model=model,\n",
    "    idx=text_to_token_ids(input_text, tokenizer),\n",
    "    max_new_tokens=35,\n",
    "    context_size=BASE_CONFIG[\"context_length\"],\n",
    "    eos_id=50256,\n",
    ")\n",
    "generated_text = token_ids_to_text(token_ids, tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "### Response:\n",
      "\n",
      "The chef cooks the meal every day.\n",
      "\n",
      "### Instruction:\n",
      "\n",
      "Convert the active sentence to passive: 'The chef cooks the\n"
     ]
    }
   ],
   "source": [
    "response_text = generated_text[len(input_text):].strip()\n",
    "print(response_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7.6 微调指令大模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training loss: 3.8216689109802244\n",
      "Validation loss: 3.7619208812713625\n"
     ]
    }
   ],
   "source": [
    "model.to(device)\n",
    "\n",
    "\n",
    "def calc_loss_batch(input_batch, target_batch, model, device, loss_func):\n",
    "    input_batch, target_batch = input_batch.to(device), target_batch.to(device)\n",
    "    logits = model(input_batch)\n",
    "    loss = loss_func(logits.flatten(0, 1), target_batch.flatten())\n",
    "    return loss\n",
    "\n",
    "\n",
    "def calc_loss_loader(data_loader, model, device, num_batches=None, loss_func=torch.nn.functional.cross_entropy):\n",
    "    total_loss = 0.\n",
    "    if len(data_loader) == 0:\n",
    "        return float(\"nan\")\n",
    "    elif num_batches is None:\n",
    "        num_batches = len(data_loader)\n",
    "    else:\n",
    "        # Reduce the number of batches to match the total number of batches in the data loader\n",
    "        # if num_batches exceeds the number of batches in the data loader\n",
    "        num_batches = min(num_batches, len(data_loader))\n",
    "    for i, (input_batch, target_batch) in enumerate(data_loader):\n",
    "        if i < num_batches:\n",
    "            loss = calc_loss_batch(input_batch, target_batch, model, device, loss_func)\n",
    "            total_loss += loss.item()\n",
    "        else:\n",
    "            break\n",
    "    return total_loss / num_batches\n",
    "\n",
    "\n",
    "with torch.no_grad():\n",
    "    train_loss = calc_loss_loader(train_loader, model, device, num_batches=5)\n",
    "    val_loss = calc_loss_loader(val_loader, model, device, num_batches=5)\n",
    "\n",
    "print(\"Training loss:\", train_loss)\n",
    "print(\"Validation loss:\", val_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">> Epoch 1, step  00000: train loss 2.5283, val loss 2.5044\n",
      ">> Epoch 1, step  00005: train loss 1.0608, val loss 1.0728\n",
      ">> Epoch 1, step  00010: train loss 0.8761, val loss 0.9573\n",
      ">> Epoch 1, step  00015: train loss 0.8720, val loss 0.9120\n",
      ">> Epoch 1, step  00020: train loss 0.8390, val loss 0.8811\n",
      ">> Epoch 1, step  00025: train loss 0.7663, val loss 0.8519\n",
      ">> Epoch 1, step  00030: train loss 0.7216, val loss 0.8435\n",
      ">> Epoch 1, step  00035: train loss 0.6777, val loss 0.8131\n",
      ">> Epoch 1, step  00040: train loss 0.7298, val loss 0.8059\n",
      ">> Epoch 1, step  00045: train loss 0.6232, val loss 0.7914\n",
      ">> Epoch 1, step  00050: train loss 0.5969, val loss 0.7729\n",
      ">> Epoch 1, step  00055: train loss 0.5428, val loss 0.7549\n",
      ">> Epoch 1, step  00060: train loss 0.6150, val loss 0.7428\n",
      ">> Epoch 1, step  00065: train loss 0.7551, val loss 0.7413\n",
      ">> Epoch 1, step  00070: train loss 0.5588, val loss 0.7250\n",
      ">> Epoch 1, step  00075: train loss 0.6557, val loss 0.7150\n",
      ">> Epoch 1, step  00080: train loss 0.5558, val loss 0.7070\n",
      ">> Epoch 1, step  00085: train loss 0.4991, val loss 0.7027\n",
      ">> Epoch 1, step  00090: train loss 0.5249, val loss 0.7002\n",
      ">> Epoch 1, step  00095: train loss 0.5262, val loss 0.6911\n",
      ">> Epoch 1, step  00100: train loss 0.5047, val loss 0.6902\n",
      ">> Epoch 1, step  00105: train loss 0.4423, val loss 0.6811\n",
      ">> Epoch 1, step  00110: train loss 0.4256, val loss 0.6805\n",
      ">> Epoch 1, step  00115: train loss 0.4554, val loss 0.6679\n",
      ">> Below is an instruction that describes a task. Write a response that appropriately completes the request.\n",
      "\n",
      "### Instruction:\n",
      "Convert the active sentence to passive: 'The chef cooks the meal every day.' --> Below is an instruction that describes a task. Write a response that appropriately completes the request.\n",
      "\n",
      "### Instruction:\n",
      "Convert the active sentence to passive: 'The chef cooks the meal every day.'\n",
      "\n",
      "### Response:\n",
      "The chef cooks the meal every day.<|endoftext|>The following is an instruction that describes a task. Write a response that appropriately completes the request.\n",
      "\n",
      "### Instruction:\n",
      "What is the opposite of 'fast'?\n",
      "\n",
      "\n",
      ">> Epoch 2, step  00120: train loss 0.4505, val loss 0.6600\n",
      ">> Epoch 2, step  00125: train loss 0.4682, val loss 0.6697\n",
      ">> Epoch 2, step  00130: train loss 0.3777, val loss 0.6798\n",
      ">> Epoch 2, step  00135: train loss 0.3979, val loss 0.6904\n",
      ">> Epoch 2, step  00140: train loss 0.3931, val loss 0.6776\n",
      ">> Epoch 2, step  00145: train loss 0.3931, val loss 0.6686\n",
      ">> Epoch 2, step  00150: train loss 0.4745, val loss 0.6635\n",
      ">> Epoch 2, step  00155: train loss 0.3631, val loss 0.6654\n",
      ">> Epoch 2, step  00160: train loss 0.4047, val loss 0.6713\n",
      ">> Epoch 2, step  00165: train loss 0.4194, val loss 0.6732\n",
      ">> Epoch 2, step  00170: train loss 0.3359, val loss 0.6650\n",
      ">> Epoch 2, step  00175: train loss 0.3622, val loss 0.6696\n",
      ">> Epoch 2, step  00180: train loss 0.4339, val loss 0.6709\n",
      ">> Epoch 2, step  00185: train loss 0.3707, val loss 0.6663\n",
      ">> Epoch 2, step  00190: train loss 0.3704, val loss 0.6624\n",
      ">> Epoch 2, step  00195: train loss 0.3552, val loss 0.6496\n",
      ">> Epoch 2, step  00200: train loss 0.3264, val loss 0.6386\n",
      ">> Epoch 2, step  00205: train loss 0.3431, val loss 0.6368\n",
      ">> Epoch 2, step  00210: train loss 0.3108, val loss 0.6307\n",
      ">> Epoch 2, step  00215: train loss 0.3241, val loss 0.6300\n",
      ">> Epoch 2, step  00220: train loss 0.3707, val loss 0.6312\n",
      ">> Epoch 2, step  00225: train loss 0.2948, val loss 0.6296\n",
      ">> Epoch 2, step  00230: train loss 0.3128, val loss 0.6314\n",
      ">> Below is an instruction that describes a task. Write a response that appropriately completes the request.\n",
      "\n",
      "### Instruction:\n",
      "Convert the active sentence to passive: 'The chef cooks the meal every day.' --> Below is an instruction that describes a task. Write a response that appropriately completes the request.\n",
      "\n",
      "### Instruction:\n",
      "Convert the active sentence to passive: 'The chef cooks the meal every day.'\n",
      "\n",
      "### Response:\n",
      "The meal is cooked every day by the chef.<|endoftext|>The following is an instruction that describes a task. Write a response that appropriately completes the request.\n",
      "\n",
      "### Instruction:\n",
      "What is the opposite of 'expensive'?\n",
      "Training completed in 883.96 minutes.\n"
     ]
    }
   ],
   "source": [
    "start_time = time.time()\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=0.00005, weight_decay=0.1)\n",
    "num_epochs = 2\n",
    "train_losses, val_losses, tokens_seen = train_model_simple(\n",
    "    model, train_loader, val_loader, optimizer, device,\n",
    "    num_epochs=num_epochs, eval_freq=5, eval_iter=5,\n",
    "    start_context=format_input(val_data[0]), tokenizer=tokenizer\n",
    ")\n",
    "\n",
    "end_time = time.time()\n",
    "execution_time_minutes = (end_time - start_time) / 60\n",
    "print(f\"Training completed in {execution_time_minutes:.2f} minutes.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAA90AAAHpCAYAAACful8UAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjYuMywgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/P9b71AAAACXBIWXMAAA9hAAAPYQGoP6dpAACLWUlEQVR4nOzdd3wUdf7H8ffupveeEEjovXfBhoqiIiJ2xX56d4qFs57n2X+K9dSzexbs9RQ9O9JERTpIDZ1ASCW9J7vz+2OSTQIEkpBkNsnr+XjMY3dnZ2c/m5lA3vv9zvdrMwzDEAAAAAAAaHZ2qwsAAAAAAKC9InQDAAAAANBCCN0AAAAAALQQQjcAAAAAAC2E0A0AAAAAQAshdAMAAAAA0EII3QAAAAAAtBBCNwAAAAAALYTQDQAAAABACyF0AwDQRuzatUs2m01r1qyxuhQAANBAhG4AAFqRzWY77PLAAw9YXWKjZGZm6vrrr1diYqJ8fX0VFxenSZMm6ddff7W6NAAAPIKX1QUAANCRpKamuu9//PHHuu+++5SUlOReFxQUZEVZTXbeeeepvLxcb7/9tnr06KH09HTNmzdP+/fvt7o0AAA8Ai3dAAC0ori4OPcSGhoqm83mfhwTE6N//etf6tKli3x9fTVs2DB9//339e7L6XTqmmuuUb9+/ZScnCxJ+vLLLzVixAj5+fmpR48eevDBB1VZWel+jc1m0+uvv65p06YpICBAvXv31ldffeV+PicnR9OnT1d0dLT8/f3Vu3dvvfXWW4d8/9zcXC1evFiPP/64TjrpJHXt2lVjxozR3XffrbPPPrvOdtdee62io6MVEhKik08+WWvXrq2zr6OtGwAAT0XoBgDAQzz33HN6+umn9dRTT+mPP/7QpEmTdPbZZ2vr1q0HbVtWVqYLLrhAa9as0eLFi5WYmKjFixfriiuu0C233KKNGzfq1Vdf1ezZs/XII4/Uee2DDz6oCy+8UH/88YfOPPNMTZ8+XdnZ2ZKke++9Vxs3btR3332nTZs26eWXX1ZUVNQh6w0KClJQUJDmzJmjsrKyej/XBRdcoIyMDH333XdauXKlRowYoVNOOcX9ns1RNwAAHssAAACWeOutt4zQ0FD34/j4eOORRx6ps83o0aONG264wTAMw9i5c6chyVi8eLFxyimnGMcdd5yRm5vr3vaUU04xHn300Tqvf/fdd41OnTq5H0sy/vnPf7ofFxYWGpKM7777zjAMw5gyZYpx9dVXN/gzfPbZZ0Z4eLjh5+dnjB8/3rj77ruNtWvXup9fvHixERISYpSWltZ5Xc+ePY1XX3212eoGAMBT0dINAIAHyM/P1759+3TsscfWWX/sscdq06ZNddZdcsklKioq0o8//qjQ0FD3+rVr1+qhhx5yt0AHBQXpuuuuU2pqqoqLi93bDRkyxH0/MDBQISEhysjIkCRdf/31+uijjzRs2DDdeeed+u233w5b93nnnad9+/bpq6++0umnn66FCxdqxIgRmj17trumwsJCRUZG1qlr586d2r59e7PVDQCAp2IgNQAA2pgzzzxT7733npYsWaKTTz7Zvb6wsFAPPvigzj333INe4+fn577v7e1d5zmbzSaXyyVJOuOMM7R79259++23mjt3rk455RTNmDFDTz31VL31+Pn56dRTT9Wpp56qe++9V9dee63uv/9+XXXVVSosLFSnTp20cOHCg14XFhbWbHUDAOCpCN0AAHiAkJAQxcfH69dff9WJJ57oXv/rr79qzJgxdba9/vrrNWjQIJ199tn65ptv3NuPGDFCSUlJ6tWr11HVEh0drSuvvFJXXnmljj/+eN1xxx2HDd0HGjBggObMmeOuKS0tTV5eXurWrdsht2+uugEA8ESEbgAAPMQdd9yh+++/Xz179tSwYcP01ltvac2aNXr//fcP2vamm26S0+nUWWedpe+++07HHXec7rvvPp111llKTEzU+eefL7vdrrVr12r9+vX6v//7vwbVcN9992nkyJEaOHCgysrK9PXXX6t///6H3Hb//v264IILdM0112jIkCEKDg7WihUr9MQTT2jq1KmSpIkTJ2rcuHE655xz9MQTT6hPnz7at2+fvvnmG02bNk2jRo1qlroBAPBUhG4AADzEzTffrLy8PN12223KyMjQgAED9NVXX6l3796H3H7mzJlyuVw688wz9f3332vSpEn6+uuv9dBDD+nxxx+Xt7e3+vXrp2uvvbbBNfj4+Ojuu+/Wrl275O/vr+OPP14fffTRIbcNCgrS2LFj9cwzz2j79u2qqKhQQkKCrrvuOv3jH/+QZHYB//bbb3XPPffo6quvVmZmpuLi4nTCCScoNjZWkpqlbgAAPJXNMAzD6iIAAAAAAGiPGL0cAAAAAIAWQugGAAAAAKCFELoBAAAAAGghhG4AAAAAAFoIoRsAAAAAgBZC6AYAAAAAoIUQuj3Eiy++qG7dusnPz09jx47VsmXLrC4JDTRr1iyNHj1awcHBiomJ0TnnnKOkpKQ625SWlmrGjBmKjIxUUFCQzjvvPKWnp9fZJjk5WZMnT1ZAQIBiYmJ0xx13qLKyss42Cxcu1IgRI+Tr66tevXpp9uzZB9XDueQZHnvsMdlsNs2cOdO9jvOgY0hJSdFll12myMhI+fv7a/DgwVqxYoX7ecMwdN9996lTp07y9/fXxIkTtXXr1jr7yM7O1vTp0xUSEqKwsDD96U9/UmFhYZ1t/vjjDx1//PHy8/NTQkKCnnjiiYNq+fTTT9WvXz/5+flp8ODB+vbbb1vmQ+MgTqdT9957r7p37y5/f3/17NlTDz/8sGrP1Mq50P78/PPPmjJliuLj42Wz2TRnzpw6z3vSMW9ILWiaw50HFRUVuuuuuzR48GAFBgYqPj5eV1xxhfbt21dnH5wH7YwBy3300UeGj4+P8eabbxobNmwwrrvuOiMsLMxIT0+3ujQ0wKRJk4y33nrLWL9+vbFmzRrjzDPPNBITE43CwkL3Nn/961+NhIQEY968ecaKFSuMY445xhg/frz7+crKSmPQoEHGxIkTjdWrVxvffvutERUVZdx9993ubXbs2GEEBAQYt956q7Fx40bj+eefNxwOh/H999+7t+Fc8gzLli0zunXrZgwZMsS45ZZb3Os5D9q/7Oxso2vXrsZVV11lLF261NixY4fxww8/GNu2bXNv89hjjxmhoaHGnDlzjLVr1xpnn3220b17d6OkpMS9zemnn24MHTrU+P33343FixcbvXr1Mi655BL383l5eUZsbKwxffp0Y/369caHH35o+Pv7G6+++qp7m19//dVwOBzGE088YWzcuNH45z//aXh7exvr1q1rnR9GB/fII48YkZGRxtdff23s3LnT+PTTT42goCDjueeec2/DudD+fPvtt8Y999xjfP7554Yk44svvqjzvCcd84bUgqY53HmQm5trTJw40fj444+NzZs3G0uWLDHGjBljjBw5ss4+OA/aF0K3BxgzZowxY8YM92On02nEx8cbs2bNsrAqNFVGRoYhyVi0aJFhGOY/rt7e3sann37q3mbTpk2GJGPJkiWGYZj/ONvtdiMtLc29zcsvv2yEhIQYZWVlhmEYxp133mkMHDiwzntddNFFxqRJk9yPOZesV1BQYPTu3duYO3euceKJJ7pDN+dBx3DXXXcZxx13XL3Pu1wuIy4uznjyySfd63Jzcw1fX1/jww8/NAzDMDZu3GhIMpYvX+7e5rvvvjNsNpuRkpJiGIZhvPTSS0Z4eLj7vKh+7759+7ofX3jhhcbkyZPrvP/YsWONv/zlL0f3IdEgkydPNq655po6684991xj+vTphmFwLnQEB4YtTzrmDakFzeNQX74caNmyZYYkY/fu3YZhcB60R3Qvt1h5eblWrlypiRMnutfZ7XZNnDhRS5YssbAyNFVeXp4kKSIiQpK0cuVKVVRU1DnG/fr1U2JiovsYL1myRIMHD1ZsbKx7m0mTJik/P18bNmxwb1N7H9XbVO+Dc8kzzJgxQ5MnTz7oWHEedAxfffWVRo0apQsuuEAxMTEaPny4/vOf/7if37lzp9LS0uocn9DQUI0dO7bOeRAWFqZRo0a5t5k4caLsdruWLl3q3uaEE06Qj4+Pe5tJkyYpKSlJOTk57m0Od66gZY0fP17z5s3Tli1bJElr167VL7/8ojPOOEMS50JH5EnHvCG1oPXk5eXJZrMpLCxMEudBe0TotlhWVpacTmedP7IlKTY2VmlpaRZVhaZyuVyaOXOmjj32WA0aNEiSlJaWJh8fH/c/pNVqH+O0tLRDngPVzx1um/z8fJWUlHAueYCPPvpIq1at0qxZsw56jvOgY9ixY4defvll9e7dWz/88IOuv/563XzzzXr77bcl1RzHwx2ftLQ0xcTE1Hney8tLERERzXKucB60jr///e+6+OKL1a9fP3l7e2v48OGaOXOmpk+fLolzoSPypGPekFrQOkpLS3XXXXfpkksuUUhIiCTOg/bIy+oCgPZkxowZWr9+vX755RerS0Er27Nnj2655RbNnTtXfn5+VpcDi7hcLo0aNUqPPvqoJGn48OFav369XnnlFV155ZUWV4fW9Mknn+j999/XBx98oIEDB2rNmjWaOXOm4uPjORcASDIHVbvwwgtlGIZefvllq8tBC6Kl22JRUVFyOBwHjWCcnp6uuLg4i6pCU9x44436+uuvtWDBAnXp0sW9Pi4uTuXl5crNza2zfe1jHBcXd8hzoPq5w20TEhIif39/ziWLrVy5UhkZGRoxYoS8vLzk5eWlRYsW6d///re8vLwUGxvLedABdOrUSQMGDKizrn///kpOTpZUcxwPd3zi4uKUkZFR5/nKykplZ2c3y7nCedA67rjjDndr9+DBg3X55Zfrb3/7m7snDOdCx+NJx7whtaBlVQfu3bt3a+7cue5WbonzoD0idFvMx8dHI0eO1Lx589zrXC6X5s2bp3HjxllYGRrKMAzdeOON+uKLLzR//nx17969zvMjR46Ut7d3nWOclJSk5ORk9zEeN26c1q1bV+cf2Op/gKv/gB83blydfVRvU70PziVrnXLKKVq3bp3WrFnjXkaNGqXp06e773MetH/HHnvsQVMGbtmyRV27dpUkde/eXXFxcXWOT35+vpYuXVrnPMjNzdXKlSvd28yfP18ul0tjx451b/Pzzz+roqLCvc3cuXPVt29fhYeHu7c53LmCllVcXCy7ve6fWQ6HQy6XSxLnQkfkSce8IbWg5VQH7q1bt+qnn35SZGRknec5D9ohq0dygzm9j6+vrzF79mxj48aNxp///GcjLCyszgjG8FzXX3+9ERoaaixcuNBITU11L8XFxe5t/vrXvxqJiYnG/PnzjRUrVhjjxo0zxo0b536+eqqo0047zVizZo3x/fffG9HR0YecKuqOO+4wNm3aZLz44ouHnCqKc8lz1B693DA4DzqCZcuWGV5eXsYjjzxibN261Xj//feNgIAA47333nNv89hjjxlhYWHGl19+afzxxx/G1KlTDzll0PDhw42lS5cav/zyi9G7d+86U8Xk5uYasbGxxuWXX26sX7/e+Oijj4yAgICDporx8vIynnrqKWPTpk3G/fffzzRRrejKK680Onfu7J4y7PPPPzeioqKMO++8070N50L7U1BQYKxevdpYvXq1Icn417/+Zaxevdo9KrUnHfOG1IKmOdx5UF5ebpx99tlGly5djDVr1tT527H2SOScB+0LodtDPP/880ZiYqLh4+NjjBkzxvj999+tLgkNJOmQy1tvveXepqSkxLjhhhuM8PBwIyAgwJg2bZqRmppaZz+7du0yzjjjDMPf39+IiooybrvtNqOioqLONgsWLDCGDRtm+Pj4GD169KjzHtU4lzzHgaGb86Bj+N///mcMGjTI8PX1Nfr162e89tprdZ53uVzGvffea8TGxhq+vr7GKaecYiQlJdXZZv/+/cYll1xiBAUFGSEhIcbVV19tFBQU1Nlm7dq1xnHHHWf4+voanTt3Nh577LGDavnkk0+MPn36GD4+PsbAgQONb775pvk/MA4pPz/fuOWWW4zExETDz8/P6NGjh3HPPffU+aOac6H9WbBgwSH/JrjyyisNw/CsY96QWtA0hzsPdu7cWe/fjgsWLHDvg/OgfbEZhmG0Xrs6AAAAAAAdB9d0AwAAAADQQgjdAAAAAAC0EEI3AAAAAAAthNANAAAAAEALIXQDAAAAANBCCN0epKysTA888IDKysqsLgUW4jyAxHkAE+cBqnEuQOI8gInzoO1hyjAPkp+fr9DQUOXl5SkkJMTqcmARzgNInAcwcR6gGucCJM4DmDgP2h5augEAAAAAaCGEbgAAAAAAWoiX1QW0tsrKSq1evVqxsbGy2z3rO4eCggJJUkpKivLz8y2uBlbhPIDEeQAT5wGqcS5A4jyAifPAc7hcLqWnp2v48OHy8qo/Wne4a7qXL1+uMWPGWF0GAAAAAKAdWLZsmUaPHl3v8x2upTs2NlaS+YPp1KmTxdUAAAAAANqi1NRUjRkzxp0x69PhQnd1l/JOnTqpS5cuFlcDAAAAAGjLjnTZsmdd1AwAAAAAQDtC6AYAAAAAoIUQugEAAAAAaCEd7ppuAAAAAOgoXC6XysvLrS6jTfL29pbD4Tjq/RC6AQAAAKAdKi8v186dO+Vyuawupc0KCwtTXFycbDZbk/dB6AYAAACAdsYwDKWmpsrhcCghIeGII2yjLsMwVFxcrIyMDEk6qummCd0AAAAA0M5UVlaquLhY8fHxCggIsLqcNsnf31+SlJGRoZiYmCZ3NefrDgAAAABoZ5xOpyTJx8fH4kratuovLCoqKpq8D0I3AAAAALRTR3MtMprn50foBgAAAACghRC6AQAAAABoIYRuAAAAAEC7061bNz377LNWl8Ho5QAAAAAAzzBhwgQNGzasWcLy8uXLFRgYePRFHSVauj2Q02VoW0aBlu7YL8MwrC4HAAAAADyCYRiqrKxs0LbR0dEeMV0aodsDlVe6NPFfP+ui135XYVnDTigAAAAAqI9hGCour7RkaWhD4lVXXaVFixbpueeek81mk81m0+zZs2Wz2fTdd99p5MiR8vX11S+//KLt27dr6tSpio2NVVBQkEaPHq2ffvqpzv4O7F5us9n0+uuva9q0aQoICFDv3r311VdfNeeP+ZDoXu6B/H0c8vd2qKTCqZyiCgX7eVtdEgAAAIA2rKTCqQH3/WDJe298aJICfI4cPZ977jlt2bJFgwYN0kMPPSRJ2rBhgyTp73//u5566in16NFD4eHh2rNnj84880w98sgj8vX11TvvvKMpU6YoKSlJiYmJ9b7Hgw8+qCeeeEJPPvmknn/+eU2fPl27d+9WRERE83zYQ7C0pXvWrFkaPXq0goODFRMTo3POOUdJSUmHfU31Nx21Fz8/v1aquPVEBJqT2GcXl1tcCQAAAAC0vNDQUPn4+CggIEBxcXGKi4uTw+GQJD300EM69dRT1bNnT0VERGjo0KH6y1/+okGDBql37956+OGH1bNnzyO2XF911VW65JJL1KtXLz366KMqLCzUsmXLWvRzWdrSvWjRIs2YMUOjR49WZWWl/vGPf+i0007Txo0bD3vBe0hISJ1w3h4nfA8P9FZKbomyi8qsLgUAAABAG+fv7dDGhyZZ9t5Ha9SoUXUeFxYW6oEHHtA333yj1NRUVVZWqqSkRMnJyYfdz5AhQ9z3AwMDFRISooyMjKOu73AsDd3ff/99ncezZ89WTEyMVq5cqRNOOKHe19lsNsXFxTXoPcrKylRWVhNcCwoKmlZsKztDv2qUI0V5eb0kxVpdDgAAAIA2zGazNaiLt6c6sFH29ttv19y5c/XUU0+pV69e8vf31/nnn6/y8sP3FPb2rnvprs1mk8vlavZ6a/OogdTy8vIk6Yj96QsLC9W1a1clJCRo6tSp7n7+hzJr1iyFhoa6lwEDBjRrzS3l6px/6wHvd1SRvdfqUgAAAACgVfj4+MjpdB5xu19//VVXXXWVpk2bpsGDBysuLk67du1q+QKbwGNCt8vl0syZM3Xsscdq0KBB9W7Xt29fvfnmm/ryyy/13nvvyeVyafz48dq799Dh9O6771ZeXp572bhxY0t9hGZV6h0mSSovyLS2EAAAAABoJd26ddPSpUu1a9cuZWVl1dsK3bt3b33++edas2aN1q5dq0svvbTFW6ybymNC94wZM7R+/Xp99NFHh91u3LhxuuKKKzRs2DCdeOKJ+vzzzxUdHa1XX331kNv7+voqJCTEvQQHB7dE+c2uzMds7TcKsyyuBAAAAABax+233y6Hw6EBAwYoOjq63mu0//Wvfyk8PFzjx4/XlClTNGnSJI0YMaKVq20Yj+jUf+ONN+rrr7/Wzz//rC5dujTqtd7e3ho+fLi2bdvWQtVZw+kXIRVIKiZ0AwAAAOgY+vTpoyVLltRZd9VVVx20Xbdu3TR//vw662bMmFHn8YHdzQ81X3hubm6T6mwMS1u6DcPQjTfeqC+++ELz589X9+7dG70Pp9OpdevWqVOnTi1QoXWMgEhJkqM02+JKAAAAAABNZWlL94wZM/TBBx/oyy+/VHBwsNLS0iSZ87P5+/tLkq644gp17txZs2bNkmTOz3bMMceoV69eys3N1ZNPPqndu3fr2muvtexztAR7UJQkyaeM0A0AAAAAbZWlofvll1+WJE2YMKHO+rfeesvdhSA5OVl2e02DfE5Ojq677jqlpaUpPDxcI0eO1G+//dZmRiVvKO/gaEmSf0WutYUAAAAAAJrM0tB9qD71B1q4cGGdx88884yeeeaZFqrIc/iFxkiSglx5croMOew2iysCAAAAADSWx4xejroCwmMlSRHKV15JhcXVAAAAAACagtDtobyCzO7l4bZCZReVW1wNAAAAAKApCN2eqmr08kjlK6eY0A0AAAAAbRGh21MFmqOX+9vKlZeXa20tAAAAAIAmsXQgNRyGT5AK7CHKrfRVQV6O1dUAAAAAAJqAlm5PZbPpwf5f6/jy57TPGWp1NQAAAADg8bp166Znn33W6jLqIHR7sIhAH0lSDgOpAQAAAECbROj2YNWhO5uB1AAAAACgTSJ0e7Bxqe9pjs8/NTD9K6tLAQAAANAelBc1fnFW1rzeWWmuqyhp2H4b4bXXXlN8fLxcLled9VOnTtU111yj7du3a+rUqYqNjVVQUJBGjx6tn376qak/iVbDQGoeLNyZpUT7Dm0t3mN1KQAAAADag0fjG/+aC2ZLA6eZ9zf/T/r0KqnrcdLV39Rs8+xgqXj/wa99IK/hb3PBBbrpppu0YMECnXLKKZKk7Oxsff/99/r2229VWFioM888U4888oh8fX31zjvvaMqUKUpKSlJiYmLjP1croaXbgxX2v0jXlt+mz42TrC4FAAAAAFpUeHi4zjjjDH3wwQfudZ999pmioqJ00kknaejQofrLX/6iQYMGqXfv3nr44YfVs2dPffWVZ/cMpqXbg/klDNNPrjwFlXCYAAAAADSDf+xr/GscvjX3+00x92E7oP125rqjq6vK9OnTdd111+mll16Sr6+v3n//fV188cWy2+0qLCzUAw88oG+++UapqamqrKxUSUmJkpOTm+W9WwppzoNVD6RWWFapskqnfL0cFlcEAAAAoE3zCTy61zu8zKW591tlypQpMgxD33zzjUaPHq3FixfrmWeekSTdfvvtmjt3rp566in16tVL/v7+Ov/881Ve7tkDTxO6PViIUaRpjl/kkFO5xacoNoTQDQAAAKD98vPz07nnnqv3339f27ZtU9++fTVixAhJ0q+//qqrrrpK06aZ15cXFhZq165dFlbbMIRuD2YvStcz3i8p1whUatE/FRviZ3VJAAAAANCipk+frrPOOksbNmzQZZdd5l7fu3dvff7555oyZYpsNpvuvffeg0Y690QMpObJAiIlSWG2IuUUFFtcDAAAAAC0vJNPPlkRERFKSkrSpZde6l7/r3/9S+Hh4Ro/frymTJmiSZMmuVvBPRkt3Z4sIEIu2WSXocLcDElxVlcEAAAAAC3Kbrdr376DB3zr1q2b5s+fX2fdjBkz6jz2xO7mtHR7MrtDxfZgSVJpXobFxQAAAAAAGovQ7eGKvcMkSeV5mdYWAgAAAABoNEK3hyv3CZckOQsJ3QAAAADQ1hC6PVylnzmYmoqzrS0EAAAAANBohG4PZwRESJIcpfstrgQAAABAW2MYhtUltGnNMSUZo5d7OHtgtCTJp4yWbgAAAAAN4+3tLZvNpszMTEVHR8tms1ldUptiGIbKy8uVmZkpu90uHx+fJu+L0O3hvEPM0O1XkWttIQAAAADaDIfDoS5dumjv3r0eOY1WWxEQEKDExETZ7U3vJE7o9nC+VaE7qDJXhmHwDRUAAACABgkKClLv3r1VUVFhdSltksPhkJeX11FnMEK3hwsMj5MkhalAJRVOBfhwyAAAAAA0jMPhkMPhsLqMDo2B1Dycb2i0Cgx/lchH+wvLrS4HAAAAANAINJt6OFv8cJ3q+77S8kv1VXG5EiICrC4JAAAAANBAtHS3AeGB5kh52UW0dAMAAABAW0LobgMiq0J3TjGhGwAAAADaEkJ3GzAj/xl95XOPbKnrrC4FAAAAANAIhO42ILFyp4bYd8qVl2J1KQAAAACARmAgtTZgafcZ+mb1LvVw9LG6FAAAAABAIxC624DCLido3spIeZcFWl0KAAAAAKAR6F7eBoQHVI1ezkBqAAAAANCm0NLdBnRypWmafbH88mIljbO6HAAAAABAA9HS3QZ0zlmuZ3xe1hkl/7O6FAAAAABAIxC62wD/sBhJUrAzTy6XYXE1AAAAAICGInS3AQHhsZKkCOWroLTS4moAAAAAAA1F6G4DfILNlu4IWwGDqQEAAABAG0LobgsCIyVJwbYS5eTnW1wMAAAAAKChCN1tgW+onFWHqjA7w+JiAAAAAAANRehuC+x2FdpDJUmluYRuAAAAAGgrCN1tRLF3mCSprCDT2kIAAAAAAA1G6G4jynzCJUmuQkI3AAAAALQVhO42otIvQpJkFGVZXAkAAAAAoKEI3W2E4W+OYO4ozba4EgAAAABAQxG62whbYJQkybuM0A0AAAAAbQWhu43wCo5WvuGvkgrD6lIAAAAAAA3kZXUBaJjKkX/SkJ/7KMTmpWlWFwMAAAAAaBBautuI8EBfSVJ+aaUqnC6LqwEAAAAANAShu40IC/CRzWbezy2usLYYAAAAAECD0L28jXCU5+s938cV5CpQTtFxig72tbokAAAAAMARELrbCi9/Hau1kl1anpMlxYVaXREAAAAA4AgI3W2Fl4+eC7lDa7Kki8scVlcDAAAAAGgAruluQ9ZHna4FruHKLLVZXQoAAAAAoAEI3W1IRICPJCmnqNziSgAAAAAADUH38jakn7brXPvv8sqqlNTb6nIAAAAAAEdAS3cbMn7/F/qXzytKyPzZ6lIAAAAAAA1A6G5LAiIlSV5l2RYXAgAAAABoCEJ3G+IIipIk+ZblWFwJAAAAAKAhCN1tiHdItCTJvzLX2kIAAAAAAA1C6G5D/ENjJUnBzlxrCwEAAAAANAihuw0JjDBDd6gKVFLutLgaAAAAAMCRELrbkIAwM3RHKl85xczVDQAAAACejtDdhtiqRi/3t5UrJy/P4moAAAAAAEdC6G5LfINVLi9JUlF2msXFAAAAAACOxNLQPWvWLI0ePVrBwcGKiYnROeeco6SkpCO+7tNPP1W/fv3k5+enwYMH69tvv22Faj2AzaYCe5gkqSQ33dpaAAAAAABHZGnoXrRokWbMmKHff/9dc+fOVUVFhU477TQVFRXV+5rffvtNl1xyif70pz9p9erVOuecc3TOOedo/fr1rVi5dUq8QiVJ5fmZFlcCAAAAADgSLyvf/Pvvv6/zePbs2YqJidHKlSt1wgknHPI1zz33nE4//XTdcccdkqSHH35Yc+fO1QsvvKBXXnmlxWu2WqlPuFQuVRZmWV0KAAAAAOAIPOqa7ryqwcEiIiLq3WbJkiWaOHFinXWTJk3SkiVLDrl9WVmZ8vPz3UtBQUHzFWyBSr8I5RkBKiktsboUAAAAAMAReEzodrlcmjlzpo499lgNGjSo3u3S0tIUGxtbZ11sbKzS0g49sNisWbMUGhrqXgYMGNCsdbe2JcMe19Cy1zXPb5LVpQAAAAAAjsBjQveMGTO0fv16ffTRR82637vvvlt5eXnuZePGjc26/9YWEeQrScouYp5uAAAAAPB0ll7TXe3GG2/U119/rZ9//lldunQ57LZxcXFKT687cnd6erri4uIOub2vr698fX3dj/Pz84++YAtFBPpIknKKCd0AAAAA4Oksbek2DEM33nijvvjiC82fP1/du3c/4mvGjRunefPm1Vk3d+5cjRs3rqXK9CjxhRv1jvcs3ZD3jNWlAAAAAACOwNKW7hkzZuiDDz7Ql19+qeDgYPd12aGhofL395ckXXHFFercubNmzZolSbrlllt04okn6umnn9bkyZP10UcfacWKFXrttdcs+xytKdS7Uj0d67TdFS/DMGSz2awuCQAAAABQD0tbul9++WXl5eVpwoQJ6tSpk3v5+OOP3dskJycrNTXV/Xj8+PH64IMP9Nprr2no0KH67LPPNGfOnMMOvtaeBHYZrFvL/6p7K65SYVml1eUAAAAAAA7D0pZuwzCOuM3ChQsPWnfBBRfoggsuaIGKPJ9/WIy+c5ykkgqnsovKFeznbXVJAAAAAIB6eMzo5Wi46sHUGMEcAAAAADybR4xejsaZ4L1BZY49KszuISWGW10OAAAAAKAehO42aEbJq4r33qNF6cdI6m91OQAAAACAetC9vA0q8Q6TJFUUZFpbCAAAAADgsAjdbVC5r9ml3FmYZXElAAAAAIDDIXS3QU6/SEmSvZjQDQAAAACejNDdBhkBVaG7NMfiSgAAAAAAh0PoboMcQVGSJN+ybIsrAQAAAAAcDqG7DfIOjpEk+VfS0g0AAAAAnozQ3Qb5h5mhO8iZZ3ElAAAAAIDDIXS3QYHhsZKkUCNfTpdhcTUAAAAAgPoQutugoAgzdIerQHnF5RZXAwAAAACoD6G7DfIOipYk+doqlZPLYGoAAAAA4KkI3W2RT4BK5CtJKtyfZnExAAAAAID6eFldAJomzx6uUmehCgtyrS4FAAAAAFAPWrrbqH8mvKPhZa8p2aen1aUAAAAAAOpB6G6jwoL8JEnZRQykBgAAAACeitDdRkUG+kiScgjdAAAAAOCxCN1t1JiCuXrP+xEN2vO+1aUAAAAAAOpB6G6jol1ZOs6xQZGFW6wuBQAAAABQD0Yvb6MKu52qmesqZA/rq+OtLgYAAAAAcEiE7jbKN36g5rjylVDub3UpAAAAAIB60L28jQoPqB5IrcLiSgAAAAAA9aGlu42K8HXpZPsqhVYUqaxyony9HFaXBAAAAAA4AKG7jQpxOPWmz1OSpPT8uxUbEWpxRQAAAACAA9G9vI2y+4eqsurw5WenW1wNAAAAAOBQCN1tld2ufFuIJKkoh9ANAAAAAJ6I0N2GFTrMLuWleYRuAAAAAPBEhO42rMQrXJJUnp9pcSUAAAAAgEMhdLdhZb5m6HYWZllcCQAAAADgUAjdbZjTL0KSZCsmdAMAAACAJyJ0t2FGQKQkyVGSbXElAAAAAIBDIXS3YY6gKEmST3mOxZUAAAAAAA6F0N2GeQdHS5L8KwjdAAAAAOCJCN1tmF9ojCQpyJlncSUAAAAAgEMhdLdhAWGxkqQQV74Mw7C4GgAAAADAgQjdbVhIZJxchk2GpOKySqvLAQAAAAAcwMvqAtB0/hGd1d/5vkorpcXFFQr087a6JAAAAABALbR0t2E2u11hAX6SpJzicourAQAAAAAciNDdxkUE+kiSsosI3QAAAADgaQjdbdyfKz/QB97/J8fuxVaXAgAAAAA4AKG7jevp2qnxjo2y5ey0uhQAAAAAwAEI3W3cqrgLdUv5DdrkN8LqUgAAAAAAB2D08jYup9Nx+nJDrAKc0VaXAgAAAAA4AC3dbVz1QGo5DKQGAAAAAB6H0N3GxdrzNdG+Ugk5v1ldCgAAAADgAITuNi6xZLNe93laF+TOtroUAAAAAMABCN1tnF9YrCQp2JVrbSEAAAAAgIMQutu44AgzdIcZ+XK5DIurAQAAAADURuhu44IjzdDtbytXQUG+xdUAAAAAAGojdLdxvgFhKjfMmd/yslItrgYAAAAAUBuhu62z2ZRnD5EkFeWmW1wMAAAAAKA2Qnc7UGAPlSSV5GZYXAkAAAAAoDZCdztQ4h0uSarIJ3QDAAAAgCchdLcDZT5hkiRnUZa1hQAAAAAA6iB0twOVfpHmnaL91hYCAAAAAKiD0N0OGP4RkiRHCaEbAAAAADwJobsdsAdFS5K8y3MsrgQAAAAAUBuhux3wCo6W07DJVVludSkAAAAAgFq8rC4AR6+i95nqtTBSXYOCtNDqYgAAAAAAbrR0twMRwf4yZFd2ES3dAAAAAOBJCN3tQESgryQpv7RSFU6XxdUAAAAAAKrRvbwdCPXz0vPe/1aU8pWXNVxRsfFWlwQAAAAAEC3d7YLDYdfxjvUa59iowux9VpcDAAAAAKhCS3c78bLfn5Sa79RlRoS6WV0MAAAAAEASobvdWBl6ulbk5uh0p7/VpQAAAAAAqtC9vJ0ID/SRJEYwBwAAAAAPQkt3O9HbkSHZV8iR7pLU1epyAAAAAACyuKX7559/1pQpUxQfHy+bzaY5c+YcdvuFCxfKZrMdtKSlpbVOwR7s+KLv9R+ff6nH3jlWlwIAAAAAqGJp6C4qKtLQoUP14osvNup1SUlJSk1NdS8xMTEtVGEbEhAlSfIuzba4EAAAAABANUu7l59xxhk644wzGv26mJgYhYWFNX9BbZgjyAzdPhW51hYCAAAAAHBrkwOpDRs2TJ06ddKpp56qX3/99bDblpWVKT8/370UFBS0UpWtyyc0WpIUUJFjcSUAAAAAgGptKnR36tRJr7zyiv773//qv//9rxISEjRhwgStWrWq3tfMmjVLoaGh7mXAgAGtWHHr8QuNlSQFO/MsrgQAAAAAUK1NjV7et29f9e3b1/14/Pjx2r59u5555hm9++67h3zN3XffrVtvvdX9OCUlpV0G76AIM3SHGvmSYUg2m8UVAQAAAADaVEv3oYwZM0bbtm2r93lfX1+FhIS4l+Dg4FasrvWERMZJknxslSopzLW2GAAAAACApCaG7j179mjv3r3ux8uWLdPMmTP12muvNVthDbVmzRp16tSp1d/X0wQHhajY8JUk5WenW1wNAAAAAEBqYui+9NJLtWDBAklSWlqaTj31VC1btkz33HOPHnrooQbvp7CwUGvWrNGaNWskSTt37tSaNWuUnJwsyewafsUVV7i3f/bZZ/Xll19q27ZtWr9+vWbOnKn58+drxowZTfkY7YrNZlOuLUSSVJjNvOUAAAAA4AmaFLrXr1+vMWPGSJI++eQTDRo0SL/99pvef/99zZ49u8H7WbFihYYPH67hw4dLkm699VYNHz5c9913nyQpNTXVHcAlqby8XLfddpsGDx6sE088UWvXrtVPP/2kU045pSkfo90pdIRKkkpzaekGAAAAAE/QpIHUKioq5OtrdmX+6aefdPbZZ0uS+vXrp9TU1AbvZ8KECTIMo97nDwzwd955p+68887GF9xBFHuFSU6pLD/T6lIAAAAAAGpiS/fAgQP1yiuvaPHixZo7d65OP/10SdK+ffsUGRnZrAWi4cp8wiVJzkJCNwAAAAB4giaF7scff1yvvvqqJkyYoEsuuURDhw6VJH311VfubudofZV+Eao07KooLbK6FAAAAACAmti9fMKECcrKylJ+fr7Cw8Pd6//85z8rICCg2YpD46zoeZMu2zNF0yO7abzVxQAAAAAAmtbSXVJSorKyMnfg3r17t5599lklJSUpJiamWQtEw4UGB8mQXTlFFVaXAgAAAABQE0P31KlT9c4770iScnNzNXbsWD399NM655xz9PLLLzdrgWi48EAfSVJ2UbnFlQAAAAAApCaG7lWrVun444+XJH322WeKjY3V7t279c477+jf//53sxaIhot3peol72f1l8xHrC4FAAAAAKAmXtNdXFys4OBgSdKPP/6oc889V3a7Xcccc4x2797drAWi4UJ97RrtWKbCCq6rBwAAAABP0KSW7l69emnOnDnas2ePfvjhB5122mmSpIyMDIWEhDRrgWi44JhE3Vdxpe6pvFaGy2V1OQAAAADQ4TUpdN933326/fbb1a1bN40ZM0bjxo2TZLZ6Dx8+vFkLRMOFhYbrHeckfVl5jArKnVaXAwAAAAAdXpO6l59//vk67rjjlJqa6p6jW5JOOeUUTZs2rdmKQ+P4+zjk7+1QSYVTOUXlCvHztrokAAAAAOjQmhS6JSkuLk5xcXHau3evJKlLly4aM2ZMsxWGphnrv1e+zr0qyOghRfa3uhwAAAAA6NCa1L3c5XLpoYceUmhoqLp27aquXbsqLCxMDz/8sFxcS2ypW12z9arPM1Lyb1aXAgAAAAAdXpNauu+55x698cYbeuyxx3TsscdKkn755Rc98MADKi0t1SOPMGWVVUq9w6RKqaIgy+pSAAAAAKDDa1Lofvvtt/X666/r7LPPdq8bMmSIOnfurBtuuIHQbaFy3wipRHIVEroBAAAAwGpN6l6enZ2tfv36HbS+X79+ys7OPuqi0HRO/0hJkq1kv8WVAAAAAACaFLqHDh2qF1544aD1L7zwgoYMGXLUReEo+EdIkrwI3QAAAABguSZ1L3/iiSc0efJk/fTTT+45upcsWaI9e/bo22+/bdYC0Tj24GhJkk95jsWVAAAAAACa1NJ94oknasuWLZo2bZpyc3OVm5urc889Vxs2bNC7777b3DWiEXyqQndARa61hQAAAAAAmj5Pd3x8/EEDpq1du1ZvvPGGXnvttaMuDE3jHxYrSQpy5VpbCAAAAACgaS3d8FyBEWboDjEKJOZMBwAAAABLEbrbmZBwM3R7ySVnSa61xQAAAABAB0fobmfCQoKUb/hLkgqy0yyuBgAAAAA6tkZd033uuece9vnc3NyjqQXNwNthV6otRCEqUWF2qsISBlhdEgAAAAB0WI0K3aGhoUd8/oorrjiqgnD0Cu2hqnBmqSg/1+pSAAAAAKBDa1Tofuutt1qqDjSjByMf09I9JXolfJT6Wl0MAAAAAHRgXNPdDgUHhUiyKae43OpSAAAAAKBDI3S3Q+EBPpKk7CJCNwAAAABYidDdDo2sXK2XvZ/RgG2vWV0KAAAAAHRojbqmG21DrC1HJzmWa1OuzepSAAAAAKBDI3S3QyWxo/TPdVcrJLCf+ltdDAAAAAB0YITudsg7to/ec56qoa7DT/EGAAAAAGhZXNPdDkUEekuSshm9HAAAAAAsRUt3OxTu760xtk2KLyqWKo+TvHysLgkAAAAAOiRCdzsUGeird31myddWqbLcK+Qb1c3qkgAAAACgQ6J7eTsU7O+tHAVLkgpz0i2uBgAAAAA6LkJ3O2S325RnMwdRK8pOs7gaAAAAAOi4CN3tVJHDDN2leZkWVwIAAAAAHRehu50q8Q6TJFUUZFhbCAAAAAB0YITudqrMN0KS5CrMsrgSAAAAAOi4CN3tlNPPDN0q3m9tIQAAAADQgRG626uASEmSV2m2xYUAAAAAQMdF6G6n7EFRkiSfckI3AAAAAFiF0N1O+YRES5L8K3KtLQQAAAAAOjBCdzvlGxorSQpy5llcCQAAAAB0XITudiooLMa8NQolZ6XF1QAAAABAx0TobqeCI8yW7krDLqM019piAAAAAKCD8rK6ALSMiOAADSp9XYXy1wavMAVaXRAAAAAAdEC0dLdTAT4OVXgFSbIpu6jc6nIAAAAAoEMidLdTNptNEYE+kqScYkI3AAAAAFiB7uXt2HTHT+rtvUz2pFypy3SrywEAAACADoeW7nZsgG2nJjlWyJ65wepSAAAAAKBDoqW7HdsQfqrm58ZpZMhxGmB1MQAAAADQARG627Gs6LF6b1ucQh09rC4FAAAAADokupe3Y+FVA6llF1VYXAkAAAAAdEyE7nYs1rdCY22bFJf5m9WlAAAAAECHRPfydqyLa58+9n1Y2emRkv5idTkAAAAA0OHQ0t2O+YfFSpKCXHmSYVhcDQAAAAB0PITudiwo3AzdPqqUygosrgYAAAAAOh5CdzsWHhaqYsNXkuQqzLK4GgAAAADoeAjd7VhYgLf2GyGSpNJN31lcDQAAAAB0PITudszXy6H/2U6UJPnPu0da9a7FFQEAAABAx0Lobuc+Dpyu2ZWnySZD+uomgjcAAAAAtCJCdzs3dVhnPVB5pT61nyERvAEAAACgVRG627kbTuqlrpGBuqP4Mi2NPl8EbwAAAABoPYTuds7P26H/O2eQJJsu3jtNWQOulDt4r37f6vIAAAAAoF0jdHcAx/eO1jnD4mUYNl2Zer5co6+TfIKkqD5WlwYAAAAA7Rqhu4P451kDFOrvrQ2pBXoz+Hrpr4ulhNFWlwUAAAAA7Rqhu4OICvLV3Wf0kyT966etSrHH1Ty5b4205kNrCgMAAACAdszS0P3zzz9rypQpio+Pl81m05w5c474moULF2rEiBHy9fVVr169NHv27Bavs724cFSCRncLV3G5U/d/uV6GYUi5ydI7U6U510ubv7W6RAAAAABoVywN3UVFRRo6dKhefPHFBm2/c+dOTZ48WSeddJLWrFmjmTNn6tprr9UPP/zQwpW2D3a7TY9OGyxvh00/bcrQDxvSpdAEaciFUpfRUrfjrC4RAAAAANoVLyvf/IwzztAZZ5zR4O1feeUVde/eXU8//bQkqX///vrll1/0zDPPaNKkSYd8TVlZmcrKytyPCwoKjq7oNq53bLD+ckJPvbBgmx74aoOO7RWp4DOekCpKJJ8Aq8sDAAAAgHalTV3TvWTJEk2cOLHOukmTJmnJkiX1vmbWrFkKDQ11LwMGDGjpMj3ejSf3UtfIAKXll+rpH7dINlvdwP3LM9Lq96wrEAAAAADaiTYVutPS0hQbG1tnXWxsrPLz81VSUnLI19x9993Ky8tzLxs3bmyNUj1azdzd0ttLdmntntyaJ7fPl356QPryRoI3AAAAABylNhW6m8LX11chISHuJTg42OqSPELN3N3SP75Yp0qny3yix0nS6OskGQRvAAAAADhKbSp0x8XFKT09vc669PR0hYSEyN/f36Kq2i733N378vX2kt3mSptNOvNJgjcAAAAANIM2FbrHjRunefPm1Vk3d+5cjRs3zqKK2rbac3c//WOS9uVWddEneAMAAABAs7A0dBcWFmrNmjVas2aNJHNKsDVr1ig5OVmSeT32FVdc4d7+r3/9q3bs2KE777xTmzdv1ksvvaRPPvlEf/vb36wov124cFSCRnWtmrv7qw01TxC8AQAAAOCoWRq6V6xYoeHDh2v48OGSpFtvvVXDhw/XfffdJ0lKTU11B3BJ6t69u7755hvNnTtXQ4cO1dNPP63XX3+93unCcGR2u02PnjtYXnab5m5M1w8b0mqePCh4z5DeO0/a+JXkrLCsZgAAAABoK2yGYRhWF9Ga9u7dq4SEBO3Zs0ddunSxuhyP8eQPm/Xigu2KC/HTT7edqCDfWlO4G4b0wz3S7y/WrAuMloZdKp3ygGRvU1cpAAAAAMBRa2i2JC1BknTTyb2VGFE9d3dS3SdtNun0R6WbVknH/U0KjJGKMqWUVXUDt7OydYsGAAAAAA9H6IakA+bu/m2X/tibe/BGkT2liQ9It26ULnpfOvHOmucKM6Sn+0rf3kHXcwAAAACoQuiG2wl9ojV1WLxcB87dfSCHt9T/LKn7CTXrNsyRirPM1m+Hd836yvIWrRkAAAAAPBmhG3X8c/IAhfh5aX1Krbm7G2L0tdJln0un3FezriTHbP3+coa0Z5l5bTgAAAAAdCCEbtQRHeyru8/sL+mAubuPxG6Xep0i9TixZt3mb6SSbHOqsTdOlV4aJ/3+slSc3QKVAwAAAIDnIXTjIBfVN3d3Yw2bLl39vTT0UsnLX8rcJH3/d7P1++2zpV+ekfatllz1dGMHAAAAgDaOKcNwSFvSC3Tmc4tV6TL06uUjNWlg3NHtsCRXWv+ZtPJtKe2Pus/5R5jXh/c8Sep5shSWeHTvBQAAAAAtjCnDcFT6xAbrLyf2kCTd/+UGFZYd5XRg/mHmdd9/XSzNWC6d8aTU90zJJ9jsgr5xjvS/W6SFj9e8xuWiKzoAAACANo3QjXoddu7uoxHdRxr7Z+mSD6W7dkrX/ChNuFtKHCf1nlizXdpa6cme0jtTm++9AQAAAKAVEbpRrwPn7l63N6/538ThLSWOlSb8Xbrme2ngtJrn9q2WDJfkHVD3NV9cL/36byltHdeDAwAAAPBoXlYXAM9WPXf3l2v26e+f/6E7T+8nPy+7/LwdVUvVfS+HfL3t8vWyy2azNc+bj7pG6nO6VFZYsy57p7T2A/P+XEkBkVLXY81rwrsdL0X3lZrr/QEAAADgKDGQGo4os6BMpzy9UPmlR76u22aTfL1qgnh1KPf1drjDepCvl0Z0DdeEvtHqERXYuJBetF/64yNpx0Jp169SRVHd5wOjpW7HmQG8+wlSZC9COAAAAIBm19BsSehGg8zdmK5XF21XUblTZRVOlVY4VVrpMm8rnHI18SxKiPDXSX1jNKFvtMb1iJK/j6PhL3ZWSCmrpF0/SzsXS3uWSpWldbcJijND+NQXJW+/phUJAAAAAAcgdNeD0N38DMNQhdNQaaUZwMsqzDBe5g7lVbeV5v3MgjL9si1Ty3Zmq8JZc/r5eNk1tnuEJvSN0Ul9o9W9sa3glWVSykozgO9aLO1ZJjnLzCnIZq6r2e63FyS/EKnvZCkwshl/EgAAAAA6CkJ3PQjdnqOorFK/bd+vhUkZWpiUqZTckjrPJ0YEaELf6Ka1gktSRam0d7lUmif1P8tc53JKj3eXyvKk6+ZLnUea63cslHJ2Sf7h5rzh/uFSQNWtt/9Rf1YAAAAA7UtDsyUDqcEygb5eOnVArE4dECvDMLQ9s1ALNmdq4ZYMLduZreTsYr2zZLfeWbLb3Qpe3RW9Qa3g3n5S9+PrrqsslcZcJ6WskOKG1qxf86F5rfihePnXDeHVS5dR0ograrbbs8xsVQ+Oa9oPBAAAAEC7Q0s3PFJDW8EvO6ar+sQGH/0bLnnR7JZeki2V5EjFVbeGs/7XDDhHuvBt877LJT0caU5xdv0SKXaAud4wGMgNAAAAaIdo6UabdmAr+LaMQi1MOrgV/IOlyfrLiT1008m95efdyO7ntY2bYS61GYZUll83hFcvxdlSVO+abSuKpLCuUlmBFN2vZv1XN0kZm6QeE8wlYYzk5dv0OgEAAAC0KbR0o82pbgX/eHmyftqUIUnqFhmgR6YN1rG9oqwtrqKk5hpww5CeGSjlp9Q87+UvdR1nBvDuJ0pxQyS73ZJSAQAAADQdA6nVg9Ddvny/Pk0PfLVBafnmVGHnjuisf04eoIhAH4srq5K7R9q5yByobcciqSij7vP+EeZ84j1ONIN4eHe6owMAAABtAKG7HoTu9qegtEJP/ZCkd37fLcOQwgO8dc/kATpvROfGTTnW0gzD7Gq+Y6EZxHf9IpUX1t0mLFE69z9S4jHm45zd5nXmIZ2loJhWLxkAAADAoRG660Hobr9WJ+fo7s/XaXNagSRpfM9IPTJtsLpHBVpcWT2cFea84juqWsL3LpdcFdLM9VJYgrnNj/+UfnteGnejNOkRc11eivTGaZJf6CGWkLqP/SPMfYV0lhzeln1UAAAAoL1hIDV0OMMTw/W/m47TG7/s1LM/bdFv2/dr0rM/66aTeukvJ/aUj5eHXTvt8DZbtBOPkSbcJZUVmiG8OnBLkneAFBxft5W7JEfK32suDWWzm/OSxw83H+9dIWVuNh/HDmyezwMAAADgILR0o11K3l+se+as0+KtWZKk3jFBmnXuYI3qFmFxZc2gvNgMzKV5R16Ks8zryp1l0u1ba8L7D/dIS16o24JemCl9eqXZxf3AhZZyAAAAoA5autGhJUYG6J1rxuirtfv00P82amtGoc5/ZYkuHZuou07vp1D/NhwgfQKkziMavr3LJRVlSoHRNesie0o9T5E6DatZl7NL2v2ruRzI5pDCu0nRfc0lqq8U3UeK6iP5NsM86QAAAEA7RUs32r3c4nI9+u0mfbLC7I4dHeyrB6YM1JmD4zxroDWrFe2XdiyQcpMPXpxl9b/u1k1SSLx5P2WVVFkqxQyQ/MNapWwAAADACrR0A1XCAnz0xPlDde6ILvrHF+u0I7NIMz5YpZP7xeihqQPVJTzA6hI9Q2CkNPj8g9e7XFJhmpS1RcpMMpfq+xXFUnCnmm1/eUba9JU0aZY07gZzXW6ytOnrmlbykM5MiwYAAIAOg9CNDuOYHpH67pbj9dKC7Xp54XbN35yhJdv367bT+uiq8d3k5fCwgdY8hd1utmSHxJtziddWVlA3QAdGm9eAx/SrWbdnmfTD3TWPvQOlqF5m1/SoPlJk1f3InpK3f4t+FAAAAKC10b0cHdK2jAL94/P1WrYrW5KUGBGgC0d10fkjExQX6mdxde3MjoXS8telzC1S9nbJVVnPhjYzsEf1NkdUP/Wh1qwSAAAAaBTm6a4HoRvVXC5Dn6zYo1nfbVZeSYUkyW6TJvSN0YWjEnRK/xh50/rdvJwV5oBtWVukrK1VyxZzKc2t2S6qr3TjsprH70w1p1Q788maQeRKciRnpRQQabbGWygjv1SlFS4lRnKpAgAAQEfBNd3AEdjtNl08JlFnD4vXt+vS9MnyPVq2K1vzN2do/uYMRQX56LwRXXTh6AT1jA6yutz2weFttmRH9a673jCkoixpf1UIt3vVfW7vSqm8wJy3vNqy16UF/yfZvc3rykM6mbfu+/F11/m0TCDelVWkc176VSXlTn1z8/HqFcO5AgAAgBq0dAO1bM8s1Ccr9ui/K1OUVVgzYveoruG6aHSCJg/ppAAfvqtqVYYhZWw0W8X7nil5+Zjrf/yn9NsLkhr4T5hfqNTteOni92vWrfvM3H/f02umPsvcIuXulmx2ye4wp0uze9W673DfL6yUbvhgjbbsr1SaInV87yi9c80YRsUHAADoAOheXg9CNxqiwunSgs0Z+mTFHs3fnCFX1W9JkK+XpgztpItGJ2pol1DCldWcFVJBmlSQai75qVLBvqrbWusqiszte02ULvtvzetnJUpledLNq6WIHua6ufdLvz7bqDI2q7vOrpilcqdLr10+Uqelv2E+MeJy8zp1AAAAtDt0LweOgrfDrtMGxum0gXFKzy/VZyv36pMVe7R7f7E+XLZHHy7bo76xwbpwdIKmDe+siEAfq0s+yP7CMqXklmhw53b85YDDWwpLMJf6GIZUlm+G79qclVK346TyQnNE9Woh8VKnYZLLKRnOWreV5vRpVfeLSstVXlEhLzmV0Cla13brrpcWbtfD32zUqcbrspVkSwPOrtnv8telVe+a4b56iexp3gZGM40aAABAO0VLN9BAhmFo6c5sfbx8j75dl6qySpckycdh16kDYnXR6AQd1ytKdru14WlbRqHe+GWH/rsqReWVLl1zbHf9c3J/y+tqT/67cq9u+3StJOn5S4ZrytB4FZVV6pSnFyk9v1jv9l+u4yLypNMekXyrrvH++m/SijcPvUOfICmiuxTRUwrvKgXGmEE8ooeUMLqVPhUAAAAag+7l9SB0oznklVToq7X79PHyZK1PyXev7xzmr/NGdNb5IxNadSTr6i8E/vPzDs3bnHHQ8+cO76zHzx/CaOzNYFVyji5+9XeVO1266eReuu20vu7nvlyTols+WiM/b7vm3zZB8WG15h3P2SWlbzSnTcveUbPk7lG916Uf2B3+mcGSl690xRwptOrfr52LpawkM6S7lyjJL4zWcwAAgBZE93KgBYX6e+vyY7rq8mO6asO+PH2yfI++WJ2ilNwS/Xv+Nv17/jaN7R6hC0Yl6MzBcS02+FqF06Vv16Xq9cU7tS4lT5KZsyb2j9V1x/dQSm6xbv/0D32+OkX5pRV64dIR8vN2tEgtHUFqXon+/M5KlTtdmjQwVn+b2KfO82cPjdf7vydr2a5sPfLtJr146YiaJ8O7mcuBKsuknN21QniyVJwlFWVK8cNrtqsolfKSzfs+tUZIX/9faeVbB+/X7iX5hx96iR0oDb+s1gdba+4zLNHssg8AAIBmQ0s30ExKK5z6cWO6Pl2xR79sy1L1b1agj0NnDYnXBaO6aGTX8Ga5vrqgtEIfL9+jt37dpZTcEkmSr5dd54/soj8d1109ak1xNm9Tum54f5XKKl0a0y1Cr181SiF+BKvGKil36oJXf9P6lHz1iwvWf68fr0Dfg79M2bAvT1Oe/0UuQ/rwumM0rmdk8xTgckr7t0mFGea16NXn0fI3pO3zzZBelGlOvVaWf/h99Z4kTf+k5vEjnaSKYunmNWY3d0n67Xkz0PuFST6Bkpef5O1fs3j5H/w4OE7qfnzNfrO2SQ4vc/q26lHnDYMWeAAA0C7QvbwehG60hn25Jfp81V59unKvdu8vdq/vERWo80Z20Xkjuigu1K9J+5392y59uDRZBWWVkqTIQB9dMa6bLjsmUZFBvod83bKd2frT7OUqKKvUgE4hevuaMYoOPvS2OJhhGLrpw9X6+o9URQT66MsZxyohov7LB/45Z53e+z1Z/eKC9fVNx8mrtbv1V5SareUluVJJTs1SWvU4oqc5srpkjgD/7xHm+r+tM1vCJel/Mw/dgn44ieOla76refxkb6koQ/rrr1LcIHPdglnSz0+Y86s7fMxQ7vCpelz7fvXiI4V3l855sWa/Pz8llRVII64wB6OTpOJs8/MFREq+IQR7AADQ4uheDlgoPsxfN57cWzNO6qVlO7P16cq9+nZdqnZkFenJH5L09I9JOqFPtC4YmaCJA2Lk63X4Lt/rU/L0+uId+vqPVFVWzV/WMzpQ1x3fQ+cM73zELuNjukfoo78coyvfXK6Nqfm64JXf9O6fxh42OKLGC/O36es/UuXtsOmVy0Ye8ed226l99fUfqdqcVqD3lybryvHdWqfQat5+5jXfoQ34YtHhbYbtAx1zg9TndDPIVhRLFSU1S2Vp1bpSqbLW+tiBB9ThL3kHmLfVXBWS4ZKcZebSECW5dR+v+cC8Nr7PpJrQvf6/0re3m/ftXmb4DoiSAiKq7lctgVFV96vWdxpas9+srWZ3//CuNfO2l+aZvQds9voXLx+CfkdUXmT+DviHS3bGywAA1I+WbqCVFJVV6pt1qfpsxV4t25XtXh8W4K2pQ+N1wagEDeoc6l7vchlatCVT/1m8Q79t3+9eP65HpK47obsm9Ilp9Ijku7KKdNkbS7U3p0SxIb5655qx6hsXfPQfrh37fn2a/vreSknSY+cO1sVjGjbv9ru/79a9c9YrxM9LC26fUG8vhA6nrNCcps1ZbrayOyvMIO4sN6dxc5ZXPa5eys3rzfucVrOPJS9JeXulcTOk0M5V616U5j9SMyd7Q0T0lG5eVfP45WOl9PXS5V9IPU821614S/p65pH3ZfeuCfbBseY+qm350ezynzC2Zno7utl7vp2Lpf1bpe4n1ny5s+VH6bs7zUs5ygvNdQ4fc6rBkM61bqvuh1bdZ1pAAGiXaOkGPEygr5cuHJWgC0claFdWkT5buVefrdyrtPxSvb1kt95eslv9O4XogpFdFODj0Bu/7NTWDPOPOofdprOGdNJ1x/eoE8wbq1tUoP57/Xhd8cYyJaUX6MJXl+jNq0ZrZNfw5vqY7cqm1Hzd+skaSdJV47s1OHBL0qVjEvXB0mRtSs3XUz9u0axzB7dQlW2Mb1DNNGpNNe6GQ6ybYS4VJVLx/lpLds39oqy6zwXF1N1HQIQUFGtev17N7mW2Yhuu+hfJ/KKgMM1cirPq7vfXZ6Xdv0rnv1kTujf9T/rir1Wt71Vh3S9UsjmqwpnNvLXZq+5LcvhKZ/2rZr/LX5cyNktDLqqZWi4zSVr9rhQUJ4V0qgl/QXE119V3JIZhtki7xzyoWgozpcJ08/KHwqrFWS79bX3Na395Rto+T5r6Uk3ottmlnJ1138NZbs5OkLOrniJs0j8zan7+y183B0wcOK1msERnhXnLQIYA0C4RugELdIsK1O2T+upvp/bRL9uy9OmKPfpxY7o2pebroa83urcL8vXSJWMSdNWx3dW59vRTRyE2xE8f/+UYXTN7uVYl5+qy15fqlctH6sQ+0c2y//Yiq7BM1769QsXlTh3XK0r/nNy/Ua932G168OyBuvDVJfpoebIuHZOowV2a/oUJGsjbv+Fd6w905f8OXjfi8prr3+tTUVIr3GfVBKhq8cPNsBbWtWZd8X6zVT6vqGZU+iPx8q8burf8IG390dx/dejO2WUOgncQm/klQ3CtIF57SRxvXlPfFrhcdbtzb5tnftnQ6xQpumoKv61zpW9uNcN1ZUnD911Ral6eIUmJx5it2EGxNc93GSld/b35swyKMb8IKUyT8veZvS/y90n5KVXLPikvxfzypPYXHhvmSLsWS7GDa0L31h+ljy41v3gJiJT8a18WccAlEnWWiPbdgm4Y5uUsZYXmcXRVmoNKuiqreslUPY4bXHPcMreYl5+EdZViB5jrygrMn7ur0vxd9A83f3a1Z3Xwbp7/Y1uNYZif3XBW3brMz2BnhhLAE9G9HPAQecUV+mptij5blaLiskpdNDpBF41OUHALjTReXF6p699bpUVbMuXtsOlfFw7TlKHxLfJebU15pUuXvb5Uy3Zlq1tkgObMOFZhAU1rJbzlo9X6cs0+jUgM03+vH98so9ejHSgvNsNaUa2W99I8SUZVC3rVrYya+3Yv6biZNftY+7E5on3/s2quTc/cIq16WypIkwpSa8Kfs/wwxdikezNrWlnnPSSl/iEd81dzrnjJ3N+eZbVCSlhVUAk4+tDncpljBxRl1UyXV5R16MdFmWavgr/X+qLivfOlbXOls1+o+YJk2zzpvXNrtvHyl4Kq57GPMa/tD4qtCc+BMebjiB7Nf322y1k3CK16V0rfII28Uoqp+jJv1TvSVzc1br82h3RvVk298//P3O8x10vdTzDX7d8ubfpK8g40ZyHwCTj8/Yb2hnC5zOPgDr9VQdhVKwhXh+KKEvPyivJCqd9ZNT+L9f+Vdi+R+p5ec56lb5A+vdrsnVBeYIZtw3nkem5ebR47SZp7v9m7ZNyN0qRHzHW5e6RnBx1+H15+B0+veMp9NV/kZCZJGZukyF41A0NWlkvp62qNeVFcz+0B606+T4qumnJy7cfm4JK9TpXOeKzq5+uUnu5bK1S7aoXrqlsd4s/3896QBp9v3t/6k/TdHeZlLdNeqdnm67+Z/x5Uj7fhHWje+gTUWhcgefmavzdevuZ0kgERNbW5Ks0vpPj/DKB7OdDWhAZ46/Jx3XT5uG6t8n4BPl76zxWjdNuna/W/tft080erlVtSocuP6XrkF7djhmHovi/Xa9mubAX7eun1K0c3OXBL0t1n9NfcjelalZyrL1an6NwRfNkHmX/gRvSoCQpNMfSig9dF96kJGtUMwwz11QG8zpJi/gFeu1tz8lJp9y/S0Itr1u1dIX1yiBZ/u/fBQdwvrGbd8bfW7Hv1+1LaH+Z+q1t4N35phqyGBKvaKkpqWia7HWcOfFd9fb8kdR4p/WmuGa4DY8xgaVVAOLDl8VA9J4ZNl/pOPuDSiAMukyjJrrvO4Vv3C4Lk380W9EHn1axL3yD99EAjavU2z83bt9UE8C/+Km362jyvRl5prtu1WHrn7Ibvt9pdu2pmSNj5s7RytvlFSHXolk3KSjr0ax2+5rlk96q7OLzM11UL7WIe/5BaXyL7BJpTJdq9zHOt9qwOJTlmiKwsNb+oKkited2Jd9Xc3/y1+YXUsMtqZlMozZX+c3Ljfw5j/lwTusvyzS/Pag9EabObXzA1Vu3W+uL9UvaOuj1sJGndf6WyvMbtd/K/pNF/Mu/vWiy9M1WKGSDdsKRmm3enSfmpZkj3rgrr/uEHjHHQpepyl1ha5NHhELqBDszHy65nLxqmMH9v98BfuUXluvHkXh22RXb2b7v00fI9stukf186XL1iju7647hQP914ci898X2SZn23WacNjFPQIeb3bk4rd2fr23Vpuua45rssAW2YzVYVPqPqjtZen5PvMUNAl9E163wCzRYzd1DJrWrVrDCviy7KOMT7OqQT76x5vPFLaesPZrioDt2+wTWB2zfUvL49MNoceb665oMex9S97r526381/zApYcyRP6unsDuqPntkw1/jrKz7+PjbzMDdeUTNupDO0tBLzUsZyovNVtbywlr3i8zFVXVJhKvCbF2u/SVMRYnZ6lxZa7YB+yH+DbPZa4Vhb/Mz2b3MAOYbbA6IWLvm3qeZx7br+Jp14d2kK74yx33wCa66rVoa2gNhzHXmUltAhDT9k0Nvbxjmz6Q4++AwHlZrHI+gWClxXE3Lt2S2CIcmVLUO+9dtKa53XYA5iGO1fmeZvxMBUbV+ljZzqkW7w/w9sjuqfr61Hx+43m5+MVGt10Tpmh8P7jZ/8j/N41lRUnMeuJeqdZUl5vGuqLr1q3VpVPV54HXA4KBZ2xp+qYzNYV7qcuId0sirzHXF2eYXMWGJdc9hoJ2gezkAGYahZ37aqn/P2ypJuubY7vrn5P6NHh29rVu8NVNXvrlMLkO658z+uu6Eo2iFrKWs0qlJz/ysXfuL9ZcTeujuMxt3fXhjfLw8Wfd8sV6VLkPRwb5688rRXEuO5lc9QFn13O/VQbz2fPCV5dLpj9a8ZvV7ZpjvN8W8Nloy/6gvyTGvTz7wj3i0HmdF1RRoVcErslYozN9nrguMqglfzgpzW3vtlmemTesQXE7zSwqXs6bLuSSlrDKvna8srZlasjj74DEOClJrvmib8lxN6N75s/T2FCmyt3TTipr9zrnBPDeD46ouB4k1B4YMijHXBUTSag5L0b0cQIPZbDbdemofhQd468H/bdSbv+5UbnG5Hj9/iLwdHeMPqR2ZhZrx/iq5DOn8kV107fHdm23fvl4O3TdlgK6ZvUJv/rpTF45OUM/ooxzB+wBOl6HHvtuk/yw2R1YO8fNSZkGZLnx1iZ6/ZLgmDog9wh6ARrDZakaib+igdcMvO3hddWsgrOXwrro8IOzg52p30669vYMv8zoku6Nuy3e1hrZOu5zmbAH5KWYvgWo2R9W0igd0h9/yvdlVvj42u9ljIii2Zhl6sdT9ePP58iJzTIqg2KOfOQM4CrR0A6jji9V7dfunf8jpMjSxf4xeuHSE/Lzb97fIeSUVmvbSr9qRWaQRiWH68M/HyNer+T/zNbOXa/7mDJ3YJ1qzrx7dbF34C8sqdcuHqzVvs9nFd+bE3rrmuO668YPV+nlLpuw26b6zBuiqY5vviwQAAFrchjlm63hhhjnNX2G6VFB1W5SpQw4od/bz0ogrzPvbF0jvniNF95dm/F6zzSvHmbMb2L1qLoVwL46Dxw2we5mt8gOmmq/P3SP99m+zm/zxt9bsd/cSc5wM3+CaxSfI2nEl0KJo6QbQJNOGd1Gov7euf2+VftqUoSveWKbXrxqlkBYaRd1qTpehmz9crR2ZReoU6qdXLh/ZIoFbku49a4B+2ZqlRVsyNW9TRrO0Pu/NKda1b6/Q5rQC+XrZ9dQFQ92j0L9x5Sjd9+V6fbhsjx7430YlZ5fonsn95ehglw0AANqogefU/5yz0pzhoHYQL0yXOo+q2aa80LyOPiim7msLM80ZJBqj16k19/P2SsteM6/Prx26v7tDSlt38GttdjN8V4dw36oxC+xe0pCLpCEX1uz32zslv5C6o84vfNwcZNBml2QzA7z7vr1q1ouqqeOqR7nvNbHmy4fibOm/10oypMu/qNnvTw+aXfurR8W32ep+4VBnrIaqLyMSxkrH3lyzj69uMnsqnPqQWbckbfjCHJTzwFkNXJWHmPWgaokbUveSpNdPlSY/1bCxSNoAQjeAg5zcL1bv/mms/jR7uZbtytZFr/6uG0/qpVHdwhUb4nfkHbQhs77dpEVbMuXnbdd/rhilmOCW+3zdowL1p+O76+WF2/XQ1xt1XO+oo+pFsHJ3jv7y7gplFZYrOthX/7lilIYlhLmf93bY9ei0wUqMCNTj32/Wm7/u1J6cYj138TAF+PDPPwCgDXN4mdd1B8dJnerZpv8U6Z5Uc4yJ2q78n+QsOzgMuirNMO86cHFK8cNqXh8cJx1/uxmea4voYQbKsqop78ry5Z4Ksiy/6vEBag9aWVYgJX1Td2A9Sdq5SNr9awN/MFUCo2vuu5zS9nnmfcOoaXXP3i6lrDj4tYdTe6BFl8uc8lCSTr63Vr0/SyvebNx+D7R/m/kzbCfoXg6gXhv25enKN5crq7Bm1NrOYf4a1S1cI7uGa0RiuPrFBcurjV73/emKPbrjsz8kSS9eOkKTh9T3v3bzKSqr1MlPL1R6fpnumNRXM07q1aT9zFmdojv/+4fKK10a0ClEr185SvGHGan8f2v36bZP16q80qUhXUL1+pUt+wUDAAAdnmGYgw6WFZqBurygJpBXD0gXO1DqNMTcvjjbnOnBy08adknNfjZ+aQ5GZ1QFeBk19w1X3dHtq+/HDJC6HWu+vqLUbH22O6TBF9SE7pSVZi+B6tfKOMQXEVX3jarbsK5Sr1PM17uc0i/PmDWMv6lmjI7N35j7rtNK7n1wq7l7GkCHOTNF9bX4krT7Nymmf800gx6qodmS0A3gsPZkF+v1xTu0fFeONqfly3XAvxiBPg4NSwzTyK4RGtk1XMMTwzy+K3pRWaX+NXeL3vp1p1yGdPMpvXXrqX1a7f2/XJOiWz5aI39vh+bdduJhw/KBXC5Dz/y0Rc/P3yZJOm1ArJ65aJgCGzAN2Ypd2brunRXKKa5Q5zB/zb56tHrHBh/xdQAAADgYobsehG6g6QrLKrUmOVcrd+doxe5srUnOVUFZ3blibTapb2ywRnQN18jEcI3qFq7EiACPmfd77sZ03f/leu3LK5UkXTw6QY9OG9yq06MZhqELX12i5btyNGVovJ6/ZHiDXldS7tRtn67Rt+vM69D+emJP3Tmpb6Nq35VVpKtnL9fOrCIF+3np1ctGanyvqCO/EAAAAHUQuutB6Aaaj9NlaGtGgVbsytGq3TlasTtHydnFB20XFeSjEVUB/Kwh8Y1q2W0uqXkleuCrDfphQ7okKSHCXw9PHaQJfWOO8MqWsWFfnqY8/4tchvTRn4/RMT0iD7t9en6prntnhf7Ymydvh02PThusC0YlHPY19ckpKtef312h5bty5GW36bHzhuj8kfx7CAAA0BiE7noQuoGWlVFQqlW7c7Vyd7ZW7s7R+pR8lTtd7ue97DadM7yz/npiD/WKafmuzU6XoXeW7NJTPySpqNwpL7tN153QQzef3Fv+PtZOhXbPF+v0/tJk9YsL1tc3HVfvtfHrU/J07dsrlJZfqvAAb716+SiN6R5xVO9dWuHUHZ/9of+t3SdJuvnkXvrbqX08pkcCAACApyN014PQDbSu0gqn1qfkaeXuHM3bnKFlO7Pdz502IFZ/ndBTIxJbZpCM9Sl5+scX6/TH3jxJ0vDEMM06d7D6xYW0yPs1Vk5RuSY8tVB5JRV6aOpAXTGu20HbfL8+VX/7eK1KKpzqHROkN64crcTIgGZ5f5fL0NNzk/Tigu2SpGnDO+ux8wa32JRpAAAA7Qmhux6EbsBaq5Nz9Mqi7e5u3pI0tnuErp/QUyf2iW6Wltaisko9M3eL3qwaKC3Yz0t3nd5Pl45JbNVrtxvi3SW7dO+XGxTq760Ft09QRKCPJPO675cWbteTPyRJkk7sE63nLx3eIoPUfbw8Wf/4Yr2cLkNjukfotctHKizAp9nfBwAAoD0hdNeD0A14hm0ZBXp10Q59sTpFlVVDovfvFKLrJ/TUmYPimjwN2U8b03VfrYHSzhrSSfedNUAxHjq/uNNl6Kznf9Gm1HxdOjZRj04brLJKp+7+fJ0+X5UiSbpqfDf9c3L/Fp2abfHWTN3w3ioVlFWqR3SgZl81ptla1AEAANojQnc9CN2AZ9mXW6I3ftmpD5clq7jcKUlKjAjQn0/oofNHdpGfd8O6OqflleqBrzbo+w3myN5dwv318DmDdJJFA6U1xtId+3XRa7/LZpPevnqM/j1vq1bszpHDbtMDZw/U5cd0bZU6Nqfl65q3lmtfXqkiA330nytHtVjXfwAAgLaO0F0PQjfgmXKKyvXOkt2a/dtO5RRXSJKignx1zXHddNkxXevtVu10GXp3yS499eMWFZZVymG36drju2vmKX0sHyitMW7+cLW+qhrUTDK7xL80fYSO7x3dqnWk55fqT28v1/qUfPl62fXsRcN0xuBOrVoDAABAW0DorgehG/BsxeWV+nj5Hr2+eKdSckskScG+Xpp+TFddc2y3Ot3EDxwobViCOVBa/06eMVBaY6TmlejkpxappMKprpEBeuPK0eoVE2RJLUVllbr5w9WatzlDNpt08ehE3TChpxIi6G4OAABQjdBdD0I30DZUOF3639p9ennhdm3NKJQk+TjsOm9kF11+TFd9vmpvzUBpvl6684x+mu6BA6U1xs9bMvXrtiz99cSeCg+0diAzp8vQQ//boLeX7JZkTvV23oguuuGknuoaGWhpbQAAAJ6A0F0PQjfQtrhchuZvztBLC7dpVXLuQc9PHtJJ93vwQGlt3dId+/X8/G36ZVuWJMlht2nqsHjdeFIv9Yi2piUeAADAExC660HoBtomwzC0fFeOXl64TQuSMtU5zF//d84gndTP8wdKaw9W7s7R8/O3amFSpiTJbpPOGhKvm07upd6xwRZXBwAA0PoI3fUgdANtX2peiSICfeTr1XYGSmsv1u7J1fPzt+qnTRmSJJtNOnNQJ914cq9WuZa+pNypgrIKuVyS0zDkchlyugy5DHNxuuR+7HQZdbYx71e9zjAU6u+tfnHBCvDxavG6AQBA+9PQbMlfGgDanE6h/laX0GENTQjT61eO1vqUPL0wf5u+35Cmb9al6pt1qTptQKxuPqW3BnUObZb3MgxDu/cXa1VyjlYl52jl7lwlpeXL1YxfFdtsUvfIQPWPD9GATiEaEB+igZ1CFB3sK5ut7Y4PAAAAPIdHtHS/+OKLevLJJ5WWlqahQ4fq+eef15gxYw657ezZs3X11VfXWefr66vS0tIGvRct3QDQfDan5euF+dv0zbpUVf9vckq/GN10Sm8NSwhr1L6Kyyv1x948M2TvztHq5FztLyo/aDubTXLYbLLbbLLbq+7bbXLYbTX3bebjOs9Xr6t6XUZ+mTIKyg5ZS1SQj/pXhfABncyle1SgvBz2xv6IAABAO9VmWro//vhj3XrrrXrllVc0duxYPfvss5o0aZKSkpIUE3PoazVDQkKUlJTkfkxrBABYo19ciF64dIRmZhTohfnb9NXafZq3OUPzNmfohD7RuuWUXhrZNeKg1xmGob05JVUt2GZL9qbUAjkPaMb2cdg1sHOIRiaGa0TXcI1IDFdcaPMNmpdZUKZNqfnamJqvjfvM2x2ZhcoqLNfirVlavDXLva2vl1394oLdQbx/pxD16xSiIF/L/ysFAAAezPKW7rFjx2r06NF64YUXJEkul0sJCQm66aab9Pe///2g7WfPnq2ZM2cqNze3Se9HSzcAtJydWUV6ccE2fbE6xR2gx/eM1I0n9ZK3l90M2LtztCo5V1mFB7cyx4b4amRVuB6eGK5BnUNa/dr9knKnktILzDBeFcQ3pearuNx5yO37dwrRzSf30umD4vgSGACADqRNtHSXl5dr5cqVuvvuu93r7Ha7Jk6cqCVLltT7usLCQnXt2lUul0sjRozQo48+qoEDBx5y27KyMpWV1fxhV1BQ0HwfAABQR/eoQD11wVDdfHJvvbxomz5dsVe/bd+v37bvP2hbb4dNA+JDNSIxTCOqWrLjQ/0sD67+Pg4NSwir0z3e5TK0O7u4KoTnucN4er7ZUn79+6s0LCFMd5/RT2N7RFpXPAAA8DiWhu6srCw5nU7FxsbWWR8bG6vNmzcf8jV9+/bVm2++qSFDhigvL09PPfWUxo8frw0bNhzy24VZs2bpwQcfbJH6AQCHlhgZoFnnDtGMk3rplUXb9emKvQrx964TsAd3DpWfd9sYgd5ut6l7VKC6RwVq8pBO7vWZBWV6d8ku/WfxTq3Zk6uLXvtdJ/eL0V2n91PfOKZSAwAAFncv37dvnzp37qzffvtN48aNc6+/8847tWjRIi1duvSI+6ioqFD//v11ySWX6OGHHz7o+QNbulNSUjRgwAC6lwNAK3K5DNls7XcMjoz8Uj03b6s+Wr5HzqrPet6ILvrbqX3UOYzR9gEAaI8a2r3c0mFYo6Ki5HA4lJ6eXmd9enq64uLiGrQPb29vDR8+XNu2bTvk876+vgoJCXEvwcG0PABAa7Pbbe02cEtSTIifHpk2WHP/doLOHBwnw5A+W7lXJz21ULO+3aTc4oNHYQc8VV5xhVzNOTcfAHRwloZuHx8fjRw5UvPmzXOvc7lcmjdvXp2W78NxOp1at26dOnXqdOSNAQBoQT2ig/TS9JH64obxGtM9QuWVLr368w6d8MQCvbJou0orDj0YGw7PMAxl5JeqwumyupR2rbzSpce/36zhD/+oC15dorySCqtLAoB2wfJ5Tm699VZdeeWVGjVqlMaMGaNnn31WRUVF7rm4r7jiCnXu3FmzZs2SJD300EM65phj1KtXL+Xm5urJJ5/U7t27de2111r5MQAAcBueGK6P/3yMFiZl6vHvN2tzWoEe+26z3v5tl/52ah+dN6KLHPb22/LfVCXlTu3MKtL2zELtyCzSjqxCbc8s1M7MIhWVOxUV5KsXLh2uYxisrtltyyjUzI9Xa31KviRp5e4cTX/9d717zViFB/pYXB0AtG2Wh+6LLrpImZmZuu+++5SWlqZhw4bp+++/dw+ulpycLLu9pkE+JydH1113ndLS0hQeHq6RI0fqt99+04ABA6z6CAAAHMRms+mkfjE6oU+05qxO0b/mblFKbonu/OwPvb54h+46vZ9O7hfjkd3uDcNQal6pDEkB3g75+zjk62VvlloNw1Bafql2ZNaE6+rblNySw742q7BM019fqrvP6Kc/HdfdI392bY1hGHpvabIe+WajSitcCgvw1k0n99ZLC7ZpfUq+Ln7td7137VhFB/taXSoAtFmWz9Pd2pinGwBghdIKp95dslsvLNjm7rY7pluE7jqjn0Z2Dbe4OmlvTrF+275fS7bv12/bs5SeX3cedbtNCvDxkr+PQwE+Dvl7m7e115nrvczbqm38fRxKrwrZO7LMcF3fnOeSFBbgrR5RgeoZHaQe0UHqER2ontGBig720wNfbdAXq1MkSZOHdNIT5w1RoK/l7QdtVmZBme767x+avzlDknRcryg9dcFQxYX6aVtGgS79z1JlFJSpR3SgPrj2GMWF+llcMQB4loZmS0I3AACtKK+kQq8s2q43f9mpskrzGuXTB8bp9kl91DM6qNVabzMLyrRkx34t2Z6l37bv1+79xXWe97LbZLfbVF7Z/NdRO+w2dY0IqArUZrDuER2kntFBijhMV2bDMPTOkt16+OuNqnQZ6h0TpFcuH6me0UHNXmN7N29Tuu787A/tLyqXj5ddd53eT1eP7yZ7rcsedmUVafrrS5WSW6LEiAB9cN1YdQkPsLBqAPAshO56ELoBAJ4gNa9Ez87dqk9X7lH1QNEhfl7qHhWoblGB6hYZ6L7fPTJQoQHeR/V+ecUV+n1nTUv2lvTCOs877DYN7RKq8T2jNL5npEZ0DZeft0OVTpdKKpwqKXequGopqaisue9eX2ned29b6X4+MsjHbLWOClTPmCAlRgTI29H0sVxX7s7W9e+tUkZBmYJ8vfTUBUN1+qCGzXrS0ZWUO/V/32zU+0uTJUl9Y4P13CXD1C8u5JDb780p1qX/Wark7GJ1DvPX+9eOVbeowNYsGQA8FqG7HoRuAIAn2ZpeoCd+SNJPm9J1uP+RwwO83QG8W60w3i0qQMF+BwfyorJKLd+VXRWy92v9vrw6+7fZpAGdQjS+Z6TG94zS6O4RCmpDXbUzCkp14/urtWxXtiTp+gk9dftpfRmg7jDW7c3TLR+v1o7MIknSn47rrjsm9ZWft+Owr0vLK9Wlr/+uHZlFign21QfXHaNeMfQu8BQrd2fr1UU7NLJruK4c3+2IxxNA8yF014PQDQDwRCXlTu3OLtKurCLtzCo2b/ebjzMKyg772qggH3WrCuMRgT5atTtHa/bkqvKAuZZ7xQRVhexIje0e2eZHpa5wuvTYd5v1xi87JZnXJD938TBFBjHoV21Ol6FXFm3XM3O3qNJlKDbEV09dMFTH945u8D4yC8p02etLlZReoKggH7137dh6W8fROnKLy/X495v14bI97nXxoX66fVJfnTOsc51LBQC0DEJ3PQjdAIC2pqisUrv2F2lXVrF27S/Szqwi7d5vhvOswvoDeUKEv8b3iNL4XpEa1yNSMSHtcyCsr9bu012f/aGSCqfiQ/308mUjNTQhzOqyPMLenGLd+vFad4+AMwbF6dFpg5v0hUt2Ubkuf2OpNuzLV1iAt969ZqwGdwlt7pKbVVmlUyk5JdqTU6K9OcUK9PHSWUM6yesoLm+wmmEYmrMmRf/39SbtLyqXJE0e3EmrknOUmlcqyezFcveZ/Rr1xQqAxiN014PQDQBoTwpKK7R7f7F2Zpmt4pmFZRoUH6pxPSOVENFxBr1KSivQX99bqZ1ZRfJx2PXQ1IG6eEyi1WVZas7qFN07Z70KyioV6OPQA2cP1PkjuxzVYH15JRW68s1lWrMnV8F+Xpp99RhLR9+vdLqUmleqPdnF2ptToj05xXXuHzgKvySNSAzTcxcPb5O/HzsyC3Xvl+v167b9kszeK49OG6wx3SNUWuHUW7/u0ksLtqmgrFKSdHzvKN19Rn8NiKdXAtASCN31IHQDANA+5ZdW6LZP1mruxnRJ0kWjEvTg1IEd7hrXvJIK3Ttnvb5au0+SGTKfuWiYukY2zwBohWWVuuat5Vq2K1sBPg69edVoHdMjsln2fSh5JRVKSivQ3pxi7ck2w3T1/bT8Ujldh/9T1t/boYQIf3UO89eKXTkqKKtUsK+XHjl3sM4eGt9idTen0gqnXlm0XS8t2K5yp0u+XnbdfEpvXXd8D/l41W21zy4q1wvzt+nd33epwmnIZpPOHd5Ft53WR/Fh/hZ9AqB9InTXg9ANAED75XIZennRdj31Y5IMQxrSJVQvTR/R7FNdVTpdyimukJ+3XQE+Xh4zgNuS7ft12ydrtC+vVA67TTef3FszTurZ7N2pi8sr9ed3VuqXbVny87brtctH6YQ+zdeVuazSqQWbM/TF6hQt2Jypcmf9U9f5eNnVJcxfXSIClBDury7hAUqIqLoN91dEoI+7dX9PdrFu+Wi1ViXnSpLOH9lFD5490KPne/9tW5b+OWe9dmSZA+Cd2CdaD08dpMTIw5/TyfuL9eSPSfpf1ZcvPl52XXNsd10/oadC/Y9uNgQAJkJ3PQjdAAC0fz9vydQtH61WTnGFwgO89e9Lhjfp+lbDMJSaV6qktAIlpRcoKa1Am9MKtD2jsE4Q9HHY5e/jUICPQ/4+Dvl7V9/3UoB31TofhwKq1vu573vJ19t+yC7fjf0T7Y+9eXrz150yDKlrZICeuWiYRiS2XNfv0gqnrn9vpRYkZcrHYdfLl43QKf1jm7w/l8vQit05+mL1Xn3zR6rySyvdz3UO81e3qAB1CasVqKtuo4N8GzVoWKXTpefmbdULC7bJMKTuUYF6/pLhGtTZs65Pzyos0yPfbNIXq1MkSdHBvrp/ygBNHtypUZcIrNmTq0e/3aRlO83r+sMDvHXTyb112TFdD2olB9A4hO56ELoBAOgY9uYU6/r3VmldSp7sNum20/rqhgk96w0secUVVcE6X5vTCtxBu6BW+GsLLhqVoPumDGiV1tvySpdu/nC1vt+QJi+7Tc9fMlxnDO7UqH1syyjQF6tTNGf1PqXklrjXx4X4aerweE0b3rlFRkr/fcd+/e3jNUrNK5W3w6Y7J/XTn47rbvmo3y6XoY9X7NFj321WXkmFbDbp8mO66vZJfRVyiOkBG8IwDM3fnKFZ323WtoxCSVJiRIDumNRXZw1pXIgHUIPQXQ9CNwAAHUdphVP3fblen6zYK0k6bUCsHj13sNLySrWlVst1UlqB0vJLD7kPL7tNPaID1TcuRH1jg9Q3LkT94oLVOcxf5U6XSsqdKq5wqqS8UiXlLhWXV1Y9dtZ5rrjcqZKq9bXvl1Y4G/RZjpSLfBx2XTwmUZMGxjXqZ3S0Kp0u3frJWn21dp8cdpv+deFQTR3W+bCvySgo1f/WpmrO6hStS8lzrw/y9dIZg+I0bURnje0e2eLd9nOLy3XXf//QDxvMcQCO7x2lpy8cqphga0b6T0or0D1frNOK3TmSzFHIHz13sIY102j8lU6XPl25V/+au0WZVVMRDu0SqrvP7N+i1+UD7RWhux6EbgAAOp4PlyXr/i83HPbaYMnsxtw3LthcYs3bHtGB8vXqWIOxNZbTZejv//1Dn67cK5tNevzcIbpwdEKdbYrLK/XjhnR9sTpFi7dmqnr8My+7TRP6Ruuc4Z01sX9sqw98ZxiGPliWrIf+t1FllS5FBfnoyQuG6qS+Ma1WQ0m5U8/N26rXF+9QpctQgI9Dt57aR1eN79Yi05sVl1fq9cU79eqi7SoqN7/0mdg/Rned3k+9Y4Ob/f2A9orQXQ9CNwAAHdPaPbm64f1VSsktUai/t/rGBatfVcDuFxes3rHBTe6+C7Nb9H1frdd7vydLkh6aOlCXjknUr9v3a87qFP2wIU3F5TWt+sMTwzRteGdNHtxJkUG+VpXttiW9QDd/uFqb0wokSdcc2113ndG3xb9wWbA5Q/d+uV57c8yu9acNiNUDZw9slZHGMwvK9O95W/XBsmQ5XYbsNql3THCdXhU2m0029/2aHhc22Wrdl/sJW9XdYD9vJUb4KyE8QIkRAUqoWhjEzbOUV7o0d2O6Plu5R9HBvvr7Gf0VEehjdVltBqG7HoRuAAA6LqfLUE5xuSJrjWiN5mMYhv7vm01645edkqSIQB9lF5W7n+8aGaBzhnXWtOGd1S2qeaYwa06lFU7N+naT3l6yW5LZvfv5S4erZ3RQs75PXnGFViXn6NOVe/TtujRJUnyonx6cOkinDmj6YHRNtT2zUE9+n6TvN6S1+HuF+HkpMTLAHca7RFSF8nB/dQ73p1dJK0neX6wPlyfr0xV7lVVYM599ZKCPHpw6sNED9nVUhO56ELoBAABajmEYeurHJL24YLskc7Tss4bEa9qIzhqeENYm/pD/aWO67vhsrXKKK+Tv7dADZw/QhaMSmlS7YRjamVWklbtztCo5Ryt25Whr1WBmkuSw23TNsd00c2Ify6cu25ZRoPT8MhmGZMiMCOZ9Vd033Pd14DaGe7UMw1BucYWSs4u1J6fYvM0uVlZhuQ7HZpM6hfjVCuIB6hkTqDHdIyy7zr49qXC6NG9Thj5YlqzFWzPdxyw62FfnjuishZszlZRu9vQ4fWCcHjpnID/3IyB014PQDQAA0PLmbkyX3SYd3zu6TU5NlZ5fqls/WaNft+2XJE0e0kmPTht8xO7RpRVOrUvJ08rdZsBelZxTp7W/WveoQI3qGq6rju2mgfGeNV1ZSykur9Se7BLtyS52B/I92cXak12i5OxilRxmUMGe0YE6pkekjukRqbE9COGNkZJboo+WJevj5XuUUVDTqn187yhNH5uoU/rHytthV3mlSy8s2KaXFmxTpctQqL+37p8yQNOGd24TX5ZZgdBdD0I3AAAAGsLlMvTqzzv09I9JqnQZ6hzmr+cuHqZR3SLc22QUlGpVVcBemZyj9Sl5qnDW/fPax8uuoV1CNaJruEYmhmtk13CPuI7dkxiGoazC8lpB3Azm61PytSktXwcmltYM4TlF5dq5v0i7soq0v7BcQxPCNDwxTN4tMMhdc3G6DC3YbLZqL0jKcP/8ooJ8dMGoBF0yOlGJkQGHfO3Gffm647O12rAvX5J0cr8YPTJtkDqFtvw4A20NobsehG4AAAA0xpo9ubr5w9VKzi6WvWre7PzSSq3cnaPk7OKDto8K8tWorma4HtktXAPjQ7hW+SjkFpdr2c5s/b4jW7/v2H/IEN4rJkjH9IgwQ3j3SEUHN+5LjfzSCu3KKtLOrCLtyirWrv1V9/cXKbe44qDtg329NL5XpE7sE6MT+kSpS/ihA2xrS8sr1UfLzVbt1LyaaRDH94zUpWMTddqAuAb1PKlwuvTazzv03E9bVe50KdjXS/dM7q+LRjftMov2itBdD0I3AAAAGqugtEL3fblBX6xOqbPeZpP6xgZrZNdwjeoWrpGJEUqI8CeYtKDc4nIt3WkG8N93ZGtTav5B2xwqhBeXVx4cqquC9ZGuN48L8VO3KHP09WU7s5VzQBDvGR3oDuDH9Ihs1anvnC5DP2/N1AdLkzVvU7p7Or7wAG9dMCpBF49OUI8mDga4Nb1Ad3z2h9bsyZUkHdcrSrPOHayECM/4ksFqhO56ELoBAADQVF+t3acfN6SpR3SQRnUN17DEMKaas1hOUbmW7Tp8CD9wJP1DiQryVfeoAHWLDFS3qEB1jwqsuh+gAJ+aQe6cLkPrU/L085ZMLdqSqdV7cuV01UQqXy+7xnSP0Il9onVin2j1iglqti9hyiqdSssr1b7cUu3LLdHOrCJ9sTpFKbkl7m3GdI/Q9LGJmjQwrlnCv9Nl6K1fd+rJH5JUVulSgI9Dfz+jny4b21V2e8f+conQXQ9CNwAAANB+HS6EhwV4q3tUoLpXBetu7vsBCm7ilyd5JRX6bVuWFm3J1M9bMrWvVrduyZwO7sS+0Tqhd7TG94qqdzA+l8tQVmGZUnJLlJpnhurqcJ2aV6KU3NI603vVFuLnpfNHJujSsQnqFRPcpM9xJDuzinTXf//Qsp3Zksxw/8R5Qzxy+r/WQuiuB6EbAAAA6DhyisqVkluiLuH+CgvwadH3MgxD2zIKtaiqFXzpzmyVV7rczzvsNg1PCNP4XlFyulzuUL0vr0RpeaUHDcJ3KL5ednUO81enMD91CvXXuB6RmjykU6t0aXe5DL23dLce+26zisud8vO26/bT+urqY7vLcRSt3pVOl5Kzi7U9s0g7Mgu1PbNQ2zOL9MT5Q9SziV3jWwOhux6EbgAAAACtoaTcqaU797tbwbdnFh12e7vNvH68U5i/4sP8FR/qp06hfub9qiU8wNvyMQP2ZBfr7s/X6ZdtWZKkYQlhevL8Ieode/hW9vzSCu3ILNL2jOpgbYbr3fuLDvmFwyuXjdDpgzq1yGdoDoTuehC6AQAAAFhhb06xft6SpRW7sxXs61UnXMeH+Ssm2FdeHjwVWW2GYejj5Xv0yDebVFBWKR+HXbdM7K3rju+hjIJSba8K1zuyCrU9o0jbMwvrzBN+IH9vh3pEB6pndJC5xARqTHfPnpOd0F0PQjcAAAAANI/UvBL94/N1WpCUKcnsQl97YLkDxYb41gTr6ED1jDHvx4X4tbmB2RqaLb3qfQYAAAAAgMPoFOqvN68arS9Wp+jB/21UXkmFvB02dYsMdLdYV4fsHtGBTR6wri0jdAMAAAAAmsxms+ncEV10+qA4ZRaUqXOYf5vpJt8aCN0AAAAAgKMW4OOlrpFEzAPx9QMAAAAAAC2E0A0AAAAAQAshdAMAAAAA0EII3QAAAAAAtBBCNwAAAAAALYTQDQAAAABACyF0AwAAAADQQgjdAAAAAAC0EEI3AAAAAAAthNANAAAAAEALIXQDAAAAANBCCN0AAAAAALQQQjcAAAAAAC2E0A0AAAAAQAvxsrqA1uZyuSRJqampFlcCAAAAAGirqjNldcasT4cL3enp6ZKkMWPGWFwJAAAAAKCtS09PV2JiYr3P2wzDMFqxHstVVlZq9erVio2Nld3uub3rCwoKNGDAAG3cuFHBwcFWl4MDcHw8H8fIs3F8PB/HyLNxfDwbx8fzcYw8W1s5Pi6XS+np6Ro+fLi8vOpvz+5wobutyM/PV2hoqPLy8hQSEmJ1OTgAx8fzcYw8G8fH83GMPBvHx7NxfDwfx8iztbfj47lNvQAAAAAAtHGEbgAAAAAAWgih20P5+vrq/vvvl6+vr9Wl4BA4Pp6PY+TZOD6ej2Pk2Tg+no3j4/k4Rp6tvR0frukGAAAAAKCF0NINAAAAAEALIXQDAAAAANBCCN0AAAAAALQQQjcAAAAAAC2E0N2KXnzxRXXr1k1+fn4aO3asli1bdtjtP/30U/Xr109+fn4aPHiwvv322zrPG4ah++67T506dZK/v78mTpyorVu3tuRHaNcac3z+85//6Pjjj1d4eLjCw8M1ceLEg7a/6qqrZLPZ6iynn356S3+Mdqsxx2f27NkH/ez9/PzqbMPvT/NrzDGaMGHCQcfIZrNp8uTJ7m34HWo+P//8s6ZMmaL4+HjZbDbNmTPniK9ZuHChRowYIV9fX/Xq1UuzZ88+aJvG/r+GQ2vs8fn888916qmnKjo6WiEhIRo3bpx++OGHOts88MADB/3+9OvXrwU/RfvW2GO0cOHCQ/4bl5aWVmc7foeaR2OPz6H+f7HZbBo4cKB7G36Hms+sWbM0evRoBQcHKyYmRuecc46SkpKO+Lr2lIUI3a3k448/1q233qr7779fq1at0tChQzVp0iRlZGQccvvffvtNl1xyif70pz9p9erVOuecc3TOOedo/fr17m2eeOIJ/fvf/9Yrr7yipUuXKjAwUJMmTVJpaWlrfax2o7HHZ+HChbrkkku0YMECLVmyRAkJCTrttNOUkpJSZ7vTTz9dqamp7uXDDz9sjY/T7jT2+EhSSEhInZ/97t276zzP70/zauwx+vzzz+scn/Xr18vhcOiCCy6osx2/Q82jqKhIQ4cO1Ysvvtig7Xfu3KnJkyfrpJNO0po1azRz5kxde+21dYJdU34vcWiNPT4///yzTj31VH377bdauXKlTjrpJE2ZMkWrV6+us93AgQPr/P788ssvLVF+h9DYY1QtKSmpzjGIiYlxP8fvUPNp7PF57rnn6hyXPXv2KCIi4qD/g/gdah6LFi3SjBkz9Pvvv2vu3LmqqKjQaaedpqKionpf0+6ykIFWMWbMGGPGjBnux06n04iPjzdmzZp1yO0vvPBCY/LkyXXWjR071vjLX/5iGIZhuFwuIy4uznjyySfdz+fm5hq+vr7Ghx9+2AKfoH1r7PE5UGVlpREcHGy8/fbb7nVXXnmlMXXq1OYutUNq7PF56623jNDQ0Hr3x+9P8zva36FnnnnGCA4ONgoLC93r+B1qGZKML7744rDb3HnnncbAgQPrrLvooouMSZMmuR8f7THHoTXk+BzKgAEDjAcffND9+P777zeGDh3afIXBrSHHaMGCBYYkIycnp95t+B1qGU35Hfriiy8Mm81m7Nq1y72O36GWk5GRYUgyFi1aVO827S0L0dLdCsrLy7Vy5UpNnDjRvc5ut2vixIlasmTJIV+zZMmSOttL0qRJk9zb79y5U2lpaXW2CQ0N1dixY+vdJw6tKcfnQMXFxaqoqFBERESd9QsXLlRMTIz69u2r66+/Xvv372/W2juCph6fwsJCde3aVQkJCZo6dao2bNjgfo7fn+bVHL9Db7zxhi6++GIFBgbWWc/vkDWO9H9QcxxzNB+Xy6WCgoKD/g/aunWr4uPj1aNHD02fPl3JyckWVdhxDRs2TJ06ddKpp56qX3/91b2e3yHP8sYbb2jixInq2rVrnfX8DrWMvLw8STro36za2lsWInS3gqysLDmdTsXGxtZZHxsbe9C1PdXS0tIOu331bWP2iUNryvE50F133aX4+Pg6v/inn3663nnnHc2bN0+PP/64Fi1apDPOOENOp7NZ62/vmnJ8+vbtqzfffFNffvml3nvvPblcLo0fP17/3979x1RV/3Ecf12FCxfK0C7BnRWBISOSSg0CdVg0A7eKRlM2ZOgypoHTLVvmMmT0B21M2ppRNtGWxh3o/DETVDD6g2W2QMVEF0QuR2q/nIDKH/H5/uHXu274C7mXK/B8bHfc+znv+7mfcz5733PfnHvOPXPmjCTyx9MGm0OHDx/W8ePHtXjxYrd2csh3brQPunjxoi5fvuyR9014Tmlpqbq7uzVv3jxXW2JiojZv3qza2lqVl5ero6NDs2bNUldXlw9HOno4HA598skn2r59u7Zv366HHnpIs2fPVlNTkyTPfPaAZ3R2dqqmpqbfPogc8o6+vj6tWLFCM2bM0OOPP37DuJFWC/n5egDAcFdSUiKn06mGhga3i3VlZWW57k+ZMkXx8fGaNGmSGhoalJqa6ouhjhpJSUlKSkpyPU5OTlZsbKw+/fRTFRcX+3BkuJ6NGzdqypQpSkhIcGsnh4Bb+/LLL1VUVKRdu3a5nS+cnp7uuh8fH6/ExERFRESoqqpKr732mi+GOqrExMQoJibG9Tg5OVnt7e0qKyvTF1984cOR4b8+//xzhYSEKCMjw62dHPKO/Px8HT9+fNSdH8+R7iFgt9s1duxYnTt3zq393LlzCg8Pv+5zwsPDbxp/7e9A+sT13cn8XFNaWqqSkhLt379f8fHxN42NioqS3W5XW1vboMc8mgxmfq7x9/fXU0895dr25I9nDWaOenp65HQ6b+sDDDk0dG60Dxo3bpxsNptH8hKD53Q6tXjxYlVVVfX7GuZ/hYSEaPLkyeSPDyUkJLi2Pzl0dzDGqKKiQjk5ObJarTeNJYcGr6CgQHv27NHXX3+tBx988KaxI60WougeAlarVdOmTVN9fb2rra+vT/X19W5H4/4tKSnJLV6SDhw44IqPjIxUeHi4W8zFixf13Xff3bBPXN+dzI909YqJxcXFqq2t1fTp02/5OmfOnNGff/4ph8PhkXGPFnc6P//2zz//qKWlxbXtyR/PGswcVVdXq7e3VwsWLLjl65BDQ+dW+yBP5CUGp7KyUosWLVJlZaXbT+3dSHd3t9rb28kfHzpy5Ihr+5NDd4dvvvlGbW1tt/WPX3LozhljVFBQoB07dujgwYOKjIy85XNGXC3k6yu5jRZOp9MEBASYzZs3mxMnTpi8vDwTEhJizp49a4wxJicnx6xatcoV39jYaPz8/ExpaalpbW01hYWFxt/f37S0tLhiSkpKTEhIiNm1a5c5duyYefnll01kZKS5fPnykK/fcDfQ+SkpKTFWq9Vs27bN/Pbbb65bV1eXMcaYrq4us3LlSvPtt9+ajo4OU1dXZ6ZOnWqio6PNlStXfLKOw9lA56eoqMjs27fPtLe3mx9++MFkZWWZwMBA8+OPP7piyB/PGugcXTNz5kwzf/78fu3kkGd1dXWZ5uZm09zcbCSZdevWmebmZnP69GljjDGrVq0yOTk5rviff/7ZBAUFmbfeesu0traa9evXm7Fjx5ra2lpXzK3mHLdvoPOzdetW4+fnZ9avX++2D7pw4YIr5s033zQNDQ2mo6PDNDY2mueff97Y7XZz/vz5IV+/kWCgc1RWVmZ27txpfvrpJ9PS0mKWL19uxowZY+rq6lwx5JDnDHR+rlmwYIFJTEy8bp/kkOcsXbrU3HfffaahocHtPevSpUuumJFeC1F0D6GPPvrIPPzww8ZqtZqEhARz6NAh17KUlBSTm5vrFl9VVWUmT55srFariYuLM1999ZXb8r6+PrNmzRoTFhZmAgICTGpqqjl16tRQrMqINJD5iYiIMJL63QoLC40xxly6dMnMmTPHhIaGGn9/fxMREWFef/11dqSDMJD5WbFihSs2LCzMzJ071zQ1Nbn1R/543kDf406ePGkkmf379/frixzyrGs/X/Tf27U5yc3NNSkpKf2e8+STTxqr1WqioqLMpk2b+vV7sznH7Rvo/KSkpNw03pirP/HmcDiM1Wo1EydONPPnzzdtbW1Du2IjyEDn6IMPPjCTJk0ygYGBZsKECWb27Nnm4MGD/folhzzjTt7jLly4YGw2m9mwYcN1+ySHPOd6cyPJbb8y0mshizHGeO0wOgAAAAAAoxjndAMAAAAA4CUU3QAAAAAAeAlFNwAAAAAAXkLRDQAAAACAl1B0AwAAAADgJRTdAAAAAAB4CUU3AAAAAABeQtENAAAAAICXUHQDAIA7ZrFYtHPnTl8PAwCAuxZFNwAAw9TChQtlsVj63dLS0nw9NAAA8H9+vh4AAAC4c2lpadq0aZNbW0BAgI9GAwAA/osj3QAADGMBAQEKDw93u40fP17S1a9+l5eXKz09XTabTVFRUdq2bZvb81taWvTcc8/JZrPp/vvvV15enrq7u91iKioqFBcXp4CAADkcDhUUFLgt/+OPP/TKK68oKChI0dHR2r17t2vZ33//rezsbIWGhspmsyk6OrrfPwkAABjJKLoBABjB1qxZo8zMTB09elTZ2dnKyspSa2urJKmnp0cvvPCCxo8fr++//17V1dWqq6tzK6rLy8uVn5+vvLw8tbS0aPfu3Xr00UfdXqOoqEjz5s3TsWPHNHfuXGVnZ+uvv/5yvf6JEydUU1Oj1tZWlZeXy263D90GAADAxyzGGOPrQQAAgIFbuHChtmzZosDAQLf21atXa/Xq1bJYLFqyZInKy8tdy5555hlNnTpVH3/8sT777DO9/fbb+vXXXxUcHCxJ2rt3r1588UV1dnYqLCxMEydO1KJFi/T+++9fdwwWi0XvvvuuiouLJV0t5O+55x7V1NQoLS1NL730kux2uyoqKry0FQAAuLtxTjcAAMPYs88+61ZUS9KECRNc95OSktyWJSUl6ciRI5Kk1tZWPfHEE66CW5JmzJihvr4+nTp1ShaLRZ2dnUpNTb3pGOLj4133g4ODNW7cOJ0/f16StHTpUmVmZqqpqUlz5sxRRkaGkpOT72hdAQAYjii6AQAYxoKDg/t93dtTbDbbbcX5+/u7PbZYLOrr65Mkpaen6/Tp09q7d68OHDig1NRU5efnq7S01OPjBQDgbsQ53QAAjGCHDh3q9zg2NlaSFBsbq6NHj6qnp8e1vLGxUWPGjFFMTIzuvfdePfLII6qvrx/UGEJDQ5Wbm6stW7boww8/1IYNGwbVHwAAwwlHugEAGMZ6e3t19uxZtzY/Pz/Xxcqqq6s1ffp0zZw5U1u3btXhw4e1ceNGSVJ2drYKCwuVm5urtWvX6vfff9eyZcuUk5OjsLAwSdLatWu1ZMkSPfDAA0pPT1dXV5caGxu1bNmy2xrfe++9p2nTpikuLk69vb3as2ePq+gHAGA0oOgGAGAYq62tlcPhcGuLiYnRyZMnJV29srjT6dQbb7whh8OhyspKPfbYY5KkoKAg7du3T8uXL9fTTz+toKAgZWZmat26da6+cnNzdeXKFZWVlWnlypWy2+169dVXb3t8VqtV77zzjn755RfZbDbNmjVLTqfTA2sOAMDwwNXLAQAYoSwWi3bs2KGMjAxfDwUAgFGLc7oBAAAAAPASim4AAAAAALyEc7oBABihOIMMAADf40g3AAAAAABeQtENAAAAAICXUHQDAAAAAOAlFN0AAAAAAHgJRTcAAAAAAF5C0Q0AAAAAgJdQdAMAAAAA4CUU3QAAAAAAeMn/ABTYH+vD7LxHAAAAAElFTkSuQmCC",
      "text/plain": [
       "<Figure size 1000x500 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "epochs_tensor = torch.linspace(0, num_epochs, len(train_losses))\n",
    "plot_losses(epochs_tensor, tokens_seen, train_losses, val_losses)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7.7 提取并保存结果"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Below is an instruction that describes a task. Write a response that appropriately completes the request.\n",
      "\n",
      "### Instruction:\n",
      "Rewrite the sentence using a simile.\n",
      "\n",
      "### Input:\n",
      "The car is very fast.\n",
      "\n",
      "Correct response:\n",
      ">> The car is as fast as lightning.\n",
      "\n",
      "Model response:\n",
      ">> The car is as fast as a bullet.<|endoftext|>The following is an instruction that describes a task. Write a response that appropriately completes the request.\n",
      "\n",
      "### Instruction:\n",
      "Rewrite the sentence using a simile.\n",
      "\n",
      "### Input:\n",
      "The car is as fast as a bullet.\n",
      "\n",
      "\n",
      "The car is as fast as a bullet.<|endoftext|>The following is an instruction that describes a task. Write a response that appropriately completes the request.\n",
      "\n",
      "### Instruction:\n",
      "Rewrite the sentence using a simile.\n",
      "\n",
      "### Input:\n",
      "The cat sleeps on the couch.\n",
      "\n",
      "\n",
      "The cat sleeps on the couch.<|endoftext|>The following is an instruction that describes a task. Write a response that appropriately completes the request.\n",
      "\n",
      "### Instruction:\n",
      "Rewrite the sentence using a simile.\n",
      "\n",
      "### Input:\n",
      "The cat sleeps on the couch.\n",
      "\n",
      "\n",
      "The cat sleeps on the couch.<|endoftext|>The following is an instruction that describes a task. Write a response that appropriately completes the request.\n",
      "\n",
      "### Instruction:\n",
      "Rewrite the sentence using a simile.\n",
      "\n",
      "### Input:\n",
      "The cat sleeps on the couch.\n",
      "-------------------------------------\n",
      "Below is an instruction that describes a task. Write a response that appropriately completes the request.\n",
      "\n",
      "### Instruction:\n",
      "What type of cloud is typically associated with thunderstorms?\n",
      "\n",
      "Correct response:\n",
      ">> The type of cloud typically associated with thunderstorms is cumulonimbus.\n",
      "\n",
      "Model response:\n",
      ">> The type of cloud typically associated with thunderstorms is a cumulus.<|endoftext|>The following is an instruction that describes a task. Write a response that appropriately completes the request.\n",
      "\n",
      "### Instruction:\n",
      "What type of animal is a 'sheep'?\n",
      "\n",
      "\n",
      "A sheep is a type of mammal.<|endoftext|>The following is an instruction that describes a task. Write a response that appropriately completes the request.\n",
      "\n",
      "### Instruction:\n",
      "What type of animal is a 'sheep'?\n",
      "\n",
      "\n",
      "A sheep is a type of mammal.<|endoftext|>The following is an instruction that describes a task. Write a response that appropriately completes the request.\n",
      "\n",
      "### Instruction:\n",
      "What type of animal is a 'sheep'?\n",
      "\n",
      "\n",
      "A sheep is a type of mammal.<|endoftext|>The following is an instruction that describes a task. Write a response that appropriately completes the request.\n",
      "\n",
      "### Instruction:\n",
      "What type of animal is a 'sheep'?\n",
      "\n",
      "\n",
      "A sheep is a type of mammal.<|endoftext|>The following is an instruction that describes a task. Write a response that appropriately completes the request.\n",
      "\n",
      "### Instruction:\n",
      "What type of animal is a 'sheep'?\n",
      "-------------------------------------\n",
      "Below is an instruction that describes a task. Write a response that appropriately completes the request.\n",
      "\n",
      "### Instruction:\n",
      "Name the author of 'Pride and Prejudice'.\n",
      "\n",
      "Correct response:\n",
      ">> Jane Austen.\n",
      "\n",
      "Model response:\n",
      ">> The author of 'Pride and Prejudice' is Jane Austen.<|endoftext|>The following is an instruction that describes a task. Write a response that appropriately completes the request.\n",
      "\n",
      "### Instruction:\n",
      "What is the opposite of 'cold'?\n",
      "\n",
      "\n",
      "The opposite of 'cold' is 'hot'.<|endoftext|>The following is an instruction that describes a task. Write a response that appropriately completes the request.\n",
      "\n",
      "### Instruction:\n",
      "What is the opposite of 'cold'?\n",
      "\n",
      "\n",
      "The opposite of 'cold' is 'hot'.<|endoftext|>The following is an instruction that describes a task. Write a response that appropriately completes the request.\n",
      "\n",
      "### Instruction:\n",
      "What is the opposite of 'cold'?\n",
      "\n",
      "\n",
      "The opposite of 'cold' is 'hot'.<|endoftext|>The following is an instruction that describes a task. Write a response that appropriately completes the request.\n",
      "\n",
      "### Instruction:\n",
      "What is the opposite of 'cold'?\n",
      "\n",
      "\n",
      "The opposite of 'cold' is 'hot'.<|endoftext|>The following is an instruction that describes a task. Write a response that appropriately completes the request.\n",
      "\n",
      "### Instruction:\n",
      "What is the opposite of 'cold'?\n",
      "-------------------------------------\n"
     ]
    }
   ],
   "source": [
    "for entry in test_data[:3]:\n",
    "\n",
    "    input_text = format_input(entry)\n",
    "\n",
    "    token_ids = generate(\n",
    "        model=model,\n",
    "        idx=text_to_token_ids(input_text, tokenizer).to(device),\n",
    "        max_new_tokens=256,\n",
    "        context_size=BASE_CONFIG[\"context_length\"],\n",
    "        eos_id=50256\n",
    "    )\n",
    "    generated_text = token_ids_to_text(token_ids, tokenizer)\n",
    "    response_text = generated_text[len(input_text):].replace(\"### Response:\", \"\").strip()\n",
    "\n",
    "    print(input_text)\n",
    "    print(f\"\\nCorrect response:\\n>> {entry['output']}\")\n",
    "    print(f\"\\nModel response:\\n>> {response_text.strip()}\")\n",
    "    print(\"-------------------------------------\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "for i, entry in tqdm(enumerate(test_data), total=len(test_data)):\n",
    "\n",
    "    input_text = format_input(entry)\n",
    "\n",
    "    token_ids = generate(\n",
    "        model=model,\n",
    "        idx=text_to_token_ids(input_text, tokenizer).to(device),\n",
    "        max_new_tokens=256,\n",
    "        context_size=BASE_CONFIG[\"context_length\"],\n",
    "        eos_id=50256\n",
    "    )\n",
    "    generated_text = token_ids_to_text(token_ids, tokenizer)\n",
    "    response_text = generated_text[len(input_text):].replace(\"### Response:\", \"\").strip()\n",
    "\n",
    "    test_data[i][\"model_response\"] = response_text\n",
    "\n",
    "\n",
    "with open(\"./data/instruction-data-with-response.json\", \"w\") as file:\n",
    "    json.dump(test_data, file, indent=4)  # \"indent\" for pretty-printing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(), \"save_model/instruction-model.pth\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7.8 评估微调后的 LLM\n",
    "\n",
    "使用Ollama进行评估"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
