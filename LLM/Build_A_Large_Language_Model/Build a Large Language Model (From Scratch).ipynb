{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1 理解大语言模型 - Large Language Model (LLM)\n",
    "\n",
    "> 主要结构如下：\n",
    "从raw data中进行预训练，得出基础模型（这一部分可以了解一下元学习的概念），这个基础模型所拥有的基础能力为文本补全、短时任务的推理能力。</br>\n",
    "> 在基础模型之上，可以导入自己标记的数据进行训练，这一部分可以成为微调（finetune），得到自己的LLM，可以用于分类，总结，翻译，个人助理等任务。\n",
    "\n",
    "![1716275709784](image/从零开始构建LLM/1716275709784.png)\n",
    "\n",
    "> **Transformer** 结构概览</br>\n",
    "1、输入需要被翻译的文本</br>\n",
    "2、预处理文本</br>\n",
    "3、编码器将输入文本进行编码</br>\n",
    "4、将编码部分送入解码器</br>\n",
    "5、模型每次只完成一个单词的翻译</br>\n",
    "6、预处理文本</br>\n",
    "7、解码器生成一个单词</br>\n",
    "8、完成翻译</br>\n",
    "\n",
    "![1716275687724](image/从零开始构建LLM/1716275687724.png)\n",
    "\n",
    "> BERT与GPT区别：BERT更多的使用于文本填空，GPT则是预测下一个单词。\n",
    "\n",
    "![1716275758151](image/从零开始构建LLM/1716275758151.png)\n",
    "\n",
    "> **构建大模型步骤**</br>\n",
    "\n",
    "|阶段|子项|\n",
    "|---|---|\n",
    "|一|准备数据和样本|\n",
    "||实现注意力机制|\n",
    "||实现LLM结构|\n",
    "|二|训练|\n",
    "||模型评估|\n",
    "||加载预训练模型权重|\n",
    "|三|微调自己的模型|\n",
    "\n",
    "![1716275818354](image/从零开始构建LLM/1716275818354.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2 文本数据处理"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.1 词嵌入\n",
    "词嵌入的根本目的是为了**将非数值数据转换为向量**，这样才能放入计算机进行运算。常见词嵌入的有**Word2Vec**。在GPT架构中，没有使用这一技术，GPT3的嵌入大小达到了12288维。其中，GPT将词嵌入作为训练模型，不断调整。也就是说，**GPT将词嵌入这一部分也进行训练**。\n",
    "\n",
    "![1716433691383](image/从零开始构建LLM/1716433691383.png)\n",
    "\n",
    "## 2.2 标记文本\n",
    "标记文本就是将文本进行拆分，拆分为单个单词后，对每个单词进行唯一映射。可以使用字典进行标记，将每个单词映射为token id，再使用token id进行词嵌入。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">> Total number of character: 20479\n",
      ">> raw text: I HAD always thought Jack Gisburn rather a cheap genius--though a good fellow enough--so it was no g\n",
      "\n",
      ">> preprocessed: ['I', 'HAD', 'always', 'thought', 'Jack', 'Gisburn', 'rather', 'a', 'cheap', 'genius', '--', 'though', 'a', 'good', 'fellow', 'enough', '--', 'so', 'it', 'was', 'no', 'great', 'surprise', 'to', 'me', 'to', 'hear', 'that', ',', 'in']\n",
      ">> length: 4690\n",
      "\n",
      ">> size of vocab after removed duplicate words: 1130\n",
      ">> vocab: front 20 items\n",
      "! 0\n",
      "\" 1\n",
      "' 2\n",
      "( 3\n",
      ") 4\n",
      ", 5\n",
      "-- 6\n",
      ". 7\n",
      ": 8\n",
      "; 9\n",
      "? 10\n",
      "A 11\n",
      "Ah 12\n",
      "Among 13\n",
      "And 14\n",
      "Are 15\n",
      "Arrt 16\n",
      "As 17\n",
      "At 18\n",
      "Be 19\n",
      "Begin 20\n"
     ]
    }
   ],
   "source": [
    "filepath = os.path.join('data', 'the-verdict.txt')\n",
    "assert os.path.exists(filepath), f\"{filepath} is not exists.\"\n",
    "with open(filepath) as f:\n",
    "    raw_text = f.read()\n",
    "print(\">> Total number of character:\", len(raw_text))\n",
    "print(\">> raw text:\", raw_text[:100])\n",
    "print()\n",
    "\n",
    "# split raw text\n",
    "preprocessed = re.split(r'([,.:;?_!\"()\\']|--|\\s)', raw_text)\n",
    "preprocessed = [item.strip() for item in preprocessed if item.strip()]  # remove empty string\n",
    "print(\">> preprocessed:\", preprocessed[:30])\n",
    "print(\">> length:\", len(preprocessed))\n",
    "print()\n",
    "\n",
    "# remove duplicate words\n",
    "all_words = sorted(set(preprocessed))\n",
    "vocab_size = len(all_words)\n",
    "print(\">> size of vocab after removed duplicate words:\", vocab_size)\n",
    "\n",
    "# create vocab\n",
    "vocab = {token:integer for integer,token in enumerate(all_words)}\n",
    "print(\">> vocab: front 20 items\")\n",
    "for tok, i in vocab.items():\n",
    "    if i > 20:\n",
    "        break\n",
    "    print(tok, i)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![1716433945337](image/从零开始构建LLM/1716433945337.png)\n",
    "\n",
    "字典表的创建方式可以通过自己创建，通过创建后的字典表，可以实现文本与token id之间的互相转换。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">> original text:  \"It's the last he painted, you know,\" \n",
      "           Mrs. Gisburn said with pardonable pride.\n",
      ">> encoded data: [1, 56, 2, 850, 988, 602, 533, 746, 5, 1126, 596, 5, 1, 67, 7, 38, 851, 1108, 754, 793, 7]\n",
      ">> decoded data: \" It' s the last he painted, you know,\" Mrs. Gisburn said with pardonable pride.\n"
     ]
    }
   ],
   "source": [
    "class SimpleTokenizerV1:\n",
    "    def __init__(self, vocab):  # our vocab\n",
    "        self.str_to_int = vocab\n",
    "        self.int_to_str = {i:s for s,i in vocab.items()}  # reverse k, v\n",
    "    \n",
    "    def encode(self, text):  # our text\n",
    "        preprocessed = re.split(r'([,.?_!\"()\\']|--|\\s)', text)\n",
    "        preprocessed = [\n",
    "            item.strip() for item in preprocessed if item.strip()  # remove empty string\n",
    "        ]\n",
    "        ids = [self.str_to_int[s] for s in preprocessed]\n",
    "        return ids\n",
    "        \n",
    "    def decode(self, ids):\n",
    "        text = \" \".join([self.int_to_str[i] for i in ids])\n",
    "        # Replace spaces before the specified punctuations\n",
    "        text = re.sub(r'\\s+([,.?!\"()\\'])', r'\\1', text)\n",
    "        return text\n",
    "\n",
    "tokenizer = SimpleTokenizerV1(vocab)\n",
    "\n",
    "text = \"\"\"\"It's the last he painted, you know,\" \n",
    "           Mrs. Gisburn said with pardonable pride.\"\"\"\n",
    "print(\">> original text: \", text)\n",
    "\n",
    "ids = tokenizer.encode(text)\n",
    "print(\">> encoded data:\", ids)\n",
    "\n",
    "decoded_text = tokenizer.decode(ids)\n",
    "print(\">> decoded data:\", decoded_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![1716434053588](image/从零开始构建LLM/1716434053588.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.3 特殊处理\n",
    "正如一般的数据预处理流程，文本中的异常数据也应当注意。当上述字典表覆盖不全面时，针对不在字典表中的字符就需要特殊处理，并且不同句子之间，也需要分割符。</br>\n",
    "\n",
    "**为未知单词加入一些特殊标记**是非常有用的。作用如下：\n",
    "\n",
    "* 使用特殊标记来帮助 LLM 提供额外的上下文\n",
    "* 注：一些特殊标记如下<br/>\n",
    "    1. [BOS] Beginning of sequence. 文本开始<br/>\n",
    "    2. [EOS] end of sequence. 文本结束<br/>\n",
    "    3. [PAD] padding. 使训练文本长度统一<br/>\n",
    "    [UNK] 未知字符，不在字典表中<br/>\n",
    "* GPT-2中仅使用`<|endoftext|>`减少复杂性，`<|endoftext|>`与`[EOS]`用法类似。GPT-2同时使用`<|endoftext|>`来进行PAD操作。\n",
    "* 对于未知单词，GPT-2未使用[UNK]进行替代，而是使用字节对编码-(byte-pair encoding, BPE)将单词进行分解。\n",
    "\n",
    "因此在上述V1版本上，我们需要进行改进，将未知字符与分割符加入字典表中：\n",
    "\n",
    "`all_tokens.extend([\"<|endoftext|>\", \"<|unk|>\"])`\n",
    "\n",
    "![1716455910828](image/从零开始构建LLM/1716455910828.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">> size of vocab after removed duplicate words: 1161\n",
      ">> vocab: last 5 items\n",
      "('younger', 1156)\n",
      "('your', 1157)\n",
      "('yourself', 1158)\n",
      "('<|endoftext|>', 1159)\n",
      "('<|unk|>', 1160)\n"
     ]
    }
   ],
   "source": [
    "# preprocessed = re.split(r'([,.:;?_!\"()\\']|--|\\s)', raw_text)  # pre version\n",
    "preprocessed = re.split(r'([,.?_!\"()\\']|--|\\s)', raw_text)\n",
    "preprocessed = [item.strip() for item in preprocessed if item.strip()]\n",
    "\n",
    "all_tokens = sorted(list(set(preprocessed)))\n",
    "all_tokens.extend([\"<|endoftext|>\", \"<|unk|>\"])\n",
    "\n",
    "vocab = {token:integer for integer,token in enumerate(all_tokens)}\n",
    "\n",
    "vocab_size = len(vocab)\n",
    "print(\">> size of vocab after removed duplicate words:\", vocab_size)\n",
    "\n",
    "print(\">> vocab: last 5 items\")\n",
    "for i, tok in enumerate(list(vocab.items())[-5:]):\n",
    "    print(tok)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">> input text: Hello, do you like tea? <|endoftext|> In the sunlit terraces of the palace.\n",
      ">> encoded data: [1160, 5, 362, 1155, 642, 1000, 10, 1159, 57, 1013, 981, 1009, 738, 1013, 1160, 7]\n",
      ">> decoded data: <|unk|>, do you like tea? <|endoftext|> In the sunlit terraces of the <|unk|>.\n"
     ]
    }
   ],
   "source": [
    "class SimpleTokenizerV2:\n",
    "    def __init__(self, vocab):\n",
    "        self.str_to_int = vocab\n",
    "        self.int_to_str = { i:s for s,i in vocab.items()}\n",
    "    \n",
    "    def encode(self, text):\n",
    "        preprocessed = re.split(r'([,.?_!\"()\\']|--|\\s)', text)\n",
    "        preprocessed = [item.strip() for item in preprocessed if item.strip()]\n",
    "        preprocessed = [\n",
    "            item if item in self.str_to_int \n",
    "            else \"<|unk|>\" for item in preprocessed\n",
    "        ]\n",
    "\n",
    "        ids = [self.str_to_int[s] for s in preprocessed]\n",
    "        return ids\n",
    "        \n",
    "    def decode(self, ids):\n",
    "        text = \" \".join([self.int_to_str[i] for i in ids])\n",
    "        # Replace spaces before the specified punctuations\n",
    "        text = re.sub(r'\\s+([,.?!\"()\\'])', r'\\1', text)\n",
    "        return text\n",
    "\n",
    "\n",
    "tokenizer = SimpleTokenizerV2(vocab)\n",
    "\n",
    "text1 = \"Hello, do you like tea?\"\n",
    "text2 = \"In the sunlit terraces of the palace.\"\n",
    "\n",
    "text = \" <|endoftext|> \".join((text1, text2))\n",
    "\n",
    "print(\">> input text:\", text)\n",
    "\n",
    "ids = tokenizer.encode(text)\n",
    "print(\">> encoded data:\", ids)\n",
    "\n",
    "decoded_text = tokenizer.decode(ids)\n",
    "print(\">> decoded data:\", decoded_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.4 字节对编码\n",
    "\n",
    "`pip install tiktoken`\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in indexes: https://pypi.tuna.tsinghua.edu.cn/simple\n",
      "Requirement already satisfied: tiktoken in d:\\python\\python39\\lib\\site-packages (0.7.0)\n",
      "Requirement already satisfied: regex>=2022.1.18 in d:\\python\\python39\\lib\\site-packages (from tiktoken) (2023.12.25)\n",
      "Requirement already satisfied: requests>=2.26.0 in d:\\python\\python39\\lib\\site-packages (from tiktoken) (2.31.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in d:\\python\\python39\\lib\\site-packages (from requests>=2.26.0->tiktoken) (3.2.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in d:\\python\\python39\\lib\\site-packages (from requests>=2.26.0->tiktoken) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in d:\\python\\python39\\lib\\site-packages (from requests>=2.26.0->tiktoken) (2.0.4)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in d:\\python\\python39\\lib\\site-packages (from requests>=2.26.0->tiktoken) (2023.7.22)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install tiktoken\n",
    "import tiktoken"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">> encoded data: [15496, 11, 466, 345, 588, 8887, 30, 220, 50256, 554, 262, 4252, 18250, 8812, 2114, 286, 617, 34680, 27271, 13]\n",
      ">> decoded data: Hello, do you like tea? <|endoftext|> In the sunlit terraces of someunknownPlace.\n"
     ]
    }
   ],
   "source": [
    "tokenizer = tiktoken.get_encoding(\"gpt2\")\n",
    "\n",
    "text = \"Hello, do you like tea? <|endoftext|> In the sunlit terraces of someunknownPlace.\"\n",
    "\n",
    "ids = tokenizer.encode(text, allowed_special={\"<|endoftext|>\"})\n",
    "print(\">> encoded data:\", ids)\n",
    "\n",
    "decoded_text = tokenizer.decode(ids)\n",
    "print(\">> decoded data:\", decoded_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> BPE会将未知单词拆分成独立个体的单词\n",
    "\n",
    "![1716778401378](image/从零开始构建LLM/1716778401378.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.5 使用滑窗进行数据采样\n",
    "\n",
    "![1716778580547](image/从零开始构建LLM/1716778580547.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">> x: [290, 4920, 2241, 287]\n",
      ">> y: [4920, 2241, 287, 257]\n",
      "\n",
      ">> tokenizer encode in one context:\n",
      ">> [290] --> 4920\n",
      ">> [290, 4920] --> 2241\n",
      ">> [290, 4920, 2241] --> 287\n",
      ">> [290, 4920, 2241, 287] --> 257\n",
      "\n",
      ">> tokenizer decode in one context:\n",
      ">>  and -->  established\n",
      ">>  and established -->  himself\n",
      ">>  and established himself -->  in\n",
      ">>  and established himself in -->  a\n"
     ]
    }
   ],
   "source": [
    "enc_text = tokenizer.encode(raw_text)\n",
    "enc_sample = enc_text[50:]\n",
    "\n",
    "context_size = 4\n",
    "x = enc_sample[:context_size]\n",
    "y = enc_sample[1:context_size + 1]\n",
    "print(f\">> x: {x}\")\n",
    "print(f\">> y: {y}\")\n",
    "print()\n",
    "\n",
    "print(\">> tokenizer encode in one context:\")\n",
    "for i in range(1, context_size + 1):\n",
    "    context = enc_sample[:i]\n",
    "    desired = enc_sample[i]\n",
    "    print(f\">> {context} --> {desired}\")\n",
    "print()\n",
    "\n",
    "print(\">> tokenizer decode in one context:\")\n",
    "for i in range(1, context_size + 1):\n",
    "    context = enc_sample[:i]\n",
    "    desired = enc_sample[i]\n",
    "    print(f\">> {tokenizer.decode(context)} --> {tokenizer.decode([desired])}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "因此，我们主要关心的只有两个向量，输入和输出\n",
    "\n",
    "![1716778596288](image/从零开始构建LLM/1716778596288.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in indexes: https://pypi.tuna.tsinghua.edu.cn/simple\n",
      "Requirement already satisfied: torch in d:\\python\\python39\\lib\\site-packages (2.1.2)\n",
      "Requirement already satisfied: filelock in d:\\python\\python39\\lib\\site-packages (from torch) (3.12.3)\n",
      "Requirement already satisfied: typing-extensions in d:\\python\\python39\\lib\\site-packages (from torch) (4.10.0)\n",
      "Requirement already satisfied: sympy in d:\\python\\python39\\lib\\site-packages (from torch) (1.12)\n",
      "Requirement already satisfied: networkx in d:\\python\\python39\\lib\\site-packages (from torch) (3.1)\n",
      "Requirement already satisfied: jinja2 in d:\\python\\python39\\lib\\site-packages (from torch) (3.1.2)\n",
      "Requirement already satisfied: fsspec in d:\\python\\python39\\lib\\site-packages (from torch) (2023.12.2)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in d:\\python\\python39\\lib\\site-packages (from jinja2->torch) (2.1.3)\n",
      "Requirement already satisfied: mpmath>=0.19 in d:\\python\\python39\\lib\\site-packages (from sympy->torch) (1.3.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "\n",
    "class GPTDatasetV1(Dataset):\n",
    "    def __init__(self, txt, tokenizer, max_length, stride):\n",
    "        self.input_ids = []\n",
    "        self.target_ids = []\n",
    "\n",
    "        # Tokenize the entire text\n",
    "        token_ids = tokenizer.encode(txt, allowed_special={\"<|endoftext|>\"})\n",
    "\n",
    "        # Use a sliding window to chunk the book into overlapping sequences of max_length\n",
    "        for i in range(0, len(token_ids) - max_length, stride):\n",
    "            input_chunk = token_ids[i:i + max_length]\n",
    "            target_chunk = token_ids[i + 1: i + max_length + 1]\n",
    "            self.input_ids.append(torch.tensor(input_chunk))\n",
    "            self.target_ids.append(torch.tensor(target_chunk))\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.input_ids)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.input_ids[idx], self.target_ids[idx]\n",
    "\n",
    "\n",
    "def create_dataloader_v1(txt, batch_size=4, max_length=256, \n",
    "                         stride=128, shuffle=True, drop_last=True,\n",
    "                         num_workers=0):\n",
    "\n",
    "    # Initialize the tokenizer\n",
    "    tokenizer = tiktoken.get_encoding(\"gpt2\")\n",
    "\n",
    "    # Create dataset\n",
    "    dataset = GPTDatasetV1(txt, tokenizer, max_length, stride)\n",
    "\n",
    "    # Create dataloader\n",
    "    dataloader = DataLoader(\n",
    "        dataset,\n",
    "        batch_size=batch_size,\n",
    "        shuffle=shuffle,\n",
    "        drop_last=drop_last,\n",
    "        num_workers=0\n",
    "    )\n",
    "\n",
    "    return dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">> [tensor([[  40,  367, 2885, 1464],\n",
      "        [2885, 1464, 1807, 3619]]), tensor([[ 367, 2885, 1464, 1807],\n",
      "        [1464, 1807, 3619,  402]])]\n"
     ]
    }
   ],
   "source": [
    "# modify: batch_size, max_length, stride\n",
    "# will get different data\n",
    "dataloader = create_dataloader_v1(\n",
    "    raw_text, batch_size=2, max_length=4, stride=2, shuffle=False\n",
    ")\n",
    "\n",
    "data_iter = iter(dataloader)\n",
    "data = next(data_iter)\n",
    "print(f\">> {data}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.6 创建token嵌入\n",
    "\n",
    "这一部分将token id转换为嵌入向量\n",
    "\n",
    "![1716778710812](image/从零开始构建LLM/1716778710812.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">> Parameter containing:\n",
      "tensor([[ 0.3374, -0.1778, -0.1690],\n",
      "        [ 0.9178,  1.5810,  1.3010],\n",
      "        [ 1.2753, -0.2010, -0.1606],\n",
      "        [-0.4015,  0.9666, -1.1481],\n",
      "        [-1.1589,  0.3255, -0.6315],\n",
      "        [-2.8400, -0.7849, -1.4096]], requires_grad=True)\n",
      ">> tensor([[ 1.2753, -0.2010, -0.1606],\n",
      "        [-0.4015,  0.9666, -1.1481],\n",
      "        [-2.8400, -0.7849, -1.4096],\n",
      "        [ 0.9178,  1.5810,  1.3010]], grad_fn=<EmbeddingBackward0>)\n"
     ]
    }
   ],
   "source": [
    "# Simple Example\n",
    "input_ids = torch.tensor([2, 3, 5, 1])\n",
    "\n",
    "vocab_size = 6\n",
    "output_dim = 3\n",
    "\n",
    "torch.manual_seed(123)\n",
    "embedding_layer = torch.nn.Embedding(vocab_size, output_dim)\n",
    "print(f\">> {embedding_layer.weight}\")\n",
    "\n",
    "print(f\">> {embedding_layer(input_ids)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.7 编码位置向量\n",
    "当token id一致时，使用同一个词嵌入会得到相同输出，如下图所示：\n",
    "\n",
    "![1716778847577](image/从零开始构建LLM/1716778847577.png)\n",
    "\n",
    "为了解决这一问题，引入了位置编码，这样可以保证每一个编码是独一无二的\n",
    "\n",
    "![1716778935403](image/从零开始构建LLM/1716778935403.png)\n",
    "\n",
    "最后，所有的数据处理流程如下：\n",
    "\n",
    "![1716778990725](image/从零开始构建LLM/1716778990725.png)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">> Token IDs:\n",
      " tensor([[   40,   367,  2885,  1464],\n",
      "        [ 1807,  3619,   402,   271],\n",
      "        [10899,  2138,   257,  7026],\n",
      "        [15632,   438,  2016,   257],\n",
      "        [  922,  5891,  1576,   438],\n",
      "        [  568,   340,   373,   645],\n",
      "        [ 1049,  5975,   284,   502],\n",
      "        [  284,  3285,   326,    11]])\n",
      ">> Inputs shape: torch.Size([8, 4])\n",
      ">> torch.Size([8, 4, 256])\n",
      ">> pos embeddings's shape: torch.Size([4, 256])\n",
      ">> input embeddings's shape: torch.Size([8, 4, 256])\n"
     ]
    }
   ],
   "source": [
    "vocab_size = 50257\n",
    "output_dim = 256\n",
    "\n",
    "token_embedding_layer = torch.nn.Embedding(vocab_size, output_dim)\n",
    "\n",
    "max_length = 4\n",
    "dataloader = create_dataloader_v1(raw_text, batch_size=8, max_length=max_length, stride=max_length, shuffle=False)\n",
    "\n",
    "data_iter = iter(dataloader)\n",
    "\n",
    "inputs, targets = next(data_iter)\n",
    "print(f\">> Token IDs:\\n {inputs}\")\n",
    "print(f\">> Inputs shape: {inputs.shape}\")\n",
    "\n",
    "token_embeddings = token_embedding_layer(inputs)\n",
    "print(f\">> {token_embeddings.shape}\")\n",
    "# >> (8, 4, 256) -> 8: batch_size, 4: max_length, 256: output_dim\n",
    "\n",
    "context_length = max_length\n",
    "pos_embedding_layer = torch.nn.Embedding(context_length, output_dim)\n",
    "pos_embeddings = pos_embedding_layer(torch.arange(context_length))\n",
    "print(f\">> pos embeddings's shape: {pos_embeddings.shape}\")\n",
    "\n",
    "input_embeddings = token_embeddings + pos_embeddings\n",
    "print(f\">> input embeddings's shape: {input_embeddings.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3 编码注意力机制\n",
    "\n",
    "主要流程如下：\n",
    "1. 一个简单的自注意力\n",
    "2. LLM中使用的注意力机制\n",
    "3. 因果关系的注意力机制\n",
    "4. 多头注意力机制\n",
    "\n",
    "![1716780422961](image/从零开始构建LLM/1716780422961.png)\n",
    "\n",
    "## 3.1 长时序建模的问题\n",
    "\n",
    "主要问题是上下文丢失。如RNN不能在解码阶段直接从编码器中访问早期的隐藏状态。因此，它只依赖于当前的隐藏状态，它封装了所有相关的信息。这可能会导致上下文的丢失，特别是在依赖关系可能跨越较长距离的复杂句子中。\n",
    "\n",
    "## 3.2 使用注意机制捕获数据依赖关系\n",
    "\n",
    "早期为了解决RNN对于长时序问题，研究者提出以下结构，被成为*Bahdanau attention*，这一机制使得解码阶段能够访问编码早期状态。\n",
    "\n",
    "![1718697183686](image/从零开始构建LLM/1718697183686.png)\n",
    "\n",
    "之后根据*Bahdanau attention*得到启发，提出了早期的*Transformer*结构。\n",
    "\n",
    "![1716877433399](image/从零开始构建LLM/1716877433399.png)\n",
    "\n",
    "## 3.3 自注意输入的不同部分\n",
    "\n",
    "自注意力是LLM中Transformer的基石。\n",
    "在自注意力中，“自我”是指该机制通过关联单个输入序列中的不同位置来计算注意权重的能力。它关注的是本身不同部分的关系和依赖。而传统的注意力机制则是关注两个序列之间的关系\n",
    "\n",
    "### 3.3.1 一个简单的自我注意机制，没有训练权重\n",
    "\n",
    "自注意的目标是为每个输入元素计算一个上下文向量，它结合了来自所有其他输入元素的信息。在自注意力中，我们的目标是为每一个输入元素${x^{(i)}}$计算上下文向量${z^{(i)}}$。一个上下文向量可以被解释为一个丰富的嵌入向量。</br>\n",
    "如下图所示，*Your journey starts with one step*为输入句子，现在关注${x^{(2)}}$与${z^{(2)}}$，${z^{(2)}}$包含了从${x^{(1)}}$到${x^{(T)}}$之间的所有信息。\n",
    "在自注意过程中，上下文向量起着至关重要的作用。它们的目的是通过在序列中合并来自所有其他元素的信息，在输入序列中（如句子）中创建每个元素的丰富表示，如下图所示。\n",
    "\n",
    "![1716879045635](image/从零开始构建LLM/1716879045635.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![1716881565809](image/从零开始构建LLM/1716881565809.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">> tensor([0.9544, 1.4950, 1.4754, 0.8434, 0.7070, 1.0865])\n"
     ]
    }
   ],
   "source": [
    "inputs = torch.tensor(\n",
    "  [[0.43, 0.15, 0.89], # Your     (x^1)\n",
    "   [0.55, 0.87, 0.66], # journey  (x^2)\n",
    "   [0.57, 0.85, 0.64], # starts   (x^3)\n",
    "   [0.22, 0.58, 0.33], # with     (x^4)\n",
    "   [0.77, 0.25, 0.10], # one      (x^5)\n",
    "   [0.05, 0.80, 0.55]] # step     (x^6)\n",
    ")\n",
    "\n",
    "query = inputs[1]  # 2nd input token is the query\n",
    "\n",
    "attn_scores_2 = torch.empty(inputs.shape[0])\n",
    "for i, x_i in enumerate(inputs):\n",
    "    attn_scores_2[i] = torch.dot(x_i, query) # dot product (transpose not necessary here since they are 1-dim vectors)\n",
    "\n",
    "print(f\">> {attn_scores_2}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> 上述操作可以理解为矩阵的乘法 dot product，其中值越大，表示相关性越高\n",
    "\n",
    "紧接着需要对其进行归一化操作"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">> attn_scores for x^2: tensor([0.1455, 0.2278, 0.2249, 0.1285, 0.1077, 0.1656])\n",
      ">> attn_scores's sum for x^2: 1.0000001192092896\n"
     ]
    }
   ],
   "source": [
    "attn_scores_2 = attn_scores_2 / attn_scores_2.sum()\n",
    "print(f\">> attn_scores for x^2: {attn_scores_2}\")\n",
    "print(f\">> attn_scores's sum for x^2: {attn_scores_2.sum()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> 在实际中，更多的是使用softmax操作，这一操作在处理极值和梯度时有更好的表现。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">> attn_weights_naive for x^2: tensor([0.1630, 0.1770, 0.1765, 0.1603, 0.1570, 0.1663])\n",
      ">> attn_weights_naive's sum for x^2: 1.0\n",
      "\n",
      ">> attn_weights for x^2: tensor([0.1630, 0.1770, 0.1765, 0.1603, 0.1570, 0.1663])\n",
      ">> attn_weights's sum for x^2: 1.0\n"
     ]
    }
   ],
   "source": [
    "def softmax_naive(x):\n",
    "    return torch.exp(x) / torch.exp(x).sum(dim=0)\n",
    "\n",
    "attn_weights_2_naive = softmax_naive(attn_scores_2)\n",
    "print(f\">> attn_weights_naive for x^2: {attn_weights_2_naive}\")\n",
    "print(f\">> attn_weights_naive's sum for x^2: {attn_weights_2_naive.sum()}\")\n",
    "print()\n",
    "\n",
    "attn_weights_2 = torch.softmax(attn_scores_2, dim=0)\n",
    "print(f\">> attn_weights for x^2: {attn_weights_2}\")\n",
    "print(f\">> attn_weights's sum for x^2: {attn_weights_2.sum()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">> context_vec: tensor([0.4325, 0.5937, 0.5349])\n"
     ]
    }
   ],
   "source": [
    "# Above All\n",
    "query = inputs[1]\n",
    "context_vec_2 = torch.zeros(query.shape)\n",
    "for i, x_i in enumerate(inputs):\n",
    "    context_vec_2 += attn_weights_2[i] * x_i\n",
    "print(f\">> context_vec: {context_vec_2}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3.2 为所有输入计算权重\n",
    "\n",
    "![1716945187106](image/从零开始构建LLM/1716945187106.png)\n",
    "\n",
    "计算流程与之前一致\n",
    "\n",
    "![1716945198064](image/从零开始构建LLM/1716945198064.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">> attn scores: tensor([[0.9995, 0.9544, 0.9422, 0.4753, 0.4576, 0.6310],\n",
      "        [0.9544, 1.4950, 1.4754, 0.8434, 0.7070, 1.0865],\n",
      "        [0.9422, 1.4754, 1.4570, 0.8296, 0.7154, 1.0605],\n",
      "        [0.4753, 0.8434, 0.8296, 0.4937, 0.3474, 0.6565],\n",
      "        [0.4576, 0.7070, 0.7154, 0.3474, 0.6654, 0.2935],\n",
      "        [0.6310, 1.0865, 1.0605, 0.6565, 0.2935, 0.9450]])\n",
      ">> attn scores: tensor([[0.9995, 0.9544, 0.9422, 0.4753, 0.4576, 0.6310],\n",
      "        [0.9544, 1.4950, 1.4754, 0.8434, 0.7070, 1.0865],\n",
      "        [0.9422, 1.4754, 1.4570, 0.8296, 0.7154, 1.0605],\n",
      "        [0.4753, 0.8434, 0.8296, 0.4937, 0.3474, 0.6565],\n",
      "        [0.4576, 0.7070, 0.7154, 0.3474, 0.6654, 0.2935],\n",
      "        [0.6310, 1.0865, 1.0605, 0.6565, 0.2935, 0.9450]])\n",
      ">> attn weights (softmax): tensor([[0.9995, 0.9544, 0.9422, 0.4753, 0.4576, 0.6310],\n",
      "        [0.9544, 1.4950, 1.4754, 0.8434, 0.7070, 1.0865],\n",
      "        [0.9422, 1.4754, 1.4570, 0.8296, 0.7154, 1.0605],\n",
      "        [0.4753, 0.8434, 0.8296, 0.4937, 0.3474, 0.6565],\n",
      "        [0.4576, 0.7070, 0.7154, 0.3474, 0.6654, 0.2935],\n",
      "        [0.6310, 1.0865, 1.0605, 0.6565, 0.2935, 0.9450]])\n"
     ]
    }
   ],
   "source": [
    "# >> attention scores\n",
    "# method 1\n",
    "attn_scores = torch.empty(6, 6)\n",
    "for i, x_i in enumerate(inputs):\n",
    "    for j, x_j in enumerate(inputs):\n",
    "        attn_scores[i, j] = torch.dot(x_i, x_j)\n",
    "print(f\">> attn scores: {attn_scores}\")\n",
    "\n",
    "# method 2\n",
    "attn_scores = torch.matmul(inputs, inputs.T)\n",
    "print(f\">> attn scores: {attn_scores}\")\n",
    "\n",
    "# >> attention weights (softmax)\n",
    "attn_weights = torch.softmax(attn_scores, dim=1)\n",
    "print(f\">> attn weights (softmax): {attn_scores}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.4 使用训练权重实现自注意力\n",
    "\n",
    "### 3.4.1 一步一步计算注意力权重\n",
    "\n",
    "这里引入了三个权重${W_q}$, ${W_k}$, ${W_v}$，这三个权重矩阵用于将输入token ${x^i}$ 映射为查询，键， 值向量。\n",
    "\n",
    "![1716947702233](image/从零开始构建LLM/1716947702233.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(1.4676)\n"
     ]
    }
   ],
   "source": [
    "x_2 = inputs[1]\n",
    "d_in = inputs.shape[1]\n",
    "d_out = 2\n",
    "\n",
    "# requires_grad=False to reduce clutter in the outputs for illustration purposes\n",
    "W_query = torch.nn.Parameter(torch.rand(d_in, d_out), requires_grad=False)\n",
    "W_key = torch.nn.Parameter(torch.rand(d_in, d_out), requires_grad=False)\n",
    "W_value = torch.nn.Parameter(torch.rand(d_in, d_out), requires_grad=False)\n",
    "\n",
    "query_2 = torch.matmul(x_2, W_query)\n",
    "key_2 = torch.matmul(x_2, W_key)\n",
    "value_2 = torch.matmul(x_2, W_value)\n",
    "\n",
    "attn_scores_22 = torch.dot(query_2, key_2)\n",
    "print(attn_scores_22)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> 权重与注意力权重的区别：</br>\n",
    "权重 ${W}$ 是指神经网络中的权重，在训练过程中被优化的部分。<br/>\n",
    "注意权重决定了上下文向量依赖于输入的不同部分的程度。<br/>\n",
    "<br/>\n",
    "总的来说，权重参数是定义神经网络的基础的、可学习的参数，而注意里权重是上下文特定的、动态的值。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">> keys shape: torch.Size([6, 2])\n",
      ">> values shape: torch.Size([6, 2])\n",
      ">> attn weights for x_2: tensor([0.1545, 0.2136, 0.2123, 0.1320, 0.1419, 0.1457])\n",
      ">> context vector for x_2: tensor([0.3341, 1.0655])\n"
     ]
    }
   ],
   "source": [
    "keys = torch.matmul(inputs, W_key)\n",
    "values = torch.matmul(inputs, W_value)\n",
    "\n",
    "print(f\">> keys shape: {keys.shape}\")\n",
    "print(f\">> values shape: {values.shape}\")\n",
    "\n",
    "attn_scores_2 = torch.matmul(query_2, keys.T)\n",
    "\n",
    "d_k = keys.shape[-1]\n",
    "attn_weights_2 = torch.softmax(attn_scores_2 / d_k ** 0.5, dim=-1)\n",
    "print(f\">> attn weights for x_2: {attn_weights_2}\")\n",
    "\n",
    "context_vec_2 = torch.matmul(attn_weights_2, values)\n",
    "print(f\">> context vector for x_2: {context_vec_2}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.4.2 实现一个紧凑的自注意类"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "\n",
    "class SelfAttention_v1(nn.Module):\n",
    "    def __init__(self, d_in, d_out):\n",
    "        super().__init__()\n",
    "        self.d_out = d_out\n",
    "        self.W_query = nn.Parameter(torch.rand(d_in, d_out))\n",
    "        self.W_key = nn.Parameter(torch.rand(d_in, d_out))\n",
    "        self.W_value = nn.Parameter(torch.rand(d_in, d_out))\n",
    "  \n",
    "    def forward(self, x):\n",
    "        keys = torch.matmul(x, self.W_key)\n",
    "        values = torch.matmul(x, self.W_value)\n",
    "        queries = torch.matmul(x, self.W_query)\n",
    "\n",
    "        attn_scores = torch.matmul(queries, keys.T)\n",
    "        attn_weights = torch.softmax(attn_scores / keys.shape[-1] ** 0.5, dim=-1)\n",
    "        context_vec = torch.matmul(attn_weights, values)\n",
    "        return context_vec"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![1717397936262](image/从零开始构建LLM/1717397936262.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">> context: tensor([[0.2996, 0.8053],\n",
      "        [0.3061, 0.8210],\n",
      "        [0.3058, 0.8203],\n",
      "        [0.2948, 0.7939],\n",
      "        [0.2927, 0.7891],\n",
      "        [0.2990, 0.8040]], grad_fn=<MmBackward0>)\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(123)\n",
    "sa_v1 = SelfAttention_v1(d_in, d_out)\n",
    "print(f\">> context: {sa_v1(inputs)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "使用 `nn.Linear` ，除了可以有效计算矩阵外，它还优化了权值初始化方案，有助于模型训练更加稳定和有效"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SelfAttention_v2(nn.Module):\n",
    "    def __init__(self, d_in, d_out, qkv_bias=False):\n",
    "        super().__init__()\n",
    "        self.d_out = d_out\n",
    "        self.W_query = nn.Linear(d_in, d_out, bias=qkv_bias)\n",
    "        self.W_key = nn.Linear(d_in, d_out, bias=qkv_bias)\n",
    "        self.W_value = nn.Linear(d_in, d_out, bias=qkv_bias)\n",
    "  \n",
    "    def forward(self, x):\n",
    "        keys = self.W_key(x)\n",
    "        queries = self.W_query(x)\n",
    "        values = self.W_value(x)\n",
    "\n",
    "        attn_scores = torch.matmul(queries, keys.T)\n",
    "        attn_weights = torch.softmax(attn_scores / keys.shape[-1] ** 0.5, dim=-1)\n",
    "        context_vec = torch.matmul(attn_weights, values)\n",
    "        return context_vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">> context: tensor([[-0.0739,  0.0713],\n",
      "        [-0.0748,  0.0703],\n",
      "        [-0.0749,  0.0702],\n",
      "        [-0.0760,  0.0685],\n",
      "        [-0.0763,  0.0679],\n",
      "        [-0.0754,  0.0693]], grad_fn=<MmBackward0>)\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(789)\n",
    "sa_v2 = SelfAttention_v2(d_in, d_out)\n",
    "print(f\">> context: {sa_v2(inputs)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.5 用因果关系的注意力来隐藏未来的词语\n",
    "\n",
    "![1717401849186](image/从零开始构建LLM/1717401849186.png)\n",
    "\n",
    "### 3.5.1 应用因果注意力掩码\n",
    "\n",
    "![1717401901352](image/从零开始构建LLM/1717401901352.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">> attn weights: tensor([[0.1921, 0.1646, 0.1652, 0.1550, 0.1721, 0.1510],\n",
      "        [0.2041, 0.1659, 0.1662, 0.1496, 0.1665, 0.1477],\n",
      "        [0.2036, 0.1659, 0.1662, 0.1498, 0.1664, 0.1480],\n",
      "        [0.1869, 0.1667, 0.1668, 0.1571, 0.1661, 0.1564],\n",
      "        [0.1830, 0.1669, 0.1670, 0.1588, 0.1658, 0.1585],\n",
      "        [0.1935, 0.1663, 0.1666, 0.1542, 0.1666, 0.1529]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      ">> mask:  tensor([[1., 0., 0., 0., 0., 0.],\n",
      "        [1., 1., 0., 0., 0., 0.],\n",
      "        [1., 1., 1., 0., 0., 0.],\n",
      "        [1., 1., 1., 1., 0., 0.],\n",
      "        [1., 1., 1., 1., 1., 0.],\n",
      "        [1., 1., 1., 1., 1., 1.]])\n",
      ">> masked:  tensor([[0.1921, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.2041, 0.1659, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.2036, 0.1659, 0.1662, 0.0000, 0.0000, 0.0000],\n",
      "        [0.1869, 0.1667, 0.1668, 0.1571, 0.0000, 0.0000],\n",
      "        [0.1830, 0.1669, 0.1670, 0.1588, 0.1658, 0.0000],\n",
      "        [0.1935, 0.1663, 0.1666, 0.1542, 0.1666, 0.1529]],\n",
      "       grad_fn=<MulBackward0>)\n",
      ">> masked norm:  tensor([[1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.5517, 0.4483, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.3800, 0.3097, 0.3103, 0.0000, 0.0000, 0.0000],\n",
      "        [0.2758, 0.2460, 0.2462, 0.2319, 0.0000, 0.0000],\n",
      "        [0.2175, 0.1983, 0.1984, 0.1888, 0.1971, 0.0000],\n",
      "        [0.1935, 0.1663, 0.1666, 0.1542, 0.1666, 0.1529]],\n",
      "       grad_fn=<DivBackward0>)\n"
     ]
    }
   ],
   "source": [
    "queries = sa_v2.W_query(inputs)\n",
    "keys = sa_v2.W_key(inputs)\n",
    "\n",
    "attn_scores = torch.matmul(queries, keys.T)\n",
    "attn_weights = torch.softmax(attn_scores / keys.shape[-1] ** 0.5, dim=-1)\n",
    "print(\">> attn weights:\", attn_weights)\n",
    "\n",
    "context_length = attn_scores.shape[0]\n",
    "mask_simple = torch.tril(torch.ones(context_length, context_length))\n",
    "print(\">> mask: \", mask_simple)\n",
    "\n",
    "masked_simple = attn_weights * mask_simple\n",
    "print(\">> masked: \", masked_simple)\n",
    "\n",
    "row_sums = torch.sum(masked_simple, dim=-1, keepdim=True)\n",
    "masked_simple_norm = masked_simple / row_sums\n",
    "print(\">> masked norm: \", masked_simple_norm)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **信息泄露**</br>\n",
    "当应用掩码时，由于计算的权重已经进行了softmax，因此会有影响。然而，当我们在mask之后重新调整注意力权重时，本质是在一个更小的子集上重新计算softmax，因此mask位置对于softmax没有贡献。</br>\n",
    "\n",
    "因此可以将流程简化为：\n",
    "\n",
    "![1717402483387](image/从零开始构建LLM/1717402483387.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">> mask:  tensor([[0., 1., 1., 1., 1., 1.],\n",
      "        [0., 0., 1., 1., 1., 1.],\n",
      "        [0., 0., 0., 1., 1., 1.],\n",
      "        [0., 0., 0., 0., 1., 1.],\n",
      "        [0., 0., 0., 0., 0., 1.],\n",
      "        [0., 0., 0., 0., 0., 0.]])\n",
      ">> masked:  tensor([[0.2899,   -inf,   -inf,   -inf,   -inf,   -inf],\n",
      "        [0.4656, 0.1723,   -inf,   -inf,   -inf,   -inf],\n",
      "        [0.4594, 0.1703, 0.1731,   -inf,   -inf,   -inf],\n",
      "        [0.2642, 0.1024, 0.1036, 0.0186,   -inf,   -inf],\n",
      "        [0.2183, 0.0874, 0.0882, 0.0177, 0.0786,   -inf],\n",
      "        [0.3408, 0.1270, 0.1290, 0.0198, 0.1290, 0.0078]],\n",
      "       grad_fn=<MaskedFillBackward0>)\n",
      ">> attn weights:  tensor([[1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.5517, 0.4483, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.3800, 0.3097, 0.3103, 0.0000, 0.0000, 0.0000],\n",
      "        [0.2758, 0.2460, 0.2462, 0.2319, 0.0000, 0.0000],\n",
      "        [0.2175, 0.1983, 0.1984, 0.1888, 0.1971, 0.0000],\n",
      "        [0.1935, 0.1663, 0.1666, 0.1542, 0.1666, 0.1529]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n"
     ]
    }
   ],
   "source": [
    "mask = torch.triu(torch.ones(context_length, context_length), diagonal=1)\n",
    "print(\">> mask: \", mask)\n",
    "\n",
    "masked = torch.masked_fill(attn_scores, mask.bool(), -torch.inf)\n",
    "print(\">> masked: \", masked)\n",
    "\n",
    "attn_weights = torch.softmax(masked / keys.shape[-1] ** 0.5, dim=1)\n",
    "print(\">> attn weights: \", attn_weights)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.5.2 用dropout来掩盖额外的注意权重\n",
    "\n",
    "在transformer架构中，dropout通常用在两个地方：计算注意力分数之后或者应用注意力权重之前\n",
    "\n",
    "![1717558177482](image/从零开始构建LLM/1717558177482.png)\n",
    "\n",
    "需要注意的是，dropout时，会将原数值进行放大，这样能够保证注意力权重的平衡。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">> dropout rate (0.5):  tensor([[2., 2., 2., 2., 2., 2.],\n",
      "        [0., 2., 0., 0., 0., 0.],\n",
      "        [0., 0., 2., 0., 2., 0.],\n",
      "        [2., 2., 0., 0., 0., 2.],\n",
      "        [2., 0., 0., 0., 0., 2.],\n",
      "        [0., 2., 0., 0., 0., 0.]])\n",
      ">> dropout attn weights:  tensor([[2.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.0000, 0.0000, 0.6206, 0.0000, 0.0000, 0.0000],\n",
      "        [0.0000, 0.4921, 0.0000, 0.4638, 0.0000, 0.0000],\n",
      "        [0.0000, 0.3966, 0.3968, 0.3775, 0.3941, 0.0000],\n",
      "        [0.3869, 0.3327, 0.0000, 0.0000, 0.3331, 0.3058]],\n",
      "       grad_fn=<MulBackward0>)\n",
      ">> dropout rate (0.1):  tensor([[1.1111, 1.1111, 1.1111, 1.1111, 1.1111, 1.1111],\n",
      "        [1.1111, 1.1111, 1.1111, 1.1111, 1.1111, 0.0000],\n",
      "        [0.0000, 1.1111, 1.1111, 1.1111, 1.1111, 1.1111],\n",
      "        [1.1111, 1.1111, 1.1111, 0.0000, 1.1111, 1.1111],\n",
      "        [1.1111, 1.1111, 0.0000, 1.1111, 1.1111, 1.1111],\n",
      "        [1.1111, 1.1111, 1.1111, 1.1111, 1.1111, 1.1111]])\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(123)\n",
    "dropout = torch.nn.Dropout(0.5)\n",
    "example = torch.ones(6, 6)\n",
    "print(\">> dropout rate (0.5): \", dropout(example))\n",
    "\n",
    "print(\">> dropout attn weights: \", dropout(attn_weights))\n",
    "\n",
    "dropout = torch.nn.Dropout(0.1)\n",
    "example = torch.ones(6, 6)\n",
    "print(\">> dropout rate (0.1): \", dropout(example))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.5.3 实现一个紧凑的因果注意力类"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CausalAttention(nn.Module):\n",
    "    def __init__(self, d_in, d_out, context_length, dropout, qkv_bias=False):\n",
    "        super().__init__()\n",
    "        self.d_out = d_out\n",
    "        self.W_query = nn.Linear(d_in, d_out, bias=qkv_bias)\n",
    "        self.W_key = nn.Linear(d_in, d_out, bias=qkv_bias)\n",
    "        self.W_value = nn.Linear(d_in, d_out, bias=qkv_bias)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.register_buffer('mask', torch.triu(torch.ones(context_length, context_length), diagonal=1))\n",
    "\n",
    "    def forward(self, x):\n",
    "        b, num_tokens, d_in = x.shape\n",
    "        keys = self.W_key(x)\n",
    "        queries = self.W_query(x)\n",
    "        values = self.W_value(x)\n",
    "\n",
    "        attn_scores = torch.matmul(queries, keys.transpose(1, 2))\n",
    "        attn_scores.masked_fill_(self.mask.bool()[:num_tokens, :num_tokens], -torch.inf)\n",
    "        attn_weights = torch.softmax(attn_scores / keys.shape[-1] ** 0.5, dim=-1)\n",
    "        attn_weights = self.dropout(attn_weights)\n",
    "        context_vec = torch.matmul(attn_weights, values)\n",
    "        return context_vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">> batch shape:  torch.Size([2, 6, 3])\n",
      ">> context_vecs:  tensor([[[-0.4519,  0.2216],\n",
      "         [-0.5874,  0.0058],\n",
      "         [-0.6300, -0.0632],\n",
      "         [-0.5675, -0.0843],\n",
      "         [-0.5526, -0.0981],\n",
      "         [-0.5299, -0.1081]],\n",
      "\n",
      "        [[-0.4519,  0.2216],\n",
      "         [-0.5874,  0.0058],\n",
      "         [-0.6300, -0.0632],\n",
      "         [-0.5675, -0.0843],\n",
      "         [-0.5526, -0.0981],\n",
      "         [-0.5299, -0.1081]]], grad_fn=<UnsafeViewBackward0>)\n",
      ">> context_vecs.shape: torch.Size([2, 6, 2])\n"
     ]
    }
   ],
   "source": [
    "batch = torch.stack((inputs, inputs), dim=0)\n",
    "print(\">> batch shape: \", batch.shape) # 2 inputs with 6 tokens each, and each token has embedding dimension 3\n",
    "\n",
    "torch.manual_seed(123)\n",
    "\n",
    "context_length = batch.shape[1]\n",
    "ca = CausalAttention(d_in, d_out, context_length, 0.0)\n",
    "\n",
    "context_vecs = ca(batch)\n",
    "\n",
    "print(\">> context_vecs: \", context_vecs)\n",
    "print(\">> context_vecs.shape:\", context_vecs.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.6 将单个注意力扩展到多头注意力\n",
    "\n",
    "### 3.6.1 将多头注意力扩展到多头注意力\n",
    "\n",
    "![1717567513041](image/从零开始构建LLM/1717567513041.png)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadAttentionWrapper(nn.Module):\n",
    "\n",
    "    def __init__(self, d_in, d_out, context_length, dropout, num_heads, qkv_bias=False):\n",
    "        super().__init__()\n",
    "        self.heads = nn.ModuleList(\n",
    "            [CausalAttention(d_in, d_out, context_length, dropout, qkv_bias) \n",
    "             for _ in range(num_heads)]\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return torch.cat([head(x) for head in self.heads], dim=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">> context_vecs: tensor([[[-0.4519,  0.2216,  0.4772,  0.1063],\n",
      "         [-0.5874,  0.0058,  0.5891,  0.3257],\n",
      "         [-0.6300, -0.0632,  0.6202,  0.3860],\n",
      "         [-0.5675, -0.0843,  0.5478,  0.3589],\n",
      "         [-0.5526, -0.0981,  0.5321,  0.3428],\n",
      "         [-0.5299, -0.1081,  0.5077,  0.3493]],\n",
      "\n",
      "        [[-0.4519,  0.2216,  0.4772,  0.1063],\n",
      "         [-0.5874,  0.0058,  0.5891,  0.3257],\n",
      "         [-0.6300, -0.0632,  0.6202,  0.3860],\n",
      "         [-0.5675, -0.0843,  0.5478,  0.3589],\n",
      "         [-0.5526, -0.0981,  0.5321,  0.3428],\n",
      "         [-0.5299, -0.1081,  0.5077,  0.3493]]], grad_fn=<CatBackward0>)\n",
      ">> context_vecs.shape: torch.Size([2, 6, 4])\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(123)\n",
    "\n",
    "context_length = batch.shape[1] # This is the number of tokens\n",
    "d_in, d_out = 3, 2\n",
    "mha = MultiHeadAttentionWrapper(\n",
    "    d_in, d_out, context_length, 0.0, num_heads=2\n",
    ")\n",
    "\n",
    "context_vecs = mha(batch)\n",
    "\n",
    "print(\">> context_vecs:\", context_vecs)\n",
    "print(\">> context_vecs.shape:\", context_vecs.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.6.2 通过权重分割实现多头注意力"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(self, d_in, d_out, context_length, dropout, num_heads, qkv_bias=False) -> None:\n",
    "        super().__init__()\n",
    "        assert (d_out % num_heads == 0), \"d_out must be divisible by num_heads\"\n",
    "\n",
    "        self.d_out = d_out\n",
    "        self.num_heads = num_heads\n",
    "        self.head_dim = d_out // num_heads # Reduce the projection dim to match desired output dim\n",
    "\n",
    "        self.W_query = nn.Linear(d_in, d_out, bias=qkv_bias)\n",
    "        self.W_key = nn.Linear(d_in, d_out, bias=qkv_bias)\n",
    "        self.W_value = nn.Linear(d_in, d_out, bias=qkv_bias)\n",
    "        self.out_proj = nn.Linear(d_out, d_out)  # Linear layer to combine head outputs\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.register_buffer(\n",
    "            \"mask\",\n",
    "            torch.triu(torch.ones(context_length, context_length),\n",
    "                       diagonal=1)\n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        b, num_tokens, d_in = x.shape\n",
    "\n",
    "        keys = self.W_key(x) # Shape: (b, num_tokens, d_out)\n",
    "        queries = self.W_query(x)\n",
    "        values = self.W_value(x)\n",
    "\n",
    "        keys = keys.view(b, num_tokens, self.num_heads, self.head_dim)\n",
    "        queries = queries.view(b, num_tokens, self.num_heads, self.head_dim)\n",
    "        values = values.view(b, num_tokens, self.num_heads, self.head_dim)\n",
    "\n",
    "        keys = keys.transpose(1, 2)\n",
    "        queries = queries.transpose(1, 2)\n",
    "        values = values.transpose(1, 2)\n",
    "\n",
    "        attn_scores = torch.matmul(queries, keys.transpose(2, 3))\n",
    "\n",
    "        mask_bool = self.mask.bool()[:num_tokens, :num_tokens]\n",
    "\n",
    "        attn_scores.masked_fill_(mask_bool, -torch.inf)\n",
    "\n",
    "        attn_weights = torch.softmax(attn_scores / keys.shape[-1]**0.5, dim=-1)\n",
    "        attn_weights = self.dropout(attn_weights)\n",
    "        \n",
    "        context_vec = torch.matmul(attn_weights, values).transpose(1, 2)\n",
    "\n",
    "        context_vec = context_vec.contiguous().view(b, num_tokens, self.d_out)\n",
    "        context_vec = self.out_proj(context_vec)\n",
    "\n",
    "        return context_vec"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![1718186662363](image/从零开始构建LLM/1718186662363.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">> context_vecs: tensor([[[0.3190, 0.4858],\n",
      "         [0.2943, 0.3897],\n",
      "         [0.2856, 0.3593],\n",
      "         [0.2693, 0.3873],\n",
      "         [0.2639, 0.3928],\n",
      "         [0.2575, 0.4028]],\n",
      "\n",
      "        [[0.3190, 0.4858],\n",
      "         [0.2943, 0.3897],\n",
      "         [0.2856, 0.3593],\n",
      "         [0.2693, 0.3873],\n",
      "         [0.2639, 0.3928],\n",
      "         [0.2575, 0.4028]]], grad_fn=<ViewBackward0>)\n",
      ">> context_vecs.shape: torch.Size([2, 6, 2])\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(123)\n",
    "\n",
    "batch_size, context_length, d_in = batch.shape\n",
    "d_out = 2\n",
    "mha = MultiHeadAttention(d_in, d_out, context_length, 0.0, num_heads=2)\n",
    "\n",
    "context_vecs = mha(batch)\n",
    "\n",
    "print(\">> context_vecs:\", context_vecs)\n",
    "print(\">> context_vecs.shape:\", context_vecs.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4 实现GPT并生成文本\n",
    "\n",
    "## 4.1 实现一个LLM结构\n",
    "\n",
    "LLM总体框架图如下：\n",
    "1. 词嵌入\n",
    "2. 多头注意力\n",
    "3. 输出层\n",
    "\n",
    "![1718247386638](image/从零开始构建LLM/1718247386638.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# GPT-2 parameter\n",
    "GPT_CONFIG_124M = {\n",
    "    \"vocab_size\": 50257,    # Vocabulary size\n",
    "    \"context_length\": 1024, # Context length\n",
    "    \"emb_dim\": 768,         # Embedding dimension\n",
    "    \"n_heads\": 12,          # Number of attention heads\n",
    "    \"n_layers\": 12,         # Number of layers\n",
    "    \"drop_rate\": 0.1,       # Dropout rate\n",
    "    \"qkv_bias\": False       # Query-Key-Value bias\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> 根据下图，一步一步编写GPT模型\n",
    "\n",
    "![1718258629553](image/从零开始构建LLM/1718258629553.png)\n",
    "\n",
    "编写代码如下所示，但是并没有编写归一化与具体的Transformer块。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DummyGPTModel(nn.Module):\n",
    "    def __init__(self, cfg):\n",
    "        super().__init__()\n",
    "        self.tok_emb = nn.Embedding(cfg[\"vocab_size\"], cfg[\"emb_dim\"])\n",
    "        self.pos_emb = nn.Embedding(cfg[\"context_length\"], cfg[\"emb_dim\"])\n",
    "        self.drop_emb = nn.Dropout(cfg[\"drop_rate\"])\n",
    "\n",
    "        self.trf_blocks = nn.Sequential(*[DummyTransformerBlock(cfg) for _ in range(cfg[\"n_layers\"])])\n",
    "\n",
    "        self.final_norm = DummyLayerNorm(cfg[\"emb_dim\"])\n",
    "        self.out_head = nn.Linear(cfg[\"emb_dim\"], cfg[\"vocab_size\"], bias=False)\n",
    "  \n",
    "    def forward(self, in_idx):\n",
    "        batch_size, seq_len = in_idx.shape\n",
    "        tok_embeds = self.tok_emb(in_idx)\n",
    "        pos_embeds = self.pos_emb(torch.arange(seq_len, device=in_idx.device))\n",
    "        x = tok_embeds + pos_embeds\n",
    "        x = self.trf_blocks(x)\n",
    "        x = self.final_norm(x)\n",
    "        logits = self.out_head(x)\n",
    "        return logits\n",
    "\n",
    "\n",
    "class DummyTransformerBlock(nn.Module):\n",
    "    def __init__(self, cfg):\n",
    "        super().__init__()\n",
    "  \n",
    "    def forward(self, x):\n",
    "        return x\n",
    "\n",
    "\n",
    "class DummyLayerNorm(nn.Module):\n",
    "    def __init__(self, normalized_shape, eps=1e-5) -> None:\n",
    "        super().__init__()\n",
    "  \n",
    "    def forward(self, x):\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> GPT数据流\n",
    "\n",
    "![1718262314570](image/从零开始构建LLM/1718262314570.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">> batch:  tensor([[6109, 3626, 6100,  345],\n",
      "        [6109, 1110, 6622,  257]])\n",
      ">> out shape:  torch.Size([2, 4, 50257])\n",
      ">> out:  tensor([[[-1.1947,  0.1392, -0.8616,  ..., -1.4987, -0.0314, -0.4490],\n",
      "         [ 0.0497,  0.3861, -0.3281,  ..., -0.1826,  1.3084,  0.9867],\n",
      "         [ 0.7005,  1.4747, -0.4149,  ...,  1.7756, -0.2280,  0.5384],\n",
      "         [ 0.4885,  1.7545, -0.6707,  ...,  1.1501, -0.1143, -0.9368]],\n",
      "\n",
      "        [[-1.1947,  0.1392, -0.8616,  ..., -1.4987, -0.0314, -0.4490],\n",
      "         [-0.5591,  0.5797, -0.1296,  ...,  0.2691,  0.3151,  1.4046],\n",
      "         [ 0.8524,  1.2833, -0.1786,  ..., -0.1982,  0.1097,  0.2812],\n",
      "         [-0.0190, -0.8277,  0.2299,  ...,  1.7974, -0.1646, -0.1049]]],\n",
      "       grad_fn=<UnsafeViewBackward0>)\n"
     ]
    }
   ],
   "source": [
    "import tiktoken\n",
    "tokenizer = tiktoken.get_encoding(\"gpt2\")\n",
    "batch = []\n",
    "txt1 = \"Every effort moves you\"\n",
    "txt2 = \"Every day holds a\"\n",
    "\n",
    "batch.append(torch.tensor(tokenizer.encode(txt1)))\n",
    "batch.append(torch.tensor(tokenizer.encode(txt2)))\n",
    "batch = torch.stack(batch, dim=0)\n",
    "print(\">> batch: \", batch)\n",
    "\n",
    "torch.manual_seed(123)\n",
    "model = DummyGPTModel(GPT_CONFIG_124M)\n",
    "logits = model(batch)\n",
    "print(\">> out shape: \", logits.shape)\n",
    "print(\">> out: \", logits)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.2 使用layer normalization进行归一化激活\n",
    "\n",
    "> 由于梯度消失或爆炸等问题，训练多层深度神经网络有时会具有挑战性。这些问题导致了不稳定的训练动态，使网络难以有效地调整其权值，这意味着学习过程很难为神经网络找到一组参数（权值），以最小化损失函数。换句话说，该网络很难学习数据中的潜在模式，其程度将使其能够做出准确的预测或决策。</br>\n",
    "\n",
    "> 层归一化背后的主要思想是调整神经网络层的激活（输出），使其均值为0，方差为1，也称为单位方差。这种调整加速了收敛到有效的权重，并确保了一致、可靠的训练。</br>\n",
    "\n",
    "> **层归一化通常在多头注意模块前后和最终输出层之前应用。**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# torch.set_printoptions(sci_mode=True)  # set float number\n",
    "\n",
    "class LayerNorm(nn.Module):\n",
    "    def __init__(self, emb_dim) -> None:\n",
    "        super().__init__()\n",
    "        self.eps = 1e-5\n",
    "        self.scale = nn.Parameter(torch.ones(emb_dim))\n",
    "        self.shift = nn.Parameter(torch.zeros(emb_dim))\n",
    "  \n",
    "    def forward(self, x):\n",
    "        mean = x.mean(dim=-1, keepdim=True)\n",
    "        var = x.var(dim=-1, keepdim=True, unbiased=False)\n",
    "        norm_x = (x - mean) / torch.sqrt(var + self.eps)\n",
    "        return self.scale * norm_x + self.shift"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.3 使用层归一化实现前向神经网络\n",
    "\n",
    "**GeLU**激活函数\n",
    "\n",
    "在神经网络中，使用最广泛的是ReLU函数，但是在LLM，除了ReLU外，还有两种显著的激活函数：GELU (Gaussian Error Linear Unit) 和 SwiGLU (Sigmoid-Weighted Linear Unit)。GELU和SwiGLU分别是更复杂和光滑的包含高斯单元和s型门控线性单位的激活函数。他们可以表现的更好。\n",
    "\n",
    "$$\n",
    "\\text{GELU}(x) \\approx 0.5 \\cdot x \\cdot \\left(1 + \\tanh\\left[\\sqrt{\\frac{2}{\\pi}} \\cdot \\left(x + 0.044715 \\cdot x^3\\right)\\right]\\right)\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GELU(nn.Module):\n",
    "    def __init__(self) -> None:\n",
    "        super().__init__()\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return 0.5 * x * ( 1 + torch.tanh(torch.sqrt(torch.tensor(2 / torch.pi)) * (x + 0.044715 * torch.pow(x, 3))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 800x300 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# GELU Test\n",
    "import matplotlib.pyplot as plt\n",
    "gelu, relu = GELU(), nn.ReLU()\n",
    "x = torch.linspace(-3, 3, 100)\n",
    "y_gelu, y_relu = gelu(x), relu(x)\n",
    "plt.figure(figsize=(8, 3))\n",
    "for i, (y, label) in enumerate(zip([y_gelu, y_relu], ['GELU', \"ReLU\"])):\n",
    "    plt.subplot(1, 2, i+1)\n",
    "    plt.plot(x, y)\n",
    "    plt.title(f\"{label} activation function\")\n",
    "    plt.xlabel('x')\n",
    "    plt.ylabel(f\"{label}(x)\")\n",
    "    plt.grid()\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> 在此基础上实现一个前馈网络，这个前馈网络至关重要，主要解决非线性问题，并且可以探索更丰富的空间。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FeedForward(nn.Module):\n",
    "    def __init__(self, cfg):\n",
    "        super().__init__()\n",
    "        self.layer = nn.Sequential(\n",
    "            nn.Linear(cfg[\"emb_dim\"], 4 * cfg[\"emb_dim\"]),\n",
    "            GELU(),\n",
    "            nn.Linear(4 * cfg[\"emb_dim\"], cfg[\"emb_dim\"])\n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.layer(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">> ffn out size:  torch.Size([2, 3, 768])\n"
     ]
    }
   ],
   "source": [
    "ffn = FeedForward(GPT_CONFIG_124M)\n",
    "x = torch.rand(2, 3, 768)\n",
    "out = ffn(x)\n",
    "print(\">> ffn out size: \", out.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.4 增加短连接\n",
    "\n",
    "short connection通过跳过一个或多个层来创建一个梯度通过网络的更短路径，这是通过将一个层的输出添加到后面一个层的输出来实现的。这就是为什么这些连接也被称为跳过连接。在训练过程中，它们在保持梯度的流动方面起着至关重要的作用。\n",
    "\n",
    "![1718273582034](image/从零开始构建LLM/1718273582034.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ExampleDeepNeuralNetwork(nn.Module):\n",
    "    def __init__(self, layer_sizes, use_shortcut) -> None:\n",
    "        super().__init__()\n",
    "        self.use_shortcut = use_shortcut\n",
    "        self.layers = nn.ModuleList([\n",
    "            nn.Sequential(nn.Linear(layer_sizes[0], layer_sizes[1]), GELU()),\n",
    "            nn.Sequential(nn.Linear(layer_sizes[1], layer_sizes[2]), GELU()),\n",
    "            nn.Sequential(nn.Linear(layer_sizes[2], layer_sizes[3]), GELU()),\n",
    "            nn.Sequential(nn.Linear(layer_sizes[3], layer_sizes[4]), GELU()),\n",
    "            nn.Sequential(nn.Linear(layer_sizes[4], layer_sizes[5]), GELU()),\n",
    "        ])\n",
    "    \n",
    "    def forward(self, x):\n",
    "        for layer in self.layers:\n",
    "            layer_output = layer(x)\n",
    "            if self.use_shortcut and x.shape == layer_output.shape:\n",
    "                x = x + layer_output\n",
    "            else:\n",
    "                x = layer_output\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "layer_sizes = [3, 3, 3, 3, 3, 1]\n",
    "sample_input = torch.tensor([[1., 0., -1.]])\n",
    "model_without_shorcut = ExampleDeepNeuralNetwork(layer_sizes, False)\n",
    "\n",
    "def print_gradients(model, x):\n",
    "    output = model(x)\n",
    "    target = torch.tensor([[0.]])\n",
    "\n",
    "    loss = nn.MSELoss()\n",
    "    loss = loss(output, target)\n",
    "    loss.backward()\n",
    "\n",
    "    for name, param in model.named_parameters():\n",
    "        if 'weight' in name:\n",
    "            print(f\">> {name} has gradient mean of {param.grad.abs()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">> layers.0.0.weight has gradient mean of tensor([[0.0018, 0.0000, 0.0018],\n",
      "        [0.0010, 0.0000, 0.0010],\n",
      "        [0.0065, 0.0000, 0.0065]])\n",
      ">> layers.1.0.weight has gradient mean of tensor([[6.3482e-07, 3.8762e-07, 7.6243e-06],\n",
      "        [1.5987e-04, 9.7618e-05, 1.9201e-03],\n",
      "        [8.4790e-05, 5.1773e-05, 1.0183e-03]])\n",
      ">> layers.2.0.weight has gradient mean of tensor([[0.0039, 0.0031, 0.0015],\n",
      "        [0.0041, 0.0033, 0.0015],\n",
      "        [0.0050, 0.0040, 0.0019]])\n",
      ">> layers.3.0.weight has gradient mean of tensor([[0.0491, 0.0031, 0.0283],\n",
      "        [0.0257, 0.0016, 0.0148],\n",
      "        [0.0465, 0.0029, 0.0268]])\n",
      ">> layers.4.0.weight has gradient mean of tensor([[0.0417, 0.0856, 0.0110]])\n"
     ]
    }
   ],
   "source": [
    "print_gradients(model_without_shorcut, sample_input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">> layers.0.0.weight has gradient mean of tensor([[3.2044e-06, 0.0000e+00, 3.2044e-06],\n",
      "        [1.8830e-03, 0.0000e+00, 1.8830e-03],\n",
      "        [3.0898e-03, 0.0000e+00, 3.0898e-03]])\n",
      ">> layers.1.0.weight has gradient mean of tensor([[1.5119e-04, 1.8919e-05, 1.4878e-04],\n",
      "        [8.6263e-04, 1.0795e-04, 8.4891e-04],\n",
      "        [2.0144e-04, 2.5207e-05, 1.9824e-04]])\n",
      ">> layers.2.0.weight has gradient mean of tensor([[2.1789e-04, 6.5373e-06, 2.5559e-04],\n",
      "        [1.2170e-03, 3.6514e-05, 1.4276e-03],\n",
      "        [4.6673e-05, 1.4003e-06, 5.4749e-05]])\n",
      ">> layers.3.0.weight has gradient mean of tensor([[1.8056e-04, 2.5326e-05, 2.2459e-04],\n",
      "        [3.3904e-03, 4.7555e-04, 4.2170e-03],\n",
      "        [1.9221e-03, 2.6960e-04, 2.3907e-03]])\n",
      ">> layers.4.0.weight has gradient mean of tensor([[0.0184, 0.0108, 0.0193]])\n"
     ]
    }
   ],
   "source": [
    "model_without_shorcut = ExampleDeepNeuralNetwork(layer_sizes, True)\n",
    "print_gradients(model_without_shorcut, sample_input)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.5 连接注意力层和线性层\n",
    "\n",
    "**transformer block的说明**\n",
    "\n",
    "![1718347509428](image/从零开始构建LLM/1718347509428.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerBlock(nn.Module):\n",
    "    def __init__(self, cfg) -> None:\n",
    "        super().__init__()\n",
    "        self.att = MultiHeadAttention(d_in=cfg[\"emb_dim\"], \n",
    "                                      d_out=cfg[\"emb_dim\"],\n",
    "                                      context_length=cfg[\"context_length\"], \n",
    "                                      num_heads=cfg[\"n_heads\"],\n",
    "                                      dropout=cfg[\"drop_rate\"],\n",
    "                                      qkv_bias=cfg[\"qkv_bias\"])\n",
    "        self.ff = FeedForward(cfg)\n",
    "        self.norm1 = LayerNorm(cfg[\"emb_dim\"])\n",
    "        self.norm2 = LayerNorm(cfg[\"emb_dim\"])\n",
    "        self.drop_resid = nn.Dropout(cfg[\"drop_rate\"])\n",
    "    \n",
    "    def forward(self, x):\n",
    "        shortcut = x\n",
    "        x = self.norm1(x)\n",
    "        x = self.att(x)\n",
    "        x = self.drop_resid(x)\n",
    "        x = x + shortcut\n",
    "\n",
    "        shortcut = x\n",
    "\n",
    "        x = self.norm2(x)\n",
    "        x = self.ff(x)\n",
    "        x = self.drop_resid(x)\n",
    "        x = x + shortcut\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.6 编码GPT模型\n",
    "\n",
    "![1718352064601](image/从零开始构建LLM/1718352064601.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GPTModel(nn.Module):\n",
    "    def __init__(self, cfg):\n",
    "        super().__init__()\n",
    "        self.tok_emb = nn.Embedding(cfg[\"vocab_size\"], cfg[\"emb_dim\"])\n",
    "        self.pos_emb = nn.Embedding(cfg[\"context_length\"], cfg[\"emb_dim\"])\n",
    "        self.drop_emb = nn.Dropout(cfg[\"drop_rate\"])\n",
    "        \n",
    "        self.trf_blocks = nn.Sequential(\n",
    "            *[TransformerBlock(cfg) for _ in range(cfg[\"n_layers\"])])\n",
    "        \n",
    "        self.final_norm = LayerNorm(cfg[\"emb_dim\"])\n",
    "        self.out_head = nn.Linear(\n",
    "            cfg[\"emb_dim\"], cfg[\"vocab_size\"], bias=False\n",
    "        )\n",
    "\n",
    "    def forward(self, in_idx):\n",
    "        batch_size, seq_len = in_idx.shape\n",
    "        tok_embeds = self.tok_emb(in_idx)\n",
    "        pos_embeds = self.pos_emb(torch.arange(seq_len, device=in_idx.device))\n",
    "        x = tok_embeds + pos_embeds  # Shape [batch_size, num_tokens, emb_size]\n",
    "        x = self.drop_emb(x)\n",
    "        x = self.trf_blocks(x)\n",
    "        x = self.final_norm(x)\n",
    "        logits = self.out_head(x)\n",
    "        return logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">> input batch:  tensor([[6109, 3626, 6100,  345],\n",
      "        [6109, 1110, 6622,  257]])\n",
      ">> output shape:  torch.Size([2, 4, 50257])\n",
      ">> out:  tensor([[[ 0.2159,  1.7803, -1.3144,  ...,  0.1518,  0.3433, -0.2426],\n",
      "         [-0.4640,  0.0124, -0.0818,  ...,  0.1752,  0.8148,  0.2054],\n",
      "         [-0.3823,  0.1988,  0.2054,  ...,  0.0659, -0.1415,  0.1057],\n",
      "         [-0.1542,  0.7591, -0.2797,  ..., -0.2908,  0.6896, -0.1746]],\n",
      "\n",
      "        [[-0.2024,  1.6316, -0.9047,  ...,  0.1310,  0.5152, -0.3395],\n",
      "         [-0.0648,  0.8608, -0.4911,  ...,  0.5268, -0.0134,  0.3010],\n",
      "         [-0.1856, -0.6939,  0.0868,  ...,  0.5741, -0.0706,  0.2637],\n",
      "         [-0.0827, -0.1011,  0.4543,  ...,  0.1231, -0.1420,  0.0325]]],\n",
      "       grad_fn=<UnsafeViewBackward0>)\n"
     ]
    }
   ],
   "source": [
    "model = GPTModel(GPT_CONFIG_124M)\n",
    "\n",
    "out = model(batch)\n",
    "print(\">> input batch: \", batch)\n",
    "print(\">> output shape: \", out.shape)\n",
    "print(\">> out: \", out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">> total number of parameters: 163009536\n"
     ]
    }
   ],
   "source": [
    "total_params = sum(p.numel() for p in model.parameters())\n",
    "print(f\">> total number of parameters: {total_params}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> 之前提到代码参数工1.24亿，但是为何这里输出为1.63亿呢？<br/>\n",
    "其原因是在原始的GPT-2体系结构中使用了一个称为**权重绑定**的概念，这意味着原始的GPT-2体系结构在其输出层中重用了来自标记嵌入层的权重。为了理解这意味着什么，来看看我们之前通过GPTModel在模型上初始化的令牌嵌入层和线性输出层的形状："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">> Token embedding layer shape: torch.Size([50257, 768])\n",
      ">> Output layer shape: torch.Size([50257, 768])\n"
     ]
    }
   ],
   "source": [
    "print(f\">> Token embedding layer shape: {model.tok_emb.weight.shape}\")\n",
    "print(f\">> Output layer shape: {model.out_head.weight.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> 由于字典表的大小为50257，这导致嵌入层非常大。且输出层重用了嵌入层的权重，因此应减去这一部分的参数。\n",
    "\n",
    "> 在一般情况下，不是用权重绑定。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">> Number of trainable parameters considering weight tying: 124412160\n"
     ]
    }
   ],
   "source": [
    "total_params_gpt2 = total_params - sum(p.numel() for p in model.out_head.parameters())\n",
    "print(f\">> Number of trainable parameters considering weight tying: {total_params_gpt2}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total size of the model: 621.83 MB\n"
     ]
    }
   ],
   "source": [
    "total_size_bytes = total_params * 4\n",
    "total_size_mb = total_size_bytes / (1024 * 1024)\n",
    "print(f\"Total size of the model: {total_size_mb:.2f} MB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.7 生成文本\n",
    "\n",
    "![1718358855510](image/从零开始构建LLM/1718358855510.png)\n",
    "\n",
    "![1718358887750](image/从零开始构建LLM/1718358887750.png)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_text_simple(model, idx, max_new_tokens, context_size):\n",
    "    for _ in range(max_new_tokens):\n",
    "        idx_cond = idx[:, -context_size:]\n",
    "        with torch.no_grad():\n",
    "            logits = model(idx_cond)\n",
    "        logits = logits[:, -1, :]\n",
    "        probas = torch.softmax(logits, dim=-1)\n",
    "        idx_next = torch.argmax(probas, dim=-1, keepdim=True)\n",
    "        idx = torch.cat((idx, idx_next), dim=-1)\n",
    "    return idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">> encoded: [15496, 11, 314, 716]\n",
      ">> encoded_tensor.shape: torch.Size([1, 4])\n"
     ]
    }
   ],
   "source": [
    "start_context = \"Hello, I am\"\n",
    "encoded = tokenizer.encode(start_context)\n",
    "print(\">> encoded:\", encoded)\n",
    "encoded_tensor = torch.tensor(encoded).unsqueeze(0)\n",
    "print(\">> encoded_tensor.shape:\", encoded_tensor.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">> Output: tensor([[15496,    11,   314,   716, 17480, 23268,  2497, 19749,  1333, 15262]])\n",
      ">> Output length: 10\n"
     ]
    }
   ],
   "source": [
    "model.eval()\n",
    "out = generate_text_simple(\n",
    "    model=model,\n",
    "    idx=encoded_tensor,\n",
    "    max_new_tokens=6,\n",
    "    context_size=GPT_CONFIG_124M[\"context_length\"]\n",
    ")\n",
    "print(\">> Output:\", out)\n",
    "print(\">> Output length:\", len(out[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">> Generated text:  Hello, I amINGTON vow saw bourgeois triBay\n"
     ]
    }
   ],
   "source": [
    "decoded_text = tokenizer.decode(out.squeeze(0).tolist())\n",
    "print(\">> Generated text: \", decoded_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5 在未标记的数据上进行训练\n",
    "\n",
    "## 5.1 评估生成文本模型\n",
    "\n",
    "![1718851702204](image/从零开始构建LLM/1718851702204.png)\n",
    "\n",
    "5.1.1 使用GPT模型生成文本\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GPTModel(\n",
       "  (tok_emb): Embedding(50257, 768)\n",
       "  (pos_emb): Embedding(256, 768)\n",
       "  (drop_emb): Dropout(p=0.1, inplace=False)\n",
       "  (trf_blocks): Sequential(\n",
       "    (0): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layer): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_resid): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (1): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layer): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_resid): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (2): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layer): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_resid): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (3): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layer): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_resid): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (4): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layer): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_resid): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (5): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layer): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_resid): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (6): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layer): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_resid): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (7): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layer): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_resid): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (8): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layer): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_resid): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (9): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layer): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_resid): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (10): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layer): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_resid): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (11): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layer): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_resid): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "  )\n",
       "  (final_norm): LayerNorm()\n",
       "  (out_head): Linear(in_features=768, out_features=50257, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "GPT_CONFIG_124M = {\n",
    "    \"vocab_size\": 50257,\n",
    "    \"context_length\": 256, #A\n",
    "    \"emb_dim\": 768,\n",
    "    \"n_heads\": 12,\n",
    "    \"n_layers\": 12,\n",
    "    \"drop_rate\": 0.1, #B\n",
    "    \"qkv_bias\": False\n",
    "}\n",
    "model = GPTModel(GPT_CONFIG_124M)\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "与之前的进行对比，context_length减少到256（之前为1024），这一改变减少了计算需求，使得能够在桌面版计算机上运行。\n",
    "在之后的训练中，将其更新回1024，以便于加载预训练模型。\n",
    "\n",
    "---\n",
    "\n",
    "三步生成文本的过程：\n",
    "1. tokenizer将输入文本转换为一系列token IDs\n",
    "2. 模型将token IDs转换为对应的logits，logits表示每个token在字典中可能的分布状况\n",
    "3. 将logits转换为token IDs，tokenizer再进行解码，生成文本\n",
    "\n",
    "![1718853446792](image/从零开始构建LLM/1718853446792.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">> encoded: tensor([[6109, 3626, 6100,  345]])\n",
      ">> decoded_text: Every effort moves you tier dwell Emperor frequent glacierCV SaberTheme number stuffed\n"
     ]
    }
   ],
   "source": [
    "def text_to_token_ids(text, tokenizer):\n",
    "    encoded = tokenizer.encode(text, allowed_special={'<|endoftext|>'})\n",
    "    encoded_tensor = torch.tensor(encoded).unsqueeze(0)\n",
    "    return encoded_tensor\n",
    "\n",
    "def token_ids_to_text(token_ids, tokenizer):\n",
    "    decoded = tokenizer.decode(token_ids.squeeze(0).tolist())\n",
    "    return decoded\n",
    "\n",
    "start_context = \"Every effort moves you\"\n",
    "tokenizer = tiktoken.get_encoding(\"gpt2\")\n",
    "\n",
    "encoded = text_to_token_ids(start_context, tokenizer)\n",
    "print(\">> encoded:\", encoded)\n",
    "\n",
    "token_ids = generate_text_simple(model=model, idx=encoded, max_new_tokens=10, context_size=GPT_CONFIG_124M['context_length'])\n",
    "\n",
    "decoded_text = token_ids_to_text(token_ids, tokenizer)\n",
    "print(\">> decoded_text:\", decoded_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.1.2 计算文本生成的损失\n",
    "\n",
    "![1718854323771](image/从零开始构建LLM/1718854323771.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">> probas:  tensor([[[9.6198e-06, 2.4149e-05, 8.7226e-06,  ..., 3.6962e-05,\n",
      "          1.0996e-05, 2.2752e-05],\n",
      "         [4.5642e-06, 1.3964e-05, 2.1854e-05,  ..., 3.4952e-05,\n",
      "          2.3353e-05, 3.5968e-05],\n",
      "         [8.8770e-06, 1.1363e-05, 1.5514e-05,  ..., 2.9990e-05,\n",
      "          1.1737e-05, 2.2775e-05]],\n",
      "\n",
      "        [[7.2513e-06, 2.1858e-05, 1.6114e-05,  ..., 4.5776e-05,\n",
      "          2.5422e-05, 4.1621e-05],\n",
      "         [3.7363e-06, 1.7486e-05, 2.5110e-05,  ..., 2.4692e-05,\n",
      "          2.0825e-05, 4.5605e-05],\n",
      "         [2.2280e-05, 9.2259e-06, 2.2976e-05,  ..., 1.6067e-05,\n",
      "          1.4410e-05, 2.5571e-05]]])\n",
      ">> token_ids:  tensor([[[21158],\n",
      "         [15863],\n",
      "         [ 1482]],\n",
      "\n",
      "        [[26478],\n",
      "         [14290],\n",
      "         [50105]]])\n",
      ">> target batch:   effort moves you\n",
      ">> predicted batch:   slam donated Con\n"
     ]
    }
   ],
   "source": [
    "inputs = torch.tensor([[16833, 3626, 6100],   # [\"every effort moves\",\n",
    "                       [40,    1107, 588]])   #  \"I really like\"]\n",
    "\n",
    "targets = torch.tensor([[3626, 6100, 345  ],  # [\" effort moves you\",\n",
    "                        [1107,  588, 11311]]) #  \" really like chocolate\"]\n",
    "\n",
    "with torch.no_grad():\n",
    "    logits = model(inputs)\n",
    "\n",
    "probas = torch.softmax(logits, dim=-1)\n",
    "print(\">> probas: \", probas)\n",
    "\n",
    "token_ids = torch.argmax(probas, dim=-1, keepdim=True)\n",
    "print(\">> token_ids: \", token_ids)\n",
    "\n",
    "print(\">> target batch: \", token_ids_to_text(targets[0], tokenizer))\n",
    "print(\">> predicted batch: \", token_ids_to_text(token_ids[0].flatten(), tokenizer))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "由于还未被训练，因此产生的为随机文本。\n",
    "训练过程是不断减小目标与预测值之间的“距离”。\n",
    "\n",
    "![1718868480909](image/从零开始构建LLM/1718868480909.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">> Text 1: tensor([1.2231e-05, 1.0606e-05, 2.6981e-05])\n",
      ">> Text 2: tensor([9.0465e-06, 2.4282e-05, 2.5903e-05])\n",
      ">> log probas:  tensor([-11.3115, -11.4541, -10.5204, -11.6131, -10.6258, -10.5612])\n",
      ">> avg log probas:  tensor(-11.0144)\n",
      ">> neg avg log probas:  tensor(11.0144)\n"
     ]
    }
   ],
   "source": [
    "# 2-3\n",
    "text_idx = 0\n",
    "target_probas_1 = probas[text_idx, [0, 1, 2], targets[text_idx]]\n",
    "print(\">> Text 1:\", target_probas_1)\n",
    "\n",
    "text_idx = 1\n",
    "target_probas_2 = probas[text_idx, [0, 1, 2], targets[text_idx]]\n",
    "print(\">> Text 2:\", target_probas_2)\n",
    "\n",
    "# 4\n",
    "log_probas = torch.log(torch.cat((target_probas_1, target_probas_2)))\n",
    "print(\">> log probas: \", log_probas)\n",
    "\n",
    "# 5\n",
    "avg_log_probas = torch.mean(log_probas)\n",
    "print(\">> avg log probas: \", avg_log_probas)\n",
    "\n",
    "# 6\n",
    "neg_avg_log_probas = -avg_log_probas\n",
    "print(\">> neg avg log probas: \", neg_avg_log_probas)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "交叉熵 Cross Entropy Loss， 是用来衡量两个概率分布之间的差异"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">> Logits shape: torch.Size([2, 3, 50257])\n",
      ">> Targets shape: torch.Size([2, 3])\n"
     ]
    }
   ],
   "source": [
    "# Logits have shape (batch_size, num_tokens, vocab_size)\n",
    "print(\">> Logits shape:\", logits.shape)\n",
    "\n",
    "# Targets have shape (batch_size, num_tokens)\n",
    "print(\">> Targets shape:\", targets.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "在进行交叉熵之前，需要检查向量维度"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">> Flattened logits: torch.Size([6, 50257])\n",
      ">> Flattened targets: torch.Size([6])\n"
     ]
    }
   ],
   "source": [
    "logits_flat = logits.flatten(0, 1)\n",
    "targets_flat = targets.flatten()\n",
    "\n",
    "print(\">> Flattened logits:\", logits_flat.shape)\n",
    "print(\">> Flattened targets:\", targets_flat.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">> Loss:  tensor(11.0144)\n"
     ]
    }
   ],
   "source": [
    "loss = nn.functional.cross_entropy(logits_flat, targets_flat)\n",
    "print(\">> Loss: \", loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> 困惑度</br>\n",
    "\n",
    "困惑度也是评价语言模型好坏的指标。它可以提供一种更可解释的方法来理解模型在预测序列中的下一个标记时的不确定性。</br>\n",
    "困惑度衡量了模型预测的概率分布与数据集中单词的实际分布的匹配程度。与损失相似，较低的困惑度表明模型的预测更接近实际分布。</br>\n",
    "困惑度的可解释性在于它表示模型在每一步中都不确定的有效词汇表大小。</br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(60739.7578)\n"
     ]
    }
   ],
   "source": [
    "perplexity = torch.exp(loss)\n",
    "print(perplexity)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.1.3 计算训练和验证损失"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">> Characters: 20479\n",
      ">> Tokens: 5145\n"
     ]
    }
   ],
   "source": [
    "text_data = raw_text\n",
    "total_characters = len(text_data)\n",
    "total_tokens = len(tokenizer.encode(text_data))\n",
    "\n",
    "print(\">> Characters:\", total_characters)\n",
    "print(\">> Tokens:\", total_tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![1718868663879](image/从零开始构建LLM/1718868663879.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_ratio = 0.9\n",
    "split_idx = int(len(text_data) * train_ratio)\n",
    "train_data = text_data[:split_idx]\n",
    "val_data = text_data[split_idx:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = create_dataloader_v1(train_data, \n",
    "                                    batch_size=2, \n",
    "                                    max_length=GPT_CONFIG_124M['context_length'], \n",
    "                                    stride=GPT_CONFIG_124M['context_length'], \n",
    "                                    drop_last=True, \n",
    "                                    shuffle=True)\n",
    "val_loader = create_dataloader_v1(val_data, \n",
    "                                  batch_size=2, \n",
    "                                  max_length=GPT_CONFIG_124M['context_length'], \n",
    "                                  stride=GPT_CONFIG_124M['context_length'], \n",
    "                                  drop_last=False, \n",
    "                                  shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loader:\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "\n",
      "Validation loader:\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n"
     ]
    }
   ],
   "source": [
    "print(\"Train loader:\")\n",
    "for x, y in train_loader:\n",
    "    print(x.shape, y.shape)\n",
    "\n",
    "print(\"\\nValidation loader:\")\n",
    "for x, y in val_loader:\n",
    "    print(x.shape, y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_loss_batch(input_batch, target_batch, model, device):\n",
    "    input_batch, target_batch = input_batch.to(device), target_batch.to(device)\n",
    "    logits = model(input_batch)\n",
    "    loss = nn.functional.cross_entropy(logits.flatten(0, 1), target_batch.flatten())\n",
    "    return loss\n",
    "\n",
    "\n",
    "def calc_loss_loader(data_loader, model, device, num_batches=None):\n",
    "    total_loss = 0\n",
    "    if num_batches is None:\n",
    "        num_batches = len(data_loader)\n",
    "    else:\n",
    "        num_batches = min(len(data_loader), num_batches)\n",
    "    for i, (input_batch, target_batch) in enumerate(data_loader):\n",
    "        if i >= num_batches:\n",
    "            break\n",
    "        loss = calc_loss_batch(input_batch, target_batch, model, device)\n",
    "        total_loss += loss.item()\n",
    "    return total_loss / num_batches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">> Train loss: 10.989679866366917\n",
      ">> Val loss: 11.008943557739258\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)\n",
    "train_loss = calc_loss_loader(train_loader, model, device)\n",
    "print(f\">> Train loss: {train_loss}\")\n",
    "\n",
    "val_loss = calc_loss_loader(val_loader, model, device)\n",
    "print(f\">> Val loss: {val_loss}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.2 训练一个LLM\n",
    "\n",
    "![1718875144870](image/从零开始构建LLM/1718875144870.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluete_model(model, train_loader, val_loader, device, eval_iter):\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        train_loss = calc_loss_loader(train_loader, model, device, eval_iter)\n",
    "        val_loss = calc_loss_loader(val_loader, model, device, eval_iter)\n",
    "    model.train()\n",
    "    return train_loss, val_loss\n",
    "\n",
    "\n",
    "def generate_and_print_sample(model, tokenizer, device, start_context):\n",
    "    model.eval()\n",
    "    context_size = model.pos_emb.weight.shape[0]\n",
    "    excoded = text_to_token_ids(start_context, tokenizer).to(device)\n",
    "    with torch.no_grad():\n",
    "        token_ids = generate_text_simple(model, excoded, 50 , context_size)\n",
    "        decoded_text = token_ids_to_text(token_ids, tokenizer)\n",
    "        print(f\">> {start_context} --> {decoded_text}\")\n",
    "    model.train()\n",
    "\n",
    "\n",
    "def train_model_simple(model, train_loader, val_loader, optimizer, device, num_epochs,\n",
    "                       eval_freq, eval_iter, start_context, tokenizer):\n",
    "    train_losses, val_losses, track_token_seen = [], [], []\n",
    "    token_seen, global_step = 0, -1\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        for input_batch, target_batch in train_loader:\n",
    "            optimizer.zero_grad()\n",
    "            loss = calc_loss_batch(input_batch, target_batch, model, device)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            token_seen += input_batch.numel()\n",
    "            global_step += 1\n",
    "            if global_step % eval_freq == 0:\n",
    "                train_loss, val_loss = evaluete_model(model, train_loader, val_loader, device, eval_iter)\n",
    "                train_losses.append(train_loss)\n",
    "                val_losses.append(val_loss)\n",
    "                track_token_seen.append(token_seen)\n",
    "                print(f\">> Epoch {epoch + 1}, step {global_step: 06d}: train loss {train_loss:.4f}, val loss {val_loss:.4f}\")\n",
    "    \n",
    "        generate_and_print_sample(model, tokenizer, device, start_context)\n",
    "    return train_losses, val_losses, track_token_seen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">> Epoch 1, step  00000: train loss 10.7070, val loss 10.7820\n",
      ">> Epoch 1, step  00005: train loss 9.5462, val loss 9.7495\n",
      ">> Every effort moves you --> Every effort moves you,,,\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      ">> Epoch 2, step  00010: train loss 8.9008, val loss 9.1415\n",
      ">> Epoch 2, step  00015: train loss 8.4460, val loss 8.6694\n",
      ">> Every effort moves you --> Every effort moves you,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,.\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      ">> Epoch 3, step  00020: train loss 8.0585, val loss 8.2633\n",
      ">> Epoch 3, step  00025: train loss 7.6819, val loss 7.9278\n",
      ">> Every effort moves you --> Every effort moves you,,,\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      ">> Epoch 4, step  00030: train loss 7.4977, val loss 7.6869\n",
      ">> Epoch 4, step  00035: train loss 7.2686, val loss 7.5326\n",
      ">> Every effort moves you --> Every effort moves you,,,,,,,,,,,,,,, the the the,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,\n",
      ">> Epoch 5, step  00040: train loss 7.1630, val loss 7.4220\n",
      ">> Every effort moves you --> Every effort moves you,,,,,,,,,,,,,,, I,,,,,,,,,,,,,,,,,,.\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      ">> Epoch 6, step  00045: train loss 7.0491, val loss 7.2946\n",
      ">> Epoch 6, step  00050: train loss 6.9471, val loss 7.2421\n",
      ">> Every effort moves you --> Every effort moves you.\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      ">> Epoch 7, step  00055: train loss 6.9180, val loss 7.1450\n",
      ">> Epoch 7, step  00060: train loss 6.8680, val loss 7.0848\n",
      ">> Every effort moves you --> Every effort moves you,,,,,,,,,,,,,,, I,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,\n",
      ">> Epoch 8, step  00065: train loss 6.6950, val loss 7.0371\n",
      ">> Epoch 8, step  00070: train loss 6.6558, val loss 7.0065\n",
      ">> Every effort moves you --> Every effort moves you,,,\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      ">> Epoch 9, step  00075: train loss 6.6409, val loss 6.9602\n",
      ">> Epoch 9, step  00080: train loss 6.6201, val loss 6.9578\n",
      ">> Every effort moves you --> Every effort moves you, and, and, I, I had, and.\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      ">> Epoch 10, step  00085: train loss 6.5728, val loss 6.9412\n",
      ">> Every effort moves you --> Every effort moves you, and, and, I, and.\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "model = GPTModel(GPT_CONFIG_124M)\n",
    "model.to(device)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=4e-4, weight_decay=0.1)\n",
    "\n",
    "num_epochs = 10\n",
    "train_losses, val_losses, track_token_seen = train_model_simple(model, train_loader, val_loader, optimizer, device,\n",
    "                                                               num_epochs, 5, 5, \"Every effort moves you\", tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GPTModel(\n",
       "  (tok_emb): Embedding(50257, 768)\n",
       "  (pos_emb): Embedding(256, 768)\n",
       "  (drop_emb): Dropout(p=0.1, inplace=False)\n",
       "  (trf_blocks): Sequential(\n",
       "    (0): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layer): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_resid): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (1): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layer): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_resid): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (2): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layer): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_resid): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (3): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layer): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_resid): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (4): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layer): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_resid): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (5): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layer): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_resid): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (6): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layer): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_resid): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (7): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layer): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_resid): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (8): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layer): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_resid): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (9): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layer): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_resid): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (10): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layer): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_resid): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (11): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layer): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_resid): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "  )\n",
       "  (final_norm): LayerNorm()\n",
       "  (out_head): Linear(in_features=768, out_features=50257, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# save model\n",
    "torch.save(model.state_dict(), \"model.pt\")\n",
    "\n",
    "# load model\n",
    "model = GPTModel(GPT_CONFIG_124M)\n",
    "model.load_state_dict(torch.load(\"model.pt\"))\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1000x500 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def plot_losses(epochs_seen, tokens_seen, train_losses, val_losses):\n",
    "    fig, ax = plt.subplots(figsize=(10, 5))\n",
    "\n",
    "    ax.plot(epochs_seen, train_losses, label=\"train\")\n",
    "    ax.plot(epochs_seen, val_losses, linestyle='-.', label=\"val\")\n",
    "    ax.set_xlabel(\"Epochs\")\n",
    "    ax.set_ylabel(\"Loss\")\n",
    "    ax.legend()\n",
    "\n",
    "    ax1 = ax.twiny()\n",
    "    ax1.plot(tokens_seen, train_losses, alpha=0)\n",
    "    ax1.set_xlabel(\"Tokens Seen\")\n",
    "\n",
    "    fig.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "epoch_tensor = torch.linspace(0, num_epochs, len(train_losses))\n",
    "plot_losses(epoch_tensor, track_token_seen, train_losses, val_losses)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.3 控制随机性的解码策略\n",
    "\n",
    "主要介绍两个函数：`temperature scaling` 和 `top-k sampling`\n",
    "\n",
    "首先需要将模型转到cpu上，因为较小的模型在推理时不需要gpu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GPTModel(\n",
       "  (tok_emb): Embedding(50257, 768)\n",
       "  (pos_emb): Embedding(256, 768)\n",
       "  (drop_emb): Dropout(p=0.1, inplace=False)\n",
       "  (trf_blocks): Sequential(\n",
       "    (0): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layer): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_resid): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (1): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layer): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_resid): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (2): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layer): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_resid): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (3): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layer): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_resid): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (4): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layer): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_resid): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (5): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layer): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_resid): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (6): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layer): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_resid): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (7): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layer): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_resid): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (8): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layer): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_resid): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (9): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layer): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_resid): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (10): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layer): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_resid): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (11): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layer): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_resid): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "  )\n",
       "  (final_norm): LayerNorm()\n",
       "  (out_head): Linear(in_features=768, out_features=50257, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.to('cpu')\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">> generated text:  Every effort moves you, and, and, I, and.\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "tokenizer = tiktoken.get_encoding(\"gpt2\")\n",
    "token_ids = generate_text_simple(model=model,\n",
    "                                 idx=text_to_token_ids(\"Every effort moves you\", tokenizer),\n",
    "                                 max_new_tokens=25,\n",
    "                                 context_size=GPT_CONFIG_124M[\"context_length\"])\n",
    "print(\">> generated text: \", token_ids_to_text(token_ids, tokenizer))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.3.1 Temperature scaling\n",
    "\n",
    "一种向下一代标记生成任务添加概率选择过程的技术。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
