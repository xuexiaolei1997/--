## 1.时间序列基本规则法-周期因子法

· 提取时间序列的周期性特征进行预测

    · 计算周期因子factors

    · 计算base

    · 预测=base*factors

· 观察序列，当序列存在周期性时，可以用周期因子法做为baseline

## 2.线性回归-利用时间特征做线性回归

· 提取时间的周期性特点做为特征，此时训练集每条样本为"时间特征->目标值"，时间序列的依赖关系被剔除，不需要严格依赖滑窗截取训练样本。常见是将时间用0-1哑变量表达

· 观察序列，当序列存在周期性时，线性回归也可做为baseline

## 3.传统时序建模方法，ARMA/ARIMA等线性模型

* 差分方法可消除正相关但同时引入负相关
* AR项可消除正相关，MA项消除负相关
* AR项和MA项作用会相互抵消，通常包含两种要素时可尝试减少某项，避免过拟合

## 4.时间序列分解，使用加法模型或乘法模型将原始序列拆分为4部分

* 拆分为4部分：长期趋势变动T、季节变动S(显式周期，固定幅度、长度的周期波动)、循环变动C(隐式周期，周期长不具严格规则的波动)和不规则变动I。
* 乘法模型中SCI均为比例，加法模型中SCI与T有相同量纲。
* 循环变动C较为复杂，短期不体现或归入趋势变化中。
* 两类平滑方法：
  * 以滑动平均作为平滑方法提取趋势的seasonal_decompose朴素分解。
  * 以鲁棒局部加权回归作为平滑方法的STL分解。
* 季节性分析。数据中有季节性因素，与整体趋势相比显得比较弱。

## 5.特征工程着手，时间滑窗改变数据的组织方式，使用xgboost/LSTM模型/时间卷积网络等

## 6.转化为监督学习数据集，使用xgboot/LSTM模型/时间卷积网络/seq2seq(attention_based_model)

## 7.Facebook-prophet，类似于STL分解思路，在控制程度和可解释性上比传统时序模型更有优势

## 8.深度学习网络，结合CNN+RNN+Attention，作用各不相同互相配合

## 9.将时间序列转化为图像，再应用基于卷积神经网络的模型做分析

难点：

* 理解时间序列预测问题是要用历史数据预测未来数据
* 时间序列问题的训练集、测试集划分
* 特征工程方法及过程( *方法2的过程很有趣* )
* 如何转化为监督学习数据集
* LSTM计算过程理解，包括输入输出维度、参数数量等
* seq2seq过程的理解，decoder实现
* attention注意力机制的原理及实现，包括encoder-decoder attention, self attention, multi-head attention等
* 时间卷积网络的含义，顾名思义就是将CNN方法用于时间序列中，主要是dilated-convolution and causal-convolution
* prophet预测原理，各参数对模型拟合效果、泛化效果的影响
* TPA侧重选择关键变量
* 时间序列基本规则法中周期因子得计算过程
* 传统方法如周期因子、线性回归、ARMA等的预测结果表现为，预测趋势大致正确，但对波动预测不理想，体现在波动的幅度差异、相位偏移。
* 时间序列分解方法。理解加法模型和乘法模型，判断分解模型的选取及分解技巧。
