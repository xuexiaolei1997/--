## 1 使用贝叶斯进行超参数优化

```python
from bayes_opt import BayesianOptimization

# 定义搜索空间
search_space = {'num_leaves': (5, 50),
                'max_depth': (3, 15),
                'learning_rate': (0.01, 0.3),
                'n_estimators': (50, 200)}

# 定义优化函数
def optimize_lgbm(num_leaves, max_depth, learning_rate, n_estimators):
    # 定义模型
    model = LGBMRegressor(num_leaves=int(num_leaves),
                          max_depth=int(max_depth),
                          learning_rate=learning_rate,
                          n_estimators=int(n_estimators))
    # 训练模型
    model.fit(X_train, y_train)
    # 预测并计算MSE
    y_pred = model.predict(X_test)
    mape = mean_absolute_percentage_error(y_test, y_pred)
    # 返回负的MSE（因为BayesianOptimization寻找最大值）
    return -mape


# 创建Bayesian优化器并开始优化
optimizer = BayesianOptimization(f=optimize_lgbm, pbounds=search_space)
optimizer.maximize(init_points=5, n_iter=10)

# 输出最佳参数
print(optimizer.max)

best_params = optimizer.max['params']
for k, v in best_params.items():
    best_params[k] = int(v) if isinstance(search_space[k][0], int) else v
```

## 2 使用高斯过程优化

```python
from skopt import gp_minimize
from skopt.space import Integer, Real
from skopt.utils import use_named_args

# gaussian process
gp_space = []
for k, v in search_space.items():
    ptype = Integer if isinstance(v[0], int) else Real
    gp_space.append(ptype(search_space[k][0], search_space[k][1], name=k))


@use_named_args(gp_space)
def score_func(**params):
    model = LGBMRegressor(**best_params)
    model.fit(X_train, y_train)
    y_pred = model.predict(X_test)
    mape = mean_absolute_percentage_error(y_test, y_pred)
    return mape


result = gp_minimize(score_func, gp_space, n_calls=10, verbose=1)
print(result.x)
print(result.fun)

best_params = {}
for k, v in zip(search_space.keys(), result.x):
    best_params[k] = v
```
